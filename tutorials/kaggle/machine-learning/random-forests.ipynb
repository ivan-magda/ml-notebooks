{"cells":[{"metadata":{"_cell_guid":"b90d37c0-fdef-47a0-af20-fbc6a8b93b0a","_uuid":"8ee475a9b992ffb91daa2e7970f041cf5fdd82e9"},"cell_type":"markdown","source":"*This tutorial is part of the series [Learn Machine Learning](https://www.kaggle.com/learn/machine-learning). At the end of this step, you will be able to use your first sophisticated machine learning model, the Random Forest.*\n\n# Introduction\n\nDecision trees leave you with a difficult decision. A deep tree with lots of leaves will overfit because each prediction is coming from historical data from only the few houses at its leaf. But a shallow tree with few leaves will perform poorly because it fails to capture as many distinctions in the raw data.\n\nEven today's most sophisticated modeling techniques face this tension between underfitting and overfitting. But, many models have clever ideas that can lead to better performance. We'll look at the **random forest** as an example.\n\nThe random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree. It generally has much better predictive accuracy than a single decision tree and it works well with default parameters. If you keep modeling, you can learn more models with even better performance, but many of those are sensitive to getting the right parameters. \n\n# Example\n\nYou've already seen the code to load the data a few times. At the end of data-loading, we have the following variables:\n- train_X\n- val_X\n- train_y\n- val_y"},{"metadata":{"_cell_guid":"676bc8b4-c4ab-4da9-a5fc-16e30f6772a4","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"9a6efd7b10678776a5d7525cf3a61e52545859fe","collapsed":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\n    \n# Load data\nmelbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'\nmelbourne_data = pd.read_csv(melbourne_file_path) \n# Filter rows with missing values\nmelbourne_data = melbourne_data.dropna(axis=0)\n# Choose target and predictors\ny = melbourne_data.Price\nmelbourne_predictors = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n                        'YearBuilt', 'Lattitude', 'Longtitude']\nX = melbourne_data[melbourne_predictors]\n\nfrom sklearn.model_selection import train_test_split\n\n# split data into training and validation data, for both predictors and target\n# The split is based on a random number generator. Supplying a numeric value to\n# the random_state argument guarantees we get the same split every time we\n# run this script.\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0f39ad40-06d7-47f1-b18a-aa787228fabc","_uuid":"49d7b53d2639a3f31145baf072758586f9677dc6"},"cell_type":"markdown","source":"We build a RandomForest similarly to how we built a decision tree in scikit-learn."},{"metadata":{"_cell_guid":"172cccfa-69ba-42ab-96ba-dad4949eb980","_uuid":"be7bbd4c1ede885ddf5515d767e168adbadb7253","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nforest_model = RandomForestRegressor()\nforest_model.fit(train_X, train_y)\nmelb_preds = forest_model.predict(val_X)\nprint(mean_absolute_error(val_y, melb_preds))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"35438077-159b-437b-84af-d4a54f64dc69","_uuid":"120d7969e1bc5e4baaca6b17cc112f0eb1467e7d"},"cell_type":"markdown","source":"# Conclusion \nThere is likely room for further improvement, but this is a big improvement over the best decision tree error of 250,000. There are parameters which allow you to change the performance of the Random Forest much as we changed the maximum depth of the single decision tree. But one of the best features of Random Forest models is that they generally work reasonably even without this tuning.\n\nYou'll soon learn the XGBoost model, which provides better performance when tuned well with the right parameters (but which requires some skill to get the right model parameters).\n\n# Your Turn\nRun the RandomForestRegressor on your data. You should see a big improvement over your best Decision Tree models. \n\n# Continue \nYou will see more big improvements in your models as soon as you start the  Intermediate track in*Learn Machine Learning* . But you now have a model that's a good starting point to compete in a machine learning competition!\n\nFollow **[these steps](https://www.kaggle.com/dansbecker/submitting-from-a-kernel/)** to make submissions for your current model. Then you can watch your progress in subsequent steps as you climb up the leaderboard with your continually improving models."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}