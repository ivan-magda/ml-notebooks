{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Up and running with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and running a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "\n",
    "result = sess.run(f)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "\n",
    "result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "    \n",
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval()) #10\n",
    "    print(z.eval()) #15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing['data'].shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing['target'].reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.7185181e+01],\n",
       "       [ 4.3633747e-01],\n",
       "       [ 9.3952334e-03],\n",
       "       [-1.0711310e-01],\n",
       "       [ 6.4479220e-01],\n",
       "       [-4.0338000e-06],\n",
       "       [-3.7813708e-03],\n",
       "       [-4.2348403e-01],\n",
       "       [-4.3721911e-01]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with pure NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.69419202e+01]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "X = housing_data_plus_bias\n",
    "y = housing.target.reshape(-1, 1)\n",
    "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "print(theta_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.69419202e+01]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing['data'], housing['target'].reshape(-1, 1))\n",
    "\n",
    "print(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent requires scaling the feature vectors first. We could do this using TF, but let's just use Scikit-Learn for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing['data'])\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00000000e+00  6.60969987e-17  5.50808322e-18  6.60969987e-17\n",
      " -1.06030602e-16 -1.10161664e-17  3.44255201e-18 -1.07958431e-15\n",
      " -8.52651283e-15]\n",
      "[ 0.38915536  0.36424355  0.5116157  ... -0.06612179 -0.06360587\n",
      "  0.01359031]\n",
      "0.11111111111111005\n",
      "(20640, 9)\n"
     ]
    }
   ],
   "source": [
    "print(scaled_housing_data_plus_bias.mean(axis=0))\n",
    "print(scaled_housing_data_plus_bias.mean(axis=1))\n",
    "print(scaled_housing_data_plus_bias.mean())\n",
    "print(scaled_housing_data_plus_bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually computing the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.161543\n",
      "Epoch 100 MSE = 0.5305595\n",
      "Epoch 200 MSE = 0.52515554\n",
      "Epoch 300 MSE = 0.5244485\n",
      "Epoch 400 MSE = 0.52434105\n",
      "Epoch 500 MSE = 0.52432406\n",
      "Epoch 600 MSE = 0.52432144\n",
      "Epoch 700 MSE = 0.52432096\n",
      "Epoch 800 MSE = 0.5243211\n",
      "Epoch 900 MSE = 0.52432084\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learnign_rate = 0.1\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing['target'].reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learnign_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685577 ],\n",
       "       [ 0.8296403 ],\n",
       "       [ 0.11875557],\n",
       "       [-0.26556647],\n",
       "       [ 0.30572897],\n",
       "       [-0.00450184],\n",
       "       [-0.03932706],\n",
       "       [-0.8998373 ],\n",
       "       [-0.87049514]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above except for the `gradients = ...` line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = tf.gradients(mse, [theta])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.161543\n",
      "Epoch 100 MSE = 0.5305595\n",
      "Epoch 200 MSE = 0.52515554\n",
      "Epoch 300 MSE = 0.5244485\n",
      "Epoch 400 MSE = 0.524341\n",
      "Epoch 500 MSE = 0.524324\n",
      "Epoch 600 MSE = 0.5243215\n",
      "Epoch 700 MSE = 0.52432096\n",
      "Epoch 800 MSE = 0.524321\n",
      "Epoch 900 MSE = 0.52432084\n",
      "Best theta:\n",
      "[[ 2.0685577 ]\n",
      " [ 0.8296404 ]\n",
      " [ 0.11875559]\n",
      " [-0.26556668]\n",
      " [ 0.30572915]\n",
      " [-0.00450187]\n",
      " [-0.03932704]\n",
      " [-0.8998372 ]\n",
      " [-0.87049496]]\n"
     ]
    }
   ],
   "source": [
    "training_op = tf.assign(theta, theta - learnign_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could you find the partial derivatives of the following function with regards to `a` and `b`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(a, b):\n",
    "    z = 0\n",
    "    for i in range(100):\n",
    "        z = a * np.cos(z + i) + z * np.sin(b - i)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.21253923284754916"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_func(0.2, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "a = tf.Variable(0.2, name=\"a\")\n",
    "b = tf.Variable(0.3, name=\"b\")\n",
    "z = tf.constant(0.0, name=\"z0\")\n",
    "\n",
    "for i in range(100):\n",
    "    z = a * tf.cos(z + i) + z * tf.sin(b - i)\n",
    "\n",
    "grads = tf.gradients(z, [a, b])\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the function at $a=0.2$ and $b=0.3$, and the partial derivatives at that point with regards to $a$ and with regards to $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.21253741\n",
      "[-1.1388494, 0.19671395]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    print(z.eval())\n",
    "    print(sess.run(grads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a `GradientDescentOptimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learnign_rate = 0.1\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing['target'].reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learnign_rate)\n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.161543\n",
      "Epoch 100 MSE = 0.5305595\n",
      "Epoch 200 MSE = 0.52515554\n",
      "Epoch 300 MSE = 0.5244485\n",
      "Epoch 400 MSE = 0.524341\n",
      "Epoch 500 MSE = 0.524324\n",
      "Epoch 600 MSE = 0.5243215\n",
      "Epoch 700 MSE = 0.52432096\n",
      "Epoch 800 MSE = 0.524321\n",
      "Epoch 900 MSE = 0.52432084\n",
      "Best theta:\n",
      "[[ 2.0685577 ]\n",
      " [ 0.8296404 ]\n",
      " [ 0.11875559]\n",
      " [-0.26556668]\n",
      " [ 0.30572915]\n",
      " [-0.00450187]\n",
      " [-0.03932704]\n",
      " [-0.8998372 ]\n",
      " [-0.87049496]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a momentum optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learnign_rate = 0.1\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing['target'].reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learnign_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta:\n",
      "[[ 2.0685582 ]\n",
      " [ 0.82961947]\n",
      " [ 0.11875169]\n",
      " [-0.26552713]\n",
      " [ 0.30569643]\n",
      " [-0.00450299]\n",
      " [-0.03932627]\n",
      " [-0.8998851 ]\n",
      " [-0.8705405 ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding data to the training algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 7. 8.]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\n",
    "    \n",
    "print(B_val_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9. 10. 11.]\n",
      " [12. 13. 14.]]\n"
     ]
    }
   ],
   "source": [
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  # not shown in the book\n",
    "    indices = np.random.randint(m, size=batch_size)  # not shown\n",
    "    X_batch = scaled_housing_data_plus_bias[indices] # not shown\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] # not shown\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0703337 ],\n",
       "       [ 0.8637145 ],\n",
       "       [ 0.12255151],\n",
       "       [-0.31211874],\n",
       "       [ 0.38510373],\n",
       "       [ 0.00434168],\n",
       "       [-0.01232954],\n",
       "       [-0.83376896],\n",
       "       [-0.8030471 ]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and restoring a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.161543\n",
      "Epoch 100 MSE = 0.7145006\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.5555719\n",
      "Epoch 400 MSE = 0.5488112\n",
      "Epoch 500 MSE = 0.5436362\n",
      "Epoch 600 MSE = 0.5396294\n",
      "Epoch 700 MSE = 0.5365092\n",
      "Epoch 800 MSE = 0.5340678\n",
      "Epoch 900 MSE = 0.5321474\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000                                                                       \n",
    "learning_rate = 0.01                                                                  \n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")            \n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")            \n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")                                      \n",
    "error = y_pred - y                                                                    \n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")                                    \n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)            \n",
    "training_op = optimizer.minimize(mse)                                                 \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())                                \n",
    "            save_path = saver.save(sess, \"./tmp/my_model.ckpt\")\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"./tmp/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685525 ],\n",
       "       [ 0.8874027 ],\n",
       "       [ 0.14401658],\n",
       "       [-0.34770882],\n",
       "       [ 0.36178368],\n",
       "       [ 0.00393811],\n",
       "       [-0.04269556],\n",
       "       [-0.6614528 ],\n",
       "       [-0.6375277 ]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./tmp/my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(best_theta, best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to have a saver that loads and restores `theta` with a different name, such as `\"weights\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver({\"weights\": theta})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the saver also saves the graph structure itself in a second file with the extension `.meta`. You can use the function `tf.train.import_meta_graph()` to restore the graph structure. This function loads the graph into the default graph and returns a `Saver` that can then be used to restore the graph state (i.e., the variable values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"./tmp/my_model_final.ckpt.meta\")  # this loads the graph structure\n",
    "theta = tf.get_default_graph().get_tensor_by_name(\"theta:0\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./tmp/my_model_final.ckpt\")  # this restores the graph's state\n",
    "    best_theta_restored = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(best_theta, best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that you can import a pretrained model without having to have the corresponding Python code to build the graph. This is very handy when you keep tweaking and saving your model: you can load a previously saved model without having to search for the version of the code that built it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the graph\n",
    "### inside Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.3745401188473625&quot;).pbtxt = 'node {\\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;theta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;theta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2/tensor_names&quot;\\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 20640.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\240P\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/grad_ys_0&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/grad_ys_0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;index_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/mse_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/mse_grad/Reshape&quot;\\n  input: &quot;gradients/mse_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/mse_grad/Tile&quot;\\n  input: &quot;gradients/mse_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^gradients/mse_grad/truediv&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 2.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 9\\n        }\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save/Assign&quot;\\n}\\nnode {\\n  name: &quot;save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/SaveV2/tensor_names&quot;\\n  input: &quot;save/SaveV2/shape_and_slices&quot;\\n  input: &quot;theta&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;^save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;theta&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;random_uniform/max&quot;\\n  input: &quot;random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;random_uniform/RandomUniform&quot;\\n  input: &quot;random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;random_uniform/mul&quot;\\n  input: &quot;random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^theta/Assign&quot;\\n}\\nnode {\\n  name: &quot;y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 20640\\n          }\\n          dim {\\n            size: 1\\n          }\\n        }\\n        tensor_content: &quot;<stripped 82560 bytes>&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;X&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 20640\\n          }\\n          dim {\\n            size: 9\\n          }\\n        }\\n        tensor_content: &quot;<stripped 743040 bytes>&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;predictions&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;theta/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;predictions&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/Mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;sub&quot;\\n  input: &quot;gradients/Square_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/Mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/mse_grad/truediv&quot;\\n  input: &quot;gradients/Square_grad/Mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;gradients/Square_grad/Mul_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/Square_grad/Mul_1&quot;\\n  input: &quot;^gradients/sub_grad/Neg&quot;\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/sub_grad/Neg&quot;\\n  input: &quot;^gradients/sub_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/sub_grad/Neg&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/Square_grad/Mul_1&quot;\\n  input: &quot;^gradients/sub_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/Square_grad/Mul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;gradients/sub_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/sub_grad/tuple/control_dependency&quot;\\n  input: &quot;theta/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/predictions_grad/MatMul&quot;\\n  input: &quot;^gradients/predictions_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/predictions_grad/MatMul_1&quot;\\n  input: &quot;^gradients/predictions_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/predictions_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_theta/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;gradients/predictions_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update_theta/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/predictions_grad/MatMul&quot;\\n  input: &quot;^gradients/predictions_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/predictions_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Square&quot;\\n  op: &quot;Square&quot;\\n  input: &quot;sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mse&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;Square&quot;\\n  input: &quot;Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.3745401188473625&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"./tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0703337 ],\n",
       "       [ 0.8637145 ],\n",
       "       [ 0.12255151],\n",
       "       [-0.31211874],\n",
       "       [ 0.38510373],\n",
       "       [ 0.00434168],\n",
       "       [-0.01232954],\n",
       "       [-0.83376896],\n",
       "       [-0.8030471 ]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learnign_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta:\n",
      "[[-3.50344191e+21]\n",
      " [-2.35232508e+21]\n",
      " [-1.14905805e+21]\n",
      " [ 1.02714146e+21]\n",
      " [-3.17201948e+21]\n",
      " [ 2.81582050e+21]\n",
      " [-2.64538434e+23]\n",
      " [-5.38405898e+21]\n",
      " [-3.36946545e+21]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "\n",
    "file_writer.flush()\n",
    "file_writer.close()\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/sub\n"
     ]
    }
   ],
   "source": [
    "print(error.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/mse\n"
     ]
    }
   ],
   "source": [
    "print(mse.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a_1\n",
      "param/a\n",
      "param_1/a\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "a1 = tf.Variable(0, name=\"a\")      # name == \"a\"\n",
    "a2 = tf.Variable(0, name=\"a\")      # name == \"a_1\"\n",
    "\n",
    "with tf.name_scope(\"param\"):       # name == \"param\"\n",
    "    a3 = tf.Variable(0, name=\"a\")  # name == \"param/a\"\n",
    "\n",
    "with tf.name_scope(\"param\"):       # name == \"param_1\"\n",
    "    a4 = tf.Variable(0, name=\"a\")  # name == \"param_1/a\"\n",
    "\n",
    "for node in (a1, a2, a3, a4):\n",
    "    print(node.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ugly flat code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights1\")\n",
    "w2 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights2\")\n",
    "b1 = tf.Variable(0.0, name=\"bias1\")\n",
    "b2 = tf.Variable(0.0, name=\"bias2\")\n",
    "\n",
    "z1 = tf.add(tf.matmul(X, w1), b1, name=\"z1\")\n",
    "z2 = tf.add(tf.matmul(X, w2), b2, name=\"z2\")\n",
    "\n",
    "relu1 = tf.maximum(z1, 0., name=\"relu1\")\n",
    "relu2 = tf.maximum(z1, 0., name=\"relu2\")  # Oops, cut&paste error! Did you spot it?\n",
    "\n",
    "output = tf.add(relu1, relu2, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better, using a function to build the ReLUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"./tf_logs/relu1\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Even better using name scopes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)                          \n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")    \n",
    "        b = tf.Variable(0.0, name=\"bias\")                             \n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                      \n",
    "        return tf.maximum(z, 0., name=\"max\")                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"./tf_logs/relu2\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharing Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharing a `threshold` variable the classic way, by defining it outside of the `relu()` function then passing it as a parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X, threshold):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)                        \n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  \n",
    "        b = tf.Variable(0.0, name=\"bias\")                           \n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    \n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X, threshold) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        if not hasattr(relu, \"threshold\"):\n",
    "            relu.threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "        w_shape = int(X.get_shape()[1]), 1                          \n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  \n",
    "        b = tf.Variable(0.0, name=\"bias\")                           \n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    \n",
    "        return tf.maximum(z, relu.threshold, name=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\", reuse=True):\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\") as scope:\n",
    "    scope.reuse_variables()\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\", reuse=True):\n",
    "        threshold = tf.get_variable(\"threshold\")\n",
    "        w_shape = int(X.get_shape()[1]), 1                          # not shown\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "relus = [relu(X) for relu_index in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"./tf_logs/relu6\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\"):\n",
    "        threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"\", default_name=\"\") as scope:\n",
    "    first_relu = relu(X)     # create the shared variable\n",
    "    scope.reuse_variables()  # then reuse it\n",
    "    relus = [first_relu] + [relu(X) for i in range(4)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"./tf_logs/relu8\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))\n",
    "    w_shape = (int(X.get_shape()[1]), 1)                        # not shown in the book\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "    b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "    return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = []\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope(\"relu\", reuse=(relu_index >= 1)) as scope:\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"./tf_logs/relu9\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0: my_scope/x\n",
      "x1: my_scope/x_1\n",
      "x2: my_scope/x_2\n",
      "x3: my_scope/x\n",
      "x4: my_scope_1/x\n",
      "x5: my_scope/x\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.variable_scope(\"my_scope\"):\n",
    "    x0 = tf.get_variable(\"x\", shape=(), initializer=tf.constant_initializer(0.))\n",
    "    x1 = tf.Variable(0., name=\"x\")\n",
    "    x2 = tf.Variable(0., name=\"x\")\n",
    "    \n",
    "with tf.variable_scope(\"my_scope\", reuse=True):\n",
    "    x3 = tf.get_variable(\"x\")\n",
    "    x4 = tf.Variable(0., name=\"x\")\n",
    "    \n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):\n",
    "    x5 = tf.get_variable(\"my_scope/x\")\n",
    "    \n",
    "print(\"x0:\", x0.op.name)\n",
    "print(\"x1:\", x1.op.name)\n",
    "print(\"x2:\", x2.op.name)\n",
    "print(\"x3:\", x3.op.name)\n",
    "print(\"x4:\", x4.op.name)\n",
    "print(\"x5:\", x5.op.name)\n",
    "print(x0 is x3 and x3 is x5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first `variable_scope()` block first creates the shared variable `x0`, named `my_scope/x`. For all operations other than shared variables (including non-shared variables), the variable scope acts like a regular name scope, which is why the two variables `x1` and `x2` have a name with a prefix `my_scope/`. Note however that TensorFlow makes their names unique by adding an index: `my_scope/x_1` and `my_scope/x_2`.\n",
    "\n",
    "The second `variable_scope()` block reuses the shared variables in scope `my_scope`, which is why `x0 is x3`. Once again, for all operations other than shared variables it acts as a named scope, and since it's a separate block from the first one, the name of the scope is made unique by TensorFlow (`my_scope_1`) and thus the variable `x4` is named `my_scope_1/x`.\n",
    "\n",
    "The third block shows another way to get a handle on the shared variable `my_scope/x` by creating a `variable_scope()` at the root scope (whose name is an empty string), then calling `get_variable()` with the full name of the shared variable (i.e. `\"my_scope/x\"`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'Do' b'you' b'want' b'some' b'cafe?']\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "text = np.array(\"Do you want some cafe?\".split())\n",
    "text_tensor = tf.constant(text)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(text_tensor.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Home-Made Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x,y) = ((x) * (x)) * (y) + y + 2\n",
      "f(3,4) = 42\n"
     ]
    }
   ],
   "source": [
    "class Const(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var(object):\n",
    "    def __init__(self, init_value, name):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return self.a.evaluate() + self.b.evaluate()\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return self.a.evaluate() * self.b.evaluate()\n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)\n",
    "\n",
    "x = Var(3, name=\"x\")\n",
    "y = Var(4, name=\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = x²y + y + 2\n",
    "print(\"f(x,y) =\", f)\n",
    "print(\"f(3,4) =\", f.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing gradients\n",
    "### Mathematical differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx(3,4) = 24\n",
      "df/dy(3,4) = 10\n"
     ]
    }
   ],
   "source": [
    "df_dx = Mul(Const(2), Mul(x, y))  # df/dx = 2xy\n",
    "df_dy = Add(Mul(x, x), Const(1))  # df/dy = x² + 1\n",
    "\n",
    "print(\"df/dx(3,4) =\", df_dx.evaluate())\n",
    "print(\"df/dy(3,4) =\", df_dy.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx(3,4) = 24.000400000048216\n",
      "df/dy(3,4) = 10.000000000047748\n"
     ]
    }
   ],
   "source": [
    "def gradients(func, vars_list, eps=0.0001):\n",
    "    partial_derivatives = []\n",
    "    base_func_eval = func.evaluate()\n",
    "    for var in vars_list:\n",
    "        original_value = var.value\n",
    "        var.value = var.value + eps\n",
    "        tweaked_func_eval = func.evaluate()\n",
    "        var.value = original_value\n",
    "        derivative = (tweaked_func_eval - base_func_eval) / eps\n",
    "        partial_derivatives.append(derivative)\n",
    "    return partial_derivatives\n",
    "\n",
    "df_dx, df_dy = gradients(f, [x, y])\n",
    "print(\"df/dx(3,4) =\", df_dx)\n",
    "print(\"df/dy(3,4) =\", df_dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbolic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx(3,4) = 24.0\n",
      "df/dy(3,4) = 10.0\n"
     ]
    }
   ],
   "source": [
    "Const.derive = lambda self, var: Const(0)\n",
    "Var.derive = lambda self, var: Const(1) if self is var else Const(0)\n",
    "Add.derive = lambda self, var: Add(self.a.derive(var), self.b.derive(var))\n",
    "Mul.derive = lambda self, var: Add(Mul(self.a, self.b.derive(var)), Mul(self.a.derive(var), self.b))\n",
    "\n",
    "x = Var(3.0, name=\"x\")\n",
    "y = Var(4.0, name=\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = x²y + y + 2\n",
    "\n",
    "df_dx = f.derive(x)  # 2xy\n",
    "df_dy = f.derive(y)  # x² + 1\n",
    "print(\"df/dx(3,4) =\", df_dx.evaluate())\n",
    "print(\"df/dy(3,4) =\", df_dy.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation (autodiff) – forward mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualNumber(object):\n",
    "    def __init__(self, value=0.0, eps=0.0):\n",
    "        self.value = value\n",
    "        self.eps = eps\n",
    "    def __add__(self, b):\n",
    "        return DualNumber(self.value + self.to_dual(b).value,\n",
    "                          self.eps + self.to_dual(b).eps)\n",
    "    def __radd__(self, a):\n",
    "        return self.to_dual(a).__add__(self)\n",
    "    def __mul__(self, b):\n",
    "        return DualNumber(self.value * self.to_dual(b).value,\n",
    "                          self.eps * self.to_dual(b).value + self.value * self.to_dual(b).eps)\n",
    "    def __rmul__(self, a):\n",
    "        return self.to_dual(a).__mul__(self)\n",
    "    def __str__(self):\n",
    "        if self.eps:\n",
    "            return \"{:.1f} + {:.1f}ε\".format(self.value, self.eps)\n",
    "        else:\n",
    "            return \"{:.1f}\".format(self.value)\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "    @classmethod\n",
    "    def to_dual(cls, n):\n",
    "        if hasattr(n, \"value\"):\n",
    "            return n\n",
    "        else:\n",
    "            return cls(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3 + (3 + 4 \\epsilon) = 6 + 4\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0 + 4.0ε"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 + DualNumber(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(3 + 4ε)\\times(5 + 7ε) = 3 \\times 5 + 3 \\times 7ε + 4ε \\times 5 + 4ε \\times 7ε = 15 + 21ε + 20ε + 28ε^2 = 15 + 41ε + 28 \\times 0 = 15 + 41ε$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0 + 41.0ε"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DualNumber(3, 4) * DualNumber(5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.value = DualNumber(3.0)\n",
    "y.value = DualNumber(4.0)\n",
    "\n",
    "f.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.value = DualNumber(3.0, 1.0)  # 3 + ε\n",
    "y.value = DualNumber(4.0)       # 4\n",
    "\n",
    "df_dx = f.evaluate().eps\n",
    "\n",
    "x.value = DualNumber(3.0)       # 3\n",
    "y.value = DualNumber(4.0, 1.0)  # 4 + ε\n",
    "\n",
    "df_dy = f.evaluate().eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff – Reverse mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x,y) = ((x) * (x)) * (y) + y + 2\n",
      "f(3,4) = 42\n",
      "df_dx = 24.0\n",
      "df_dy = 10.0\n"
     ]
    }
   ],
   "source": [
    "class Const(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        pass\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var(object):\n",
    "    def __init__(self, init_value, name):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "        self.gradient = 0\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.gradient += gradient\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() + self.b.evaluate()\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.a.backpropagate(gradient)\n",
    "        self.b.backpropagate(gradient)\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() * self.b.evaluate()\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.a.backpropagate(gradient * self.b.value)\n",
    "        self.b.backpropagate(gradient * self.a.value)\n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)\n",
    "\n",
    "x = Var(3, name=\"x\")\n",
    "y = Var(4, name=\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = x²y + y + 2\n",
    "\n",
    "result = f.evaluate()\n",
    "f.backpropagate(1.0)\n",
    "\n",
    "print(\"f(x,y) =\", f)\n",
    "print(\"f(3,4) =\", result)\n",
    "print(\"df_dx =\", x.gradient)\n",
    "print(\"df_dy =\", y.gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff – reverse mode (using TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42.0, [24.0, 10.0])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "x = tf.Variable(3., name=\"x\")\n",
    "y = tf.Variable(4., name=\"y\")\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "gradients = tf.gradients(f, [x, y])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    f_val, gradients_val = sess.run([f, gradients])\n",
    "\n",
    "f_val, gradients_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Logistic Regression with Mini-Batch Gradient Descent using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create the moons dataset using Scikit-Learn's `make_moons()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "m = 1000\n",
    "X_moons, y_moons = make_moons(m, noise=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_moons[y_moons == 1, 0], X_moons[y_moons == 1, 1], 'go', label=\"Positive\")\n",
    "plt.plot(X_moons[y_moons == 0, 0], X_moons[y_moons == 0, 1], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must not forget to add an extra bias feature ($x_0 = 1$) to every instance. For this, we just need to add a column full of 1s on the left of the input matrix $\\mathbf{X}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moons_with_bias = np.c_[np.ones((m, 1)), X_moons]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.05146968,  0.44419863],\n",
       "       [ 1.        ,  1.03201691, -0.41974116],\n",
       "       [ 1.        ,  0.86789186, -0.25482711],\n",
       "       [ 1.        ,  0.288851  , -0.44866862],\n",
       "       [ 1.        , -0.83343911,  0.53505665]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_moons_with_bias[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Now let's reshape `y_train` to make it a column vector (i.e. a 2D array with a single column):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_moons_column_vector = y_moons.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into a training set and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "test_size = int(m * test_ratio)\n",
    "\n",
    "X_train = X_moons_with_bias[:-test_size]\n",
    "X_test = X_moons_with_bias[-test_size:]\n",
    "y_train = y_moons_column_vector[:-test_size]\n",
    "y_test = y_moons_column_vector[-test_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's create a small function to generate training batches. In this implementation we will just pick random instances from the training set for each batch. This means that a single batch may contain the same instance multiple times, and also a single epoch may not cover all the training instances (in fact it will generally cover only about two thirds of the instances). However, in practice this is not an issue and it simplifies the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X_train, y_train, batch_size):\n",
    "    rnd_indices = np.random.randint(0, len(X_train), batch_size)\n",
    "    X_batch = X_train[rnd_indices]\n",
    "    y_batch = y_train[rnd_indices]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a small batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.93189866,  0.13158788],\n",
       "       [ 1.        ,  1.07172763,  0.13482039],\n",
       "       [ 1.        , -1.01148674, -0.04686381],\n",
       "       [ 1.        ,  0.02201868,  0.19079139],\n",
       "       [ 1.        , -0.98941204,  0.02473116]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = random_batch(X_train, y_train, 5)\n",
    "X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that the data is ready to be fed to the model, we need to build that model. Let's start with a simple implementation, then we will add all the bells and whistles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's reset the default graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _moons_ dataset has two input features, since each instance is a point on a plane (i.e., 2-Dimensional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the Logistic Regression model. As we saw in chapter 4, this model first computes a weighted sum of the inputs (just like the Linear Regression model), and then it applies the sigmoid function to the result, which gives us the estimated probability for the positive class:\n",
    "\n",
    "$\\hat{p} = h_\\mathbf{\\theta}(\\mathbf{x}) = \\sigma(\\mathbf{\\theta}^T \\cdot \\mathbf{x})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $\\mathbf{\\theta}$ is the parameter vector, containing the bias term $\\theta_0$ and the weights $\\theta_1, \\theta_2, \\dots, \\theta_n$. The input vector $\\mathbf{x}$ contains a constant term $x_0 = 1$, as well as all the input features $x_1, x_2, \\dots, x_n$.\n",
    "\n",
    "Since we want to be able to make predictions for multiple instances at a time, we will use an input matrix $\\mathbf{X}$ rather than a single input vector. The $i^{th}$ row will contain the transpose of the $i^{th}$ input vector $(\\mathbf{x}^{(i)})^T$. It is then possible to estimate the probability that each instance belongs to the positive class using the following equation:\n",
    "\n",
    "$ \\hat{\\mathbf{p}} = \\sigma(\\mathbf{X} \\cdot \\mathbf{\\theta})$\n",
    "\n",
    "That's all we need to build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n_inputs + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "logits = tf.matmul(X, theta, name=\"logits\")\n",
    "y_proba = 1 / (1 + tf.exp(-logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, TensorFlow has a nice function `tf.sigmoid()` that we can use to simplify the last line of the previous code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = tf.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in chapter 4, the log loss is a good cost function to use for Logistic Regression:\n",
    "\n",
    "$J(\\mathbf{\\theta}) = -\\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{\\left[ y^{(i)} log\\left(\\hat{p}^{(i)}\\right) + (1 - y^{(i)}) log\\left(1 - \\hat{p}^{(i)}\\right)\\right]}$\n",
    "\n",
    "One option is to implement it ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-7 # to avoid an overflow when computing the log\n",
    "loss = -tf.reduce_mean(y * tf.log(y_proba + epsilon) + (1 - y) * tf.log(1 - y_proba + epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we might as well use TensorFlow's `tf.losses.log_loss()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.log_loss(labels=y, predictions=y_proba) # uses epsilon = 1e-7 by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is pretty standard: let's create the optimizer and tell it to minimize the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learnign_rate)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need now (in this minimal version) is the variable initializer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are ready to train the model and use it for predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's really nothing special about this code, it's virtually the same as the one we used earlier for Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 0.5902034\n",
      "Epoch: 100 \tLoss: 0.2744875\n",
      "Epoch: 200 \tLoss: 0.27286732\n",
      "Epoch: 300 \tLoss: 0.27538362\n",
      "Epoch: 400 \tLoss: 0.27652404\n",
      "Epoch: 500 \tLoss: 0.273172\n",
      "Epoch: 600 \tLoss: 0.2733476\n",
      "Epoch: 700 \tLoss: 0.274197\n",
      "Epoch: 800 \tLoss: 0.27484858\n",
      "Epoch: 900 \tLoss: 0.27415654\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 100 == 0:\n",
    "            loss_val = loss.eval({X: X_test, y: y_test})\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)\n",
    "        \n",
    "    y_proba_val = y_proba.eval(feed_dict={X: X_test, y: y_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we don't use the epoch number when generating batches, so we could just have a single `for` loop rather than 2 nested `for` loops, but it's convenient to think of training time in terms of number of epochs (i.e., roughly the number of times the algorithm went through the training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each instance in the test set, `y_proba_val` contains the estimated probability that it belongs to the positive class, according to the model. For example, here are the first 5 estimated probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5857171 ],\n",
       "       [0.6939736 ],\n",
       "       [0.55537176],\n",
       "       [0.9957288 ],\n",
       "       [0.50255686]], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba_val[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify each instance, we can go for maximum likelihood: classify as positive any instance whose estimated probability is greater or equal to 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (y_proba_val >= 0.5)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the use case, you may want to choose a different threshold than 0.5: make it higher if you want high precision (but lower recall), and make it lower if you want high recall (but lower precision). See chapter 3 for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the model's precision and recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8425925925925926"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9191919191919192"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these predictions to see what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_idx = y_pred.reshape(-1) # a 1D array rather than a column vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_test[y_pred_idx, 1], X_test[y_pred_idx, 2], 'go', label=\"Positive\")\n",
    "plt.plot(X_test[~y_pred_idx, 1], X_test[~y_pred_idx, 2], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that looks pretty bad, doesn't it? But let's not forget that the Logistic Regression model has a linear decision boundary, so this is actually close to the best we can do with this model (unless we add more features, as we will show in a second)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start over, but this time we will add all the bells and whistles, as listed in the exercise:\n",
    "* Define the graph within a `logistic_regression()` function that can be reused easily.\n",
    "* Save checkpoints using a `Saver` at regular intervals during training, and save the final model at the end of training.\n",
    "* Restore the last checkpoint upon startup if training was interrupted.\n",
    "* Define the graph using nice scopes so the graph looks good in TensorBoard.\n",
    "* Add summaries to visualize the learning curves in TensorBoard.\n",
    "* Try tweaking some hyperparameters such as the learning rate or the mini-batch size and look at the shape of the learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we will add 4 more features to the inputs: ${x_1}^2$, ${x_2}^2$, ${x_1}^3$ and ${x_2}^3$. This was not part of the exercise, but it will demonstrate how adding features can improve the model. We will do this manually, but you could also add them using `sklearn.preprocessing.PolynomialFeatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enhanced = np.c_[X_train,\n",
    "                         np.square(X_train[:, 1]),\n",
    "                         np.square(X_train[:, 2]),\n",
    "                         X_train[:, 1] ** 3,\n",
    "                         X_train[:, 2] ** 3]\n",
    "X_test_enhanced = np.c_[X_test,\n",
    "                        np.square(X_test[:, 1]),\n",
    "                        np.square(X_test[:, 2]),\n",
    "                        X_test[:, 1] ** 3,\n",
    "                        X_test[:, 2] ** 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the \"enhanced\" training set looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 3), (800, 7))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_train_enhanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00, -5.14696757e-02,  4.44198631e-01,\n",
       "         2.64912752e-03,  1.97312424e-01, -1.36349734e-04,\n",
       "         8.76459084e-02],\n",
       "       [ 1.00000000e+00,  1.03201691e+00, -4.19741157e-01,\n",
       "         1.06505890e+00,  1.76182639e-01,  1.09915879e+00,\n",
       "        -7.39511049e-02],\n",
       "       [ 1.00000000e+00,  8.67891864e-01, -2.54827114e-01,\n",
       "         7.53236288e-01,  6.49368582e-02,  6.53727646e-01,\n",
       "        -1.65476722e-02],\n",
       "       [ 1.00000000e+00,  2.88850997e-01, -4.48668621e-01,\n",
       "         8.34348982e-02,  2.01303531e-01,  2.41002535e-02,\n",
       "        -9.03185778e-02],\n",
       "       [ 1.00000000e+00, -8.33439108e-01,  5.35056649e-01,\n",
       "         6.94620746e-01,  2.86285618e-01, -5.78924095e-01,\n",
       "         1.53179024e-01]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_enhanced[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, next let's reset the default graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the `logistic_regression()` function to create the graph. We will leave out the definition of the inputs `X` and the targets `y`. We could include them here, but leaving them out will make it easier to use this function in a wide range of use cases (e.g. perhaps we will want to add some preprocessing steps for the inputs before we feed them to the Logistic Regression model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, initializer=None, seed=42, learning_rate=0.01):\n",
    "    n_inputs_including_bias = int(X.get_shape()[1])\n",
    "    \n",
    "    with tf.name_scope(\"logistic_regression\"):\n",
    "        with tf.name_scope(\"model\"):\n",
    "            if initializer is None:\n",
    "                initializer = tf.random_uniform([n_inputs_including_bias, 1], -1.0, 1.0, seed=seed)\n",
    "            theta = tf.Variable(initializer, name=\"theta\")\n",
    "            logits = tf.matmul(X, theta, name=\"logits\")\n",
    "            y_proba = tf.sigmoid(logits)\n",
    "        with tf.name_scope(\"train\"):\n",
    "            loss = tf.losses.log_loss(y, y_proba, scope=\"loss\")\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learnign_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "            loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "        with tf.name_scope(\"init\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "        with tf.name_scope(\"save\"):\n",
    "            saver = tf.train.Saver()\n",
    "    return y_proba, loss, training_op, loss_summary, init, saver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a little function to get the name of the log directory to save the summaries for Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"./tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create the graph, using the `logistic_regression()` function. We will also create the `FileWriter` to save the summaries to the log directory for Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 2 + 4\n",
    "logdir = log_dir(\"logreg\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(X, y)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last we can train the model! We will start by checking whether a previous training session was interrupted, and if so we will load the checkpoint and continue training from the epoch number we saved. In this example we just save the epoch number to a separate file, but in chapter 11 we will see how to store the training step directly as part of the model, using a non-trainable variable called `global_step` that we pass to the optimizer's `minimize()` method.\n",
    "\n",
    "You can try interrupting training to verify that it does indeed restore the last checkpoint when you start it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "checkpoint_path = \"./tmp/my_logreg_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./tmp/my_logreg_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 0.49356884\n",
      "Epoch: 500 \tLoss: 0.051719293\n",
      "Epoch: 1000 \tLoss: 0.037049428\n",
      "Epoch: 1500 \tLoss: 0.031261176\n",
      "Epoch: 2000 \tLoss: 0.027881632\n",
      "Epoch: 2500 \tLoss: 0.025845151\n",
      "Epoch: 3000 \tLoss: 0.024269294\n",
      "Epoch: 3500 \tLoss: 0.02297319\n",
      "Epoch: 4000 \tLoss: 0.022252217\n",
      "Epoch: 4500 \tLoss: 0.02126216\n",
      "Epoch: 5000 \tLoss: 0.020547427\n",
      "Epoch: 5500 \tLoss: 0.019943507\n",
      "Epoch: 6000 \tLoss: 0.019625125\n",
      "Epoch: 6500 \tLoss: 0.019050729\n",
      "Epoch: 7000 \tLoss: 0.01884734\n",
      "Epoch: 7500 \tLoss: 0.018401051\n",
      "Epoch: 8000 \tLoss: 0.018004239\n",
      "Epoch: 8500 \tLoss: 0.017799886\n",
      "Epoch: 9000 \tLoss: 0.017415853\n",
      "Epoch: 9500 \tLoss: 0.017347405\n",
      "Epoch: 10000 \tLoss: 0.017032422\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        file_writer.add_summary(summary_str, global_step=epoch)\n",
    "        if epoch % 500 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "    \n",
    "    saver.save(sess, final_model_path)\n",
    "    y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "    os.remove(checkpoint_epoch_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can make predictions by just classifying as positive all the instances whose estimated probability is greater or equal to 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_proba_val >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9801980198019802"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_idx = y_pred.reshape(-1) # a 1D array rather than a column vector\n",
    "\n",
    "plt.plot(X_test[y_pred_idx, 1], X_test[y_pred_idx, 2], 'go', label=\"Positive\")\n",
    "plt.plot(X_test[~y_pred_idx, 1], X_test[~y_pred_idx, 2], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's much, much better! Apparently the new features really helped a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try starting the tensorboard server, find the latest run and look at the learning curve (i.e., how the loss evaluated on the test set evolves as a function of the epoch number):\n",
    "\n",
    "```\n",
    "$ tensorboard --logdir=tf_logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can play around with the hyperparameters (e.g. the `batch_size` or the `learning_rate`) and run training again and again, comparing the learning curves. You can even automate this process by implementing grid search or randomized search. Below is a simple implementation of a randomized search on both the batch size and the learning rate. For the sake of simplicity, the checkpoint mechanism was removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "  logdir: ./tf_logs/logreg-run-20180501092619/\n",
      "  batch size: 50\n",
      "  learning_rate: 0.004430375245218265\n",
      "  training: .....................\n",
      "  precision: 0.9801980198019802\n",
      "  recall: 1.0\n",
      "Iteration 1\n",
      "  logdir: ./tf_logs/logreg-run-20180501092853/\n",
      "  batch size: 54\n",
      "  learning_rate: 0.0017826497151386947\n",
      "  training: .....................\n",
      "  precision: 0.9801980198019802\n",
      "  recall: 1.0\n",
      "Iteration 2\n",
      "  logdir: ./tf_logs/logreg-run-20180501093118/\n",
      "  batch size: 22\n",
      "  learning_rate: 0.00203228544324115\n",
      "  training: .....................\n",
      "  precision: 0.9801980198019802\n",
      "  recall: 1.0\n",
      "Iteration 3\n",
      "  logdir: ./tf_logs/logreg-run-20180501093633/\n",
      "  batch size: 74\n",
      "  learning_rate: 0.004491523825137997\n",
      "  training: .....................\n",
      "  precision: 0.9801980198019802\n",
      "  recall: 1.0\n",
      "Iteration 4\n",
      "  logdir: ./tf_logs/logreg-run-20180501093814/\n",
      "  batch size: 58\n",
      "  learning_rate: 0.07963234721775589\n",
      "  training: .....................\n",
      "  precision: 0.9801980198019802\n",
      "  recall: 1.0\n",
      "Iteration 5\n",
      "  logdir: ./tf_logs/logreg-run-20180501094023/\n",
      "  batch size: 61\n",
      "  learning_rate: 0.0004634250583294876\n",
      "  training: .....................\n",
      "  precision: 0.9801980198019802\n",
      "  recall: 1.0\n",
      "Iteration 6\n",
      "  logdir: ./tf_logs/logreg-run-20180501094224/\n",
      "  batch size: 92\n",
      "  learning_rate: 0.047706818419354494\n",
      "  training: .....................\n",
      "  precision: 0.9801980198019802\n",
      "  recall: 1.0\n",
      "Iteration 7\n",
      "  logdir: ./tf_logs/logreg-run-20180501094348/\n",
      "  batch size: 74\n",
      "  learning_rate: 0.0001694044709524274\n",
      "  training: .....................\n",
      "  precision: 0.9801980198019802\n",
      "  recall: 1.0\n",
      "Iteration 8\n",
      "  logdir: ./tf_logs/logreg-run-20180501094542/\n",
      "  batch size: 58\n",
      "  learning_rate: 0.04171461199412461\n",
      "  training: .....................\n",
      "  precision: 0.9801980198019802\n",
      "  recall: 1.0\n",
      "Iteration 9\n",
      "  logdir: ./tf_logs/logreg-run-20180501094800/\n",
      "  batch size: 61\n",
      "  learning_rate: 0.00010742922968438615\n",
      "  training: .....................\n",
      "  precision: 0.9801980198019802\n",
      "  recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "\n",
    "n_search_iterations = 10\n",
    "\n",
    "for search_iteration in range(n_search_iterations):\n",
    "    batch_size = np.random.randint(1, 100)\n",
    "    learning_rate = reciprocal(0.0001, 0.1).rvs(random_state=search_iteration)\n",
    "\n",
    "    n_inputs = 2 + 4\n",
    "    logdir = log_dir(\"logreg\")\n",
    "    \n",
    "    print(\"Iteration\", search_iteration)\n",
    "    print(\"  logdir:\", logdir)\n",
    "    print(\"  batch size:\", batch_size)\n",
    "    print(\"  learning_rate:\", learning_rate)\n",
    "    print(\"  training: \", end=\"\")\n",
    "\n",
    "    reset_graph()\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "    \n",
    "    y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(X, y, learning_rate=learning_rate)\n",
    "    \n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "    \n",
    "    n_epochs = 10001\n",
    "    n_batches = int(np.ceil(m / batch_size))\n",
    "    \n",
    "    final_model_path = \"./tmp/my_logreg_model.%d\" % search_iteration\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            for batch_index in range(n_batches):\n",
    "                X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\n",
    "            file_writer.add_summary(summary_str, epoch)\n",
    "            if epoch % 500 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "            \n",
    "        saver.save(sess, final_model_path)\n",
    "        \n",
    "        print()\n",
    "        y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        y_pred = (y_proba_val >= 0.5)\n",
    "        \n",
    "        print(\"  precision:\", precision_score(y_test, y_pred))\n",
    "        print(\"  recall:\", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reciprocal()` function from SciPy's `stats` module returns a random distribution that is commonly used when you have no idea of the optimal scale of a hyperparameter. See the exercise solutions for chapter 2 for more details. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
