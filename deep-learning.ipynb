{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dnn-config](https://www.dropbox.com/s/77hfezzpek1n00y/default-dnn-config.png?dl=1)\n",
    "\n",
    "This default configuration may need to be tweaked:\n",
    "- If you can’t find a good learning rate (convergence was too slow, so you increased the training rate, and now convergence is fast but the network’s accuracy is suboptimal), then you can try adding a learning schedule such as exponential decay.\n",
    "- If your training set is a bit too small, you can implement data augmentation. If you need a sparse model, you can add some ℓ1 regularization to the mix (and optionally zero out the tiny weights after training).\n",
    "- If you need an even sparser model, you can try using FTRL instead of Adam optimization, along with ℓ1 regularization.\n",
    "- If you need a lightning-fast model at runtime, you may want to drop Batch Normalization, and possibly replace the ELU activation function with the leaky ReLU. Having a sparse model will also help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradients Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution. This is called the **vanishing gradients** problem.\n",
    "\n",
    "In some cases, the opposite can happen: the gradients can grow bigger and bigger, so many layers get insanely large weight updates and the algorithm diverges. This is the **exploding gradients** problem, which is mostly encountered in recurrent neural networks. More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the book uses `tensorflow.contrib.layers.fully_connected()` rather than `tf.layers.dense()` (which did not exist when this chapter was written). It is now preferable to use `tf.layers.dense()`, because anything in the contrib module may change or be deleted without notice. The `dense()` function is almost identical to the `fully_connected()` function. The main differences relevant to this chapter are:\n",
    "* several parameters are renamed: `scope` becomes `name`, `activation_fn` becomes `activation` (and similarly the `_fn` suffix is removed from other parameters such as `normalizer_fn`), `weights_initializer` becomes `kernel_initializer`, etc.\n",
    "* the default `activation` is now `None` rather than `tf.nn.relu`.\n",
    "* it does not support `tensorflow.contrib.framework.arg_scope()` (introduced later in chapter 11).\n",
    "* it does not support regularizer params (introduced later in chapter 11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28 # MNIST\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_init = tf.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, kernel_initializer=he_init, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the *ReLU* activation function is not perfect. It suffers from a problem known as the *dying ReLUs*: during training, some neurons effectively die, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network’s neurons are dead, especially if you used a large learning rate. During training, if a neuron’s weights get updated such that the weighted sum of the neuron’s inputs is negative, it will start outputting 0. When this happens, the neuron is unlikely to come back to life since the gradient of the *ReLU* function is 0 when its input is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP**\n",
    "\n",
    "So which activation function should you use for the hidden layers of your deep neural networks?\n",
    "\n",
    "Although your mileage will vary, in general `ELU` > `leaky ReLU` (and its variants) > `ReLU` > `tanh` > `logistic`.\n",
    "\n",
    "- If you care a lot about runtime performance, then you may prefer `leaky ReLUs` over `ELUs`.\n",
    "- If you don’t want to tweak yet another hyperparameter, you may just use the default `α` values suggested earlier (0.01 for the `leaky ReLU`, and 1 for `ELU`).\n",
    "- If you have spare time and computing power, you can use *cross-validation* to evaluate other activation functions, in particular `RReLU` if your network is overfitting, or `PReLU` if you have a huge training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha * z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Leaky ReLU in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a neural network on MNIST using the Leaky ReLU. First let's create the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28 # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.86 Validation accuracy: 0.9044\n",
      "5 Batch accuracy: 0.94 Validation accuracy: 0.9496\n",
      "10 Batch accuracy: 0.92 Validation accuracy: 0.9654\n",
      "15 Batch accuracy: 0.94 Validation accuracy: 0.971\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9764\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9778\n",
      "30 Batch accuracy: 0.98 Validation accuracy: 0.978\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.9788\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z, alpha=0.01):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This activation function was proposed in this [great paper](https://arxiv.org/pdf/1706.02515.pdf) by Günter Klambauer, Thomas Unterthiner and Andreas Mayr, published in June 2017 (I will definitely add it to the book). It outperforms the other activation functions very significantly for deep neural networks, so you should really try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this activation function, even a 100 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: -0.26 < mean < 0.27, 0.74 < std deviation < 1.27\n",
      "Layer 10: -0.24 < mean < 0.27, 0.74 < std deviation < 1.27\n",
      "Layer 20: -0.17 < mean < 0.18, 0.74 < std deviation < 1.24\n",
      "Layer 30: -0.27 < mean < 0.24, 0.78 < std deviation < 1.20\n",
      "Layer 40: -0.38 < mean < 0.39, 0.74 < std deviation < 1.25\n",
      "Layer 50: -0.27 < mean < 0.31, 0.73 < std deviation < 1.27\n",
      "Layer 60: -0.26 < mean < 0.43, 0.74 < std deviation < 1.35\n",
      "Layer 70: -0.19 < mean < 0.21, 0.75 < std deviation < 1.21\n",
      "Layer 80: -0.18 < mean < 0.16, 0.72 < std deviation < 1.19\n",
      "Layer 90: -0.19 < mean < 0.16, 0.75 < std deviation < 1.20\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100))\n",
    "\n",
    "for layer in range(100):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1/100))\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=1)\n",
    "    stds = np.std(Z, axis=1)\n",
    "    \n",
    "    if layer % 10 == 0:\n",
    "        print(\"Layer {}: {:.2f} < mean < {:.2f}, {:.2f} < std deviation < {:.2f}\".format(\n",
    "            layer, means.min(), means.max(), stds.min(), stds.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a TensorFlow implementation (there will almost certainly be a `tf.nn.selu()` function in future TensorFlow versions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELUs can also be combined with dropout, check out [this implementation](https://github.com/bioinf-jku/SNNs/blob/master/selu.py) by the Institute of Bioinformatics, Johannes Kepler University Linz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a neural net for MNIST using the SELU activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train it. Do not forget to scale the inputs to mean 0 and standard deviation 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.88 Validation accuracy: 0.923\n",
      "5 Batch accuracy: 0.98 Validation accuracy: 0.9578\n",
      "10 Batch accuracy: 1.0 Validation accuracy: 0.9664\n",
      "15 Batch accuracy: 0.96 Validation accuracy: 0.9682\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9694\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9688\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.9694\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "means = X_train.mean(axis=0, keepdims=True)\n",
    "stds = X_train.std(axis=0, keepdims=True) + 1e-10\n",
    "X_val_scaled = (X_valid - means) / stds\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_val_scaled, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final_selu.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a [2015 paper](https://arxiv.org/pdf/1502.03167v3.pdf), 7 Sergey Ioffe and Christian Szegedy proposed a technique called *Batch Normalization* (BN) to address the vanishing/exploding gradients problems, and more generally the problem that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change (which they call the *Internal Covariate Shift* problem).\n",
    "\n",
    "The technique consists of adding an operation in the model just before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling and shifting the result using two new parameters per layer (one for scaling, the other for shifting). In other words, this operation lets the model learn the optimal scale and mean of the inputs for each layer.\n",
    "\n",
    "In order to zero-center and normalize the inputs, the algorithm needs to estimate the inputs’ mean and standard deviation. It does so by evaluating the mean and standard deviation of the inputs over the current mini-batch (hence the name “Batch Normalization”)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BN](https://www.dropbox.com/s/rmjoencidkkshu3/batch-normalization.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the book uses `tensorflow.contrib.layers.batch_norm()` rather than `tf.layers.batch_normalization()` (which did not exist when this chapter was written). It is now preferable to use `tf.layers.batch_normalization()`, because anything in the contrib module may change or be deleted without notice. Instead of using the `batch_norm()` function as a regularizer parameter to the `fully_connected()` function, we now use `batch_normalization()` and we explicitly create a distinct layer. The parameters are a bit different, in particular:\n",
    "* `decay` is renamed to `momentum`,\n",
    "* `is_training` is renamed to `training`,\n",
    "* `updates_collections` is removed: the update operations needed by batch normalization are added to the `UPDATE_OPS` collection and you need to explicity run these operations during training (see the execution phase below),\n",
    "* we don't need to specify `scale=True`, as that is the default.\n",
    "\n",
    "Also note that in order to run batch norm just _before_ each hidden layer's activation function, we apply the ELU activation function manually, right after the batch norm layer.\n",
    "\n",
    "Note: since the `tf.layers.dense()` function is incompatible with `tf.contrib.layers.arg_scope()` (which is used in the book), we now use python's `functools.partial()` function instead. It makes it easy to create a `my_dense_layer()` function that just calls `tf.layers.dense()` with the desired parameters automatically set (unless they are overridden when calling `my_dense_layer()`). As you can see, the code remains very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid repeating the same parameters over and over again, we can use Python's `partial()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a neural net for MNIST, using the ELU activation function and Batch Normalization at each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "    \n",
    "    batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=batch_norm_momentum)\n",
    "    dense_layer = partial(tf.layers.dense, kernel_initializer=he_init)\n",
    "    \n",
    "    hidden1 = dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(batch_norm_layer(hidden1))\n",
    "    hidden2 = dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(batch_norm_layer(hidden2))\n",
    "    logits_before_bn = dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = batch_norm_layer(logits_before_bn)\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: since we are using `tf.layers.batch_normalization()` rather than `tf.contrib.layers.batch_norm()` (as in the book), we need to explicitly run the extra update operations needed by batch normalization (`sess.run([training_op, extra_update_ops],...`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9042\n",
      "1 Validation accuracy: 0.928\n",
      "2 Validation accuracy: 0.9374\n",
      "3 Validation accuracy: 0.9474\n",
      "4 Validation accuracy: 0.9532\n",
      "5 Validation accuracy: 0.9572\n",
      "6 Validation accuracy: 0.9626\n",
      "7 Validation accuracy: 0.9628\n",
      "8 Validation accuracy: 0.9664\n",
      "9 Validation accuracy: 0.968\n",
      "10 Validation accuracy: 0.9694\n",
      "11 Validation accuracy: 0.9696\n",
      "12 Validation accuracy: 0.971\n",
      "13 Validation accuracy: 0.971\n",
      "14 Validation accuracy: 0.9728\n",
      "15 Validation accuracy: 0.9734\n",
      "16 Validation accuracy: 0.9728\n",
      "17 Validation accuracy: 0.975\n",
      "18 Validation accuracy: 0.9752\n",
      "19 Validation accuracy: 0.976\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What!? That's not a great accuracy for MNIST. Of course, if you train for longer it will get much better accuracy, but with such a shallow network, Batch Norm and ELU are unlikely to have very positive impact: they shine mostly for much deeper nets.\n",
    "\n",
    "Note that you could also make the training operation depend on the update operations:\n",
    "\n",
    "```python\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        training_op = optimizer.minimize(loss)\n",
    "```\n",
    "\n",
    "This way, you would just have to evaluate the `training_op` during training, TensorFlow would automatically run the update operations as well:\n",
    "\n",
    "```python\n",
    "sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "```\n",
    "\n",
    "\n",
    "One more thing: notice that the list of trainable variables is shorter than the list of all global variables. This is because the moving averages are non-trainable variables. If you want to reuse a pretrained neural network (see below), you must not forget these non-trainable variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/moving_mean:0',\n",
       " 'batch_normalization/moving_variance:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/moving_mean:0',\n",
       " 'batch_normalization_1/moving_variance:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/moving_mean:0',\n",
       " 'batch_normalization_2/moving_variance:0']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.global_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular technique to lessen the exploding gradients problem is to simply clip the gradients during backpropagation so that they never exceed some threshold. This is called *Gradient Clipping*. In general people now prefer *Batch Normalization*, but it’s still useful to know about *Gradient Clipping* and how to implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple neural net for MNIST and add gradient clipping. The first part is the same as earlier (except we added a few more layers to demonstrate reusing pretrained models, see below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply gradient clipping. For this, we need to get the gradients, use the `clip_by_value()` function to clip them, then apply them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.288\n",
      "1 Validation accuracy: 0.7936\n",
      "2 Validation accuracy: 0.8798\n",
      "3 Validation accuracy: 0.906\n",
      "4 Validation accuracy: 0.9164\n",
      "5 Validation accuracy: 0.9218\n",
      "6 Validation accuracy: 0.9296\n",
      "7 Validation accuracy: 0.9358\n",
      "8 Validation accuracy: 0.9382\n",
      "9 Validation accuracy: 0.9414\n",
      "10 Validation accuracy: 0.9456\n",
      "11 Validation accuracy: 0.9474\n",
      "12 Validation accuracy: 0.9478\n",
      "13 Validation accuracy: 0.9534\n",
      "14 Validation accuracy: 0.9568\n",
      "15 Validation accuracy: 0.9566\n",
      "16 Validation accuracy: 0.9574\n",
      "17 Validation accuracy: 0.959\n",
      "18 Validation accuracy: 0.9622\n",
      "19 Validation accuracy: 0.9612\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "        \n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is generally not a good idea to train a very large DNN from scratch: instead, you should always try to find an existing neural network that accomplishes a similar task to the one you are trying to tackle, then just reuse the lower layers of this network: this is called transfer learning. It will not only speed up training considerably, but will also require much less training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing a TensorFlow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the original model was trained using TensorFlow, you can simply restore it and train it on the new task.\n",
    "\n",
    "First you need to load the graph's structure. The `import_meta_graph()` function does just that, loading the graph's operations into the default graph, and returning a `Saver` that you can then use to restore the model's state. Note that by default, a `Saver` saves the structure of the graph into a `.meta` file, so that's the file you should load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you need to get a handle on all the operations you will need for training. If you don't know the graph's structure, you can list all the operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2/tensor_names\n",
      "save/Const\n",
      "save/RestoreV2\n",
      "eval/Const\n",
      "eval/in_top_k/InTopKV2/k\n",
      "GradientDescent/learning_rate\n",
      "clip_by_value_11/clip_value_max\n",
      "clip_by_value_11/clip_value_min\n",
      "clip_by_value_10/clip_value_max\n",
      "clip_by_value_10/clip_value_min\n",
      "clip_by_value_9/clip_value_max\n",
      "clip_by_value_9/clip_value_min\n",
      "clip_by_value_8/clip_value_max\n",
      "clip_by_value_8/clip_value_min\n",
      "clip_by_value_7/clip_value_max\n",
      "clip_by_value_7/clip_value_min\n",
      "clip_by_value_6/clip_value_max\n",
      "clip_by_value_6/clip_value_min\n",
      "clip_by_value_5/clip_value_max\n",
      "clip_by_value_5/clip_value_min\n",
      "clip_by_value_4/clip_value_max\n",
      "clip_by_value_4/clip_value_min\n",
      "clip_by_value_3/clip_value_max\n",
      "clip_by_value_3/clip_value_min\n",
      "clip_by_value_2/clip_value_max\n",
      "clip_by_value_2/clip_value_min\n",
      "clip_by_value_1/clip_value_max\n",
      "clip_by_value_1/clip_value_min\n",
      "clip_by_value/clip_value_max\n",
      "clip_by_value/clip_value_min\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "gradients/loss/loss_grad/Maximum/y\n",
      "gradients/loss/loss_grad/Const_1\n",
      "gradients/loss/loss_grad/Const\n",
      "gradients/loss/loss_grad/Shape_2\n",
      "gradients/loss/loss_grad/Prod_1\n",
      "gradients/loss/loss_grad/Maximum\n",
      "gradients/loss/loss_grad/Reshape/shape\n",
      "gradients/grad_ys_0\n",
      "gradients/Shape\n",
      "gradients/Fill\n",
      "gradients/loss/loss_grad/Reshape\n",
      "loss/Const\n",
      "outputs/bias\n",
      "save/Assign_10\n",
      "outputs/bias/read\n",
      "outputs/bias/Initializer/zeros\n",
      "outputs/bias/Assign\n",
      "outputs/kernel\n",
      "save/Assign_11\n",
      "outputs/kernel/read\n",
      "outputs/kernel/Initializer/random_uniform/max\n",
      "outputs/kernel/Initializer/random_uniform/min\n",
      "outputs/kernel/Initializer/random_uniform/sub\n",
      "outputs/kernel/Initializer/random_uniform/shape\n",
      "outputs/kernel/Initializer/random_uniform/RandomUniform\n",
      "outputs/kernel/Initializer/random_uniform/mul\n",
      "outputs/kernel/Initializer/random_uniform\n",
      "outputs/kernel/Assign\n",
      "hidden5/bias\n",
      "save/Assign_8\n",
      "hidden5/bias/read\n",
      "hidden5/bias/Initializer/zeros\n",
      "hidden5/bias/Assign\n",
      "hidden5/kernel\n",
      "save/Assign_9\n",
      "hidden5/kernel/read\n",
      "hidden5/kernel/Initializer/random_uniform/max\n",
      "hidden5/kernel/Initializer/random_uniform/min\n",
      "hidden5/kernel/Initializer/random_uniform/sub\n",
      "hidden5/kernel/Initializer/random_uniform/shape\n",
      "hidden5/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden5/kernel/Initializer/random_uniform/mul\n",
      "hidden5/kernel/Initializer/random_uniform\n",
      "hidden5/kernel/Assign\n",
      "hidden4/bias\n",
      "save/Assign_6\n",
      "hidden4/bias/read\n",
      "hidden4/bias/Initializer/zeros\n",
      "hidden4/bias/Assign\n",
      "hidden4/kernel\n",
      "save/Assign_7\n",
      "hidden4/kernel/read\n",
      "hidden4/kernel/Initializer/random_uniform/max\n",
      "hidden4/kernel/Initializer/random_uniform/min\n",
      "hidden4/kernel/Initializer/random_uniform/sub\n",
      "hidden4/kernel/Initializer/random_uniform/shape\n",
      "hidden4/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden4/kernel/Initializer/random_uniform/mul\n",
      "hidden4/kernel/Initializer/random_uniform\n",
      "hidden4/kernel/Assign\n",
      "hidden3/bias\n",
      "save/Assign_4\n",
      "hidden3/bias/read\n",
      "hidden3/bias/Initializer/zeros\n",
      "hidden3/bias/Assign\n",
      "hidden3/kernel\n",
      "save/Assign_5\n",
      "hidden3/kernel/read\n",
      "hidden3/kernel/Initializer/random_uniform/max\n",
      "hidden3/kernel/Initializer/random_uniform/min\n",
      "hidden3/kernel/Initializer/random_uniform/sub\n",
      "hidden3/kernel/Initializer/random_uniform/shape\n",
      "hidden3/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden3/kernel/Initializer/random_uniform/mul\n",
      "hidden3/kernel/Initializer/random_uniform\n",
      "hidden3/kernel/Assign\n",
      "hidden2/bias\n",
      "save/Assign_2\n",
      "hidden2/bias/read\n",
      "hidden2/bias/Initializer/zeros\n",
      "hidden2/bias/Assign\n",
      "hidden2/kernel\n",
      "save/Assign_3\n",
      "hidden2/kernel/read\n",
      "hidden2/kernel/Initializer/random_uniform/max\n",
      "hidden2/kernel/Initializer/random_uniform/min\n",
      "hidden2/kernel/Initializer/random_uniform/sub\n",
      "hidden2/kernel/Initializer/random_uniform/shape\n",
      "hidden2/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden2/kernel/Initializer/random_uniform/mul\n",
      "hidden2/kernel/Initializer/random_uniform\n",
      "hidden2/kernel/Assign\n",
      "hidden1/bias\n",
      "save/Assign\n",
      "hidden1/bias/read\n",
      "hidden1/bias/Initializer/zeros\n",
      "hidden1/bias/Assign\n",
      "hidden1/kernel\n",
      "save/Assign_1\n",
      "save/restore_all\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "hidden1/kernel/read\n",
      "hidden1/kernel/Initializer/random_uniform/max\n",
      "hidden1/kernel/Initializer/random_uniform/min\n",
      "hidden1/kernel/Initializer/random_uniform/sub\n",
      "hidden1/kernel/Initializer/random_uniform/shape\n",
      "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden1/kernel/Initializer/random_uniform/mul\n",
      "hidden1/kernel/Initializer/random_uniform\n",
      "hidden1/kernel/Assign\n",
      "init\n",
      "y\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "X\n",
      "dnn/hidden1/MatMul\n",
      "dnn/hidden1/BiasAdd\n",
      "dnn/hidden1/Relu\n",
      "dnn/hidden2/MatMul\n",
      "dnn/hidden2/BiasAdd\n",
      "dnn/hidden2/Relu\n",
      "dnn/hidden3/MatMul\n",
      "dnn/hidden3/BiasAdd\n",
      "dnn/hidden3/Relu\n",
      "dnn/hidden4/MatMul\n",
      "dnn/hidden4/BiasAdd\n",
      "dnn/hidden4/Relu\n",
      "dnn/hidden5/MatMul\n",
      "dnn/hidden5/BiasAdd\n",
      "dnn/hidden5/Relu\n",
      "dnn/outputs/MatMul\n",
      "dnn/outputs/BiasAdd\n",
      "eval/in_top_k/InTopKV2\n",
      "eval/Cast\n",
      "eval/accuracy\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "gradients/zeros_like\n",
      "gradients/loss/loss_grad/Shape_1\n",
      "gradients/loss/loss_grad/Prod\n",
      "gradients/loss/loss_grad/floordiv\n",
      "gradients/loss/loss_grad/Cast\n",
      "gradients/loss/loss_grad/Shape\n",
      "gradients/loss/loss_grad/Tile\n",
      "gradients/loss/loss_grad/truediv\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_11\n",
      "GradientDescent/update_outputs/bias/ApplyGradientDescent\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value_10\n",
      "GradientDescent/update_outputs/kernel/ApplyGradientDescent\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_9\n",
      "GradientDescent/update_hidden5/bias/ApplyGradientDescent\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value_8\n",
      "GradientDescent/update_hidden5/kernel/ApplyGradientDescent\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_7\n",
      "GradientDescent/update_hidden4/bias/ApplyGradientDescent\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value_6\n",
      "GradientDescent/update_hidden4/kernel/ApplyGradientDescent\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_5\n",
      "GradientDescent/update_hidden3/bias/ApplyGradientDescent\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value_4\n",
      "GradientDescent/update_hidden3/kernel/ApplyGradientDescent\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_3\n",
      "GradientDescent/update_hidden2/bias/ApplyGradientDescent\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value_2\n",
      "GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_1\n",
      "GradientDescent/update_hidden1/bias/ApplyGradientDescent\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value\n",
      "GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n",
      "GradientDescent\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
      "loss/loss\n"
     ]
    }
   ],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops, that's a lot of operations! It's much easier to use TensorBoard to visualize the graph. The following hack will allow you to visualize the graph within Jupyter (if it does not work with your browser, you will need to use a `FileWriter` to save the graph and then visualize it in TensorBoard):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works on Chrome, not guaranteed on other browsers\n",
    "\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.3745401188473625&quot;).pbtxt = 'node {\\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;hidden1/bias&quot;\\n        string_val: &quot;hidden1/kernel&quot;\\n        string_val: &quot;hidden2/bias&quot;\\n        string_val: &quot;hidden2/kernel&quot;\\n        string_val: &quot;hidden3/bias&quot;\\n        string_val: &quot;hidden3/kernel&quot;\\n        string_val: &quot;hidden4/bias&quot;\\n        string_val: &quot;hidden4/kernel&quot;\\n        string_val: &quot;hidden5/bias&quot;\\n        string_val: &quot;hidden5/kernel&quot;\\n        string_val: &quot;outputs/bias&quot;\\n        string_val: &quot;outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;hidden1/bias&quot;\\n        string_val: &quot;hidden1/kernel&quot;\\n        string_val: &quot;hidden2/bias&quot;\\n        string_val: &quot;hidden2/kernel&quot;\\n        string_val: &quot;hidden3/bias&quot;\\n        string_val: &quot;hidden3/kernel&quot;\\n        string_val: &quot;hidden4/bias&quot;\\n        string_val: &quot;hidden4/kernel&quot;\\n        string_val: &quot;hidden5/bias&quot;\\n        string_val: &quot;hidden5/kernel&quot;\\n        string_val: &quot;outputs/bias&quot;\\n        string_val: &quot;outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2/tensor_names&quot;\\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/in_top_k/InTopKV2/k&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/clip_value_max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/clip_value_min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/clip_value_max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/clip_value_min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/clip_value_max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/clip_value_min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/clip_value_max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/clip_value_min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/clip_value_max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/clip_value_min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/clip_value_max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/clip_value_min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/clip_value_max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/clip_value_min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/clip_value_max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/clip_value_min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/clip_value_max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/clip_value_min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/clip_value_max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/clip_value_min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/clip_value_max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/clip_value_min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/clip_value_max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/clip_value_min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape_2&quot;\\n  input: &quot;gradients/loss/loss_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/loss/loss_grad/Prod_1&quot;\\n  input: &quot;gradients/loss/loss_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/grad_ys_0&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/grad_ys_0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;index_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/loss/loss_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_10&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;save/RestoreV2:10&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;outputs/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 10\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;outputs/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_11&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;save/RestoreV2:11&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;outputs/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.3162277638912201\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.3162277638912201\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\000\\\\n\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 90\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_8&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;save/RestoreV2:8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden5/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;hidden5/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_9&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;save/RestoreV2:9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 73\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_6&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;save/RestoreV2:6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden4/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;hidden4/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_7&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;save/RestoreV2:7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 56\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_4&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;save/RestoreV2:4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden3/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;hidden3/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_5&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;save/RestoreV2:5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 39\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_2&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;save/RestoreV2:2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden2/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;hidden2/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_3&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;save/RestoreV2:3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.13093073666095734\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.13093073666095734\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;,\\\\001\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 22\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden1/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 300\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;hidden1/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 784\\n        }\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_1&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;save/RestoreV2:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save/Assign&quot;\\n  input: &quot;^save/Assign_1&quot;\\n  input: &quot;^save/Assign_10&quot;\\n  input: &quot;^save/Assign_11&quot;\\n  input: &quot;^save/Assign_2&quot;\\n  input: &quot;^save/Assign_3&quot;\\n  input: &quot;^save/Assign_4&quot;\\n  input: &quot;^save/Assign_5&quot;\\n  input: &quot;^save/Assign_6&quot;\\n  input: &quot;^save/Assign_7&quot;\\n  input: &quot;^save/Assign_8&quot;\\n  input: &quot;^save/Assign_9&quot;\\n}\\nnode {\\n  name: &quot;save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/SaveV2/tensor_names&quot;\\n  input: &quot;save/SaveV2/shape_and_slices&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;outputs/kernel&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;^save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\020\\\\003\\\\000\\\\000,\\\\001\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^hidden1/bias/Assign&quot;\\n  input: &quot;^hidden1/kernel/Assign&quot;\\n  input: &quot;^hidden2/bias/Assign&quot;\\n  input: &quot;^hidden2/kernel/Assign&quot;\\n  input: &quot;^hidden3/bias/Assign&quot;\\n  input: &quot;^hidden3/kernel/Assign&quot;\\n  input: &quot;^hidden4/bias/Assign&quot;\\n  input: &quot;^hidden4/kernel/Assign&quot;\\n  input: &quot;^hidden5/bias/Assign&quot;\\n  input: &quot;^hidden5/kernel/Assign&quot;\\n  input: &quot;^outputs/bias/Assign&quot;\\n  input: &quot;^outputs/kernel/Assign&quot;\\n}\\nnode {\\n  name: &quot;y&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        unknown_rank: true\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;X&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 784\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;hidden1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden1/MatMul&quot;\\n  input: &quot;hidden1/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  input: &quot;hidden2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden2/MatMul&quot;\\n  input: &quot;hidden2/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  input: &quot;hidden3/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden3/MatMul&quot;\\n  input: &quot;hidden3/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden3/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  input: &quot;hidden4/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden4/MatMul&quot;\\n  input: &quot;hidden4/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden4/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  input: &quot;hidden5/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden5/MatMul&quot;\\n  input: &quot;hidden5/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden5/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/outputs/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  input: &quot;outputs/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/outputs/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/outputs/MatMul&quot;\\n  input: &quot;outputs/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/in_top_k/InTopKV2&quot;\\n  op: &quot;InTopKV2&quot;\\n  input: &quot;dnn/outputs/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  input: &quot;eval/in_top_k/InTopKV2/k&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;eval/in_top_k/InTopKV2&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/accuracy&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;eval/Cast&quot;\\n  input: &quot;eval/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  op: &quot;SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;dnn/outputs/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tlabels&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  op: &quot;PreventGradient&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;message&quot;\\n    value {\\n      s: &quot;Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\\\\\\'s interaction with tf.gradients()&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/zeros_like&quot;\\n  op: &quot;ZerosLike&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape_1&quot;\\n  input: &quot;gradients/loss/loss_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/loss/loss_grad/Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;gradients/loss/loss_grad/floordiv&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/loss/loss_grad/Reshape&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/loss/loss_grad/Tile&quot;\\n  input: &quot;gradients/loss/loss_grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;gradients/loss/loss_grad/truediv&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11&quot;\\n  op: &quot;ClipByValue&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_11/clip_value_min&quot;\\n  input: &quot;clip_by_value_11/clip_value_max&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_outputs/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_11&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;outputs/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10&quot;\\n  op: &quot;ClipByValue&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_10/clip_value_min&quot;\\n  input: &quot;clip_by_value_10/clip_value_max&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_outputs/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_10&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9&quot;\\n  op: &quot;ClipByValue&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_9/clip_value_min&quot;\\n  input: &quot;clip_by_value_9/clip_value_max&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden5/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden5/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8&quot;\\n  op: &quot;ClipByValue&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_8/clip_value_min&quot;\\n  input: &quot;clip_by_value_8/clip_value_max&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden5/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7&quot;\\n  op: &quot;ClipByValue&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_7/clip_value_min&quot;\\n  input: &quot;clip_by_value_7/clip_value_max&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden4/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden4/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6&quot;\\n  op: &quot;ClipByValue&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_6/clip_value_min&quot;\\n  input: &quot;clip_by_value_6/clip_value_max&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden4/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5&quot;\\n  op: &quot;ClipByValue&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_5/clip_value_min&quot;\\n  input: &quot;clip_by_value_5/clip_value_max&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden3/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden3/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4&quot;\\n  op: &quot;ClipByValue&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_4/clip_value_min&quot;\\n  input: &quot;clip_by_value_4/clip_value_max&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden3/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3&quot;\\n  op: &quot;ClipByValue&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_3/clip_value_min&quot;\\n  input: &quot;clip_by_value_3/clip_value_max&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden2/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2&quot;\\n  op: &quot;ClipByValue&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_2/clip_value_min&quot;\\n  input: &quot;clip_by_value_2/clip_value_max&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden2/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1&quot;\\n  op: &quot;ClipByValue&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_1/clip_value_min&quot;\\n  input: &quot;clip_by_value_1/clip_value_max&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden1/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value&quot;\\n  op: &quot;ClipByValue&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value/clip_value_min&quot;\\n  input: &quot;clip_by_value/clip_value_max&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden1/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update_hidden1/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden1/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden2/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden2/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden3/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden3/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden4/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden4/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden5/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden5/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_outputs/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_outputs/kernel/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/loss&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;loss/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.3745401188473625&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you know which operations you need, you can get a handle on them using the graph's `get_operation_by_name()` or `get_tensor_by_name()` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are the author of the original model, you could make things easier for people who will reuse your model by giving operations very clear names and documenting them. Another approach is to create a collection containing all the important operations that people will want to get a handle on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for op in (X, y, accuracy, training_op):\n",
    "    tf.add_to_collection(\"my_important\", op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way people who reuse your model will be able to simply write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, accuracy, training_op = tf.get_collection(\"my_important\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start a session, restore the model's state and continue training on your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, let's test this for real!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9636\n",
      "1 Validation accuracy: 0.9632\n",
      "2 Validation accuracy: 0.9658\n",
      "3 Validation accuracy: 0.9652\n",
      "4 Validation accuracy: 0.9646\n",
      "5 Validation accuracy: 0.965\n",
      "6 Validation accuracy: 0.969\n",
      "7 Validation accuracy: 0.9682\n",
      "8 Validation accuracy: 0.9682\n",
      "9 Validation accuracy: 0.9684\n",
      "10 Validation accuracy: 0.9704\n",
      "11 Validation accuracy: 0.971\n",
      "12 Validation accuracy: 0.9668\n",
      "13 Validation accuracy: 0.97\n",
      "14 Validation accuracy: 0.9712\n",
      "15 Validation accuracy: 0.9726\n",
      "16 Validation accuracy: 0.9718\n",
      "17 Validation accuracy: 0.971\n",
      "18 Validation accuracy: 0.9712\n",
      "19 Validation accuracy: 0.9712\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you have access to the Python code that built the original graph, you can use it instead of `import_meta_graph()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "learning_rate = 0.01\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And continue training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9642\n",
      "1 Validation accuracy: 0.9632\n",
      "2 Validation accuracy: 0.9656\n",
      "3 Validation accuracy: 0.9652\n",
      "4 Validation accuracy: 0.9646\n",
      "5 Validation accuracy: 0.9652\n",
      "6 Validation accuracy: 0.9688\n",
      "7 Validation accuracy: 0.9686\n",
      "8 Validation accuracy: 0.9682\n",
      "9 Validation accuracy: 0.9686\n",
      "10 Validation accuracy: 0.9704\n",
      "11 Validation accuracy: 0.9712\n",
      "12 Validation accuracy: 0.967\n",
      "13 Validation accuracy: 0.9698\n",
      "14 Validation accuracy: 0.9708\n",
      "15 Validation accuracy: 0.9724\n",
      "16 Validation accuracy: 0.9718\n",
      "17 Validation accuracy: 0.9712\n",
      "18 Validation accuracy: 0.9708\n",
      "19 Validation accuracy: 0.9712\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general you will want to reuse only the lower layers. If you are using `import_meta_graph()` it will load the whole graph, but you can simply ignore the parts you do not need. In this example, we add a new 4th hidden layer on top of the pretrained 3rd layer (ignoring the old 4th hidden layer). We also build a new output layer, the loss for this new output, and a new optimizer to minimize it. We also need another saver to save the whole graph (containing both the entire old graph plus the new operations), and an initialization operation to initialize all the new variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_hidden4 = 20  # new layer\n",
    "n_outputs = 10  # new layer\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "hidden3 = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden4/Relu:0\")\n",
    "\n",
    "new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"new_hidden4\")\n",
    "new_logits = tf.layers.dense(new_hidden4, n_outputs, name=\"new_outputs\")\n",
    "\n",
    "with tf.name_scope(\"new_loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"new_eval\"):\n",
    "    correct = tf.nn.in_top_k(new_logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"new_train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can train this new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9246\n",
      "1 Validation accuracy: 0.945\n",
      "2 Validation accuracy: 0.953\n",
      "3 Validation accuracy: 0.9582\n",
      "4 Validation accuracy: 0.9606\n",
      "5 Validation accuracy: 0.9562\n",
      "6 Validation accuracy: 0.9622\n",
      "7 Validation accuracy: 0.9624\n",
      "8 Validation accuracy: 0.9642\n",
      "9 Validation accuracy: 0.9644\n",
      "10 Validation accuracy: 0.9656\n",
      "11 Validation accuracy: 0.9664\n",
      "12 Validation accuracy: 0.965\n",
      "13 Validation accuracy: 0.9676\n",
      "14 Validation accuracy: 0.9678\n",
      "15 Validation accuracy: 0.9688\n",
      "16 Validation accuracy: 0.9692\n",
      "17 Validation accuracy: 0.97\n",
      "18 Validation accuracy: 0.9688\n",
      "19 Validation accuracy: 0.968\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = new_saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have access to the Python code that built the original graph, you can just reuse the parts you need and drop the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you must create one `Saver` to restore the pretrained model (giving it the list of variables to restore, or else it will complain that the graphs don't match), and another `Saver` to save the new model, once it is trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9024\n",
      "1 Validation accuracy: 0.9332\n",
      "2 Validation accuracy: 0.943\n",
      "3 Validation accuracy: 0.947\n",
      "4 Validation accuracy: 0.9516\n",
      "5 Validation accuracy: 0.9532\n",
      "6 Validation accuracy: 0.9558\n",
      "7 Validation accuracy: 0.9592\n",
      "8 Validation accuracy: 0.9586\n",
      "9 Validation accuracy: 0.9608\n",
      "10 Validation accuracy: 0.9626\n",
      "11 Validation accuracy: 0.962\n",
      "12 Validation accuracy: 0.964\n",
      "13 Validation accuracy: 0.9662\n",
      "14 Validation accuracy: 0.966\n",
      "15 Validation accuracy: 0.9662\n",
      "16 Validation accuracy: 0.9672\n",
      "17 Validation accuracy: 0.9674\n",
      "18 Validation accuracy: 0.9682\n",
      "19 Validation accuracy: 0.9678\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden[123]\") # regex\n",
    "restore_path = tf.train.Saver(reuse_vars) # restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_path.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    for epoch in range(n_epochs):                                            \n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): \n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})        \n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})     \n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)                   \n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing Models from Other Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model was trained using another framework, you will need to load the model parameters manually (e.g., using Theano code if it was trained with Theano), then assign them to the appropriate variables. This can be quite tedious. For example, the following code shows how you would copy the weight and biases from the first hidden layer of a model trained using another framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, for each variable we want to reuse, we find its initializer's assignment operation, and we get its second input, which corresponds to the initialization value. When we run the initializer, we replace the initialization values with the ones we want, using a `feed_dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61.  83. 105.]]\n"
     ]
    }
   ],
   "source": [
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n",
    "original_b = [7., 8., 9.]                 # Load the biases from the other framework\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] Build the rest of the model\n",
    "\n",
    "# Get a handle on the assignment nodes for the hidden1 variables\n",
    "graph = tf.get_default_graph()\n",
    "assign_kernel = graph.get_operation_by_name(\"hidden1/kernel/Assign\")\n",
    "assign_bias = graph.get_operation_by_name(\"hidden1/bias/Assign\")\n",
    "init_kernel = assign_kernel.inputs[1]\n",
    "init_bias = assign_bias.inputs[1]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init, feed_dict={init_kernel: original_w, init_bias: original_b})\n",
    "    # [...] Train the model on your new task\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the weights variable created by the `tf.layers.dense()` function is called `\"kernel\"` (instead of `\"weights\"` when using the `tf.contrib.layers.fully_connected()`, as in the book), and the biases variable is called `bias` instead of `biases`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach (initially used in the book) would be to create dedicated assignment nodes and dedicated placeholders. This is more verbose and less efficient, but you may find this more explicit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61.  83. 105.]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3\n",
    "\n",
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n",
    "original_b = [7., 8., 9.]                 # Load the biases from the other framework\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] Build the rest of the model\n",
    "\n",
    "# Get a handle on the variables of layer hidden1\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):  # root scope\n",
    "    hidden1_weights = tf.get_variable(\"hidden1/kernel\")\n",
    "    hidden1_biases = tf.get_variable(\"hidden1/bias\")\n",
    "\n",
    "# Create dedicated placeholders and assignment nodes\n",
    "original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden1))\n",
    "original_biases = tf.placeholder(tf.float32, shape=n_hidden1)\n",
    "assign_hidden1_weights = tf.assign(hidden1_weights, original_weights)\n",
    "assign_hidden1_biases = tf.assign(hidden1_biases, original_biases)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(assign_hidden1_weights, feed_dict={original_weights: original_w})\n",
    "    sess.run(assign_hidden1_biases, feed_dict={original_biases: original_b})\n",
    "    # [...] Train the model on your new task\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could also get a handle on the variables using `get_collection()` and specifying the `scope`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden1/bias:0' shape=(3,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could use the graph's `get_tensor_by_name()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/bias:0' shape=(3,) dtype=float32_ref>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/bias:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freezing the Lower Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is likely that the lower layers of the first DNN have learned to detect low-level features in pictures that will be useful across both image classification tasks, so you can just reuse these layers as they are. It is generally a good idea to “freeze” their weights when training the new DNN: if the lower-layer weights are fixed, then the higher-layer weights will be easier to train (because they won’t have to learn a moving target). To freeze the lower layers during training, one solution is to give the optimizer the list of variables to train, excluding the variables from the lower layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden[34]|outputs\")\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.8964\n",
      "1 Validation accuracy: 0.9298\n",
      "2 Validation accuracy: 0.94\n",
      "3 Validation accuracy: 0.9442\n",
      "4 Validation accuracy: 0.948\n",
      "5 Validation accuracy: 0.951\n",
      "6 Validation accuracy: 0.9508\n",
      "7 Validation accuracy: 0.9538\n",
      "8 Validation accuracy: 0.9554\n",
      "9 Validation accuracy: 0.957\n",
      "10 Validation accuracy: 0.9562\n",
      "11 Validation accuracy: 0.9566\n",
      "12 Validation accuracy: 0.9572\n",
      "13 Validation accuracy: 0.9578\n",
      "14 Validation accuracy: 0.959\n",
      "15 Validation accuracy: 0.9576\n",
      "16 Validation accuracy: 0.9574\n",
      "17 Validation accuracy: 0.9602\n",
      "18 Validation accuracy: 0.9592\n",
      "19 Validation accuracy: 0.9602\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden[123]\") # regular expression\n",
    "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to add a stop_gradient() layer in the graph. Any layer below it will be frozen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused frozen\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training code is exactly the same as earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.902\n",
      "1 Validation accuracy: 0.9302\n",
      "2 Validation accuracy: 0.9438\n",
      "3 Validation accuracy: 0.9478\n",
      "4 Validation accuracy: 0.9514\n",
      "5 Validation accuracy: 0.9522\n",
      "6 Validation accuracy: 0.9524\n",
      "7 Validation accuracy: 0.9556\n",
      "8 Validation accuracy: 0.9556\n",
      "9 Validation accuracy: 0.9558\n",
      "10 Validation accuracy: 0.957\n",
      "11 Validation accuracy: 0.9552\n",
      "12 Validation accuracy: 0.9572\n",
      "13 Validation accuracy: 0.9582\n",
      "14 Validation accuracy: 0.9582\n",
      "15 Validation accuracy: 0.957\n",
      "16 Validation accuracy: 0.9566\n",
      "17 Validation accuracy: 0.9578\n",
      "18 Validation accuracy: 0.9594\n",
      "19 Validation accuracy: 0.958\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden[123]\")\n",
    "restore_saver = tf.train.Saver(reuse_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caching the Frozen Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the frozen layers won’t change, it is possible to cache the output of the topmost frozen layer for each training instance. Since training goes through the whole dataset many times, this will give you a huge speed boost as you will only need to go through the frozen layers once per training instance (instead of once per epoch). For example, you could first run the whole training set through the lower layers (assuming you have enough RAM), then during training, instead of building batches of training instances, you would build batches of outputs from hidden layer   2 and feed them to the training operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\") # reused frozen & cached\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden[123]\")\n",
    "restore_saver = tf.train.Saver(reuse_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.902\n",
      "1 Validation accuracy: 0.9302\n",
      "2 Validation accuracy: 0.9438\n",
      "3 Validation accuracy: 0.9478\n",
      "4 Validation accuracy: 0.9514\n",
      "5 Validation accuracy: 0.9522\n",
      "6 Validation accuracy: 0.9524\n",
      "7 Validation accuracy: 0.9556\n",
      "8 Validation accuracy: 0.9556\n",
      "9 Validation accuracy: 0.9558\n",
      "10 Validation accuracy: 0.957\n",
      "11 Validation accuracy: 0.9552\n",
      "12 Validation accuracy: 0.9572\n",
      "13 Validation accuracy: 0.9582\n",
      "14 Validation accuracy: 0.9582\n",
      "15 Validation accuracy: 0.957\n",
      "16 Validation accuracy: 0.9566\n",
      "17 Validation accuracy: 0.9578\n",
      "18 Validation accuracy: 0.9594\n",
      "19 Validation accuracy: 0.958\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_batches = len(X_train) // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    h2_cache = sess.run(hidden2, feed_dict={X: X_train})\n",
    "    h2_cache_valid = sess.run(hidden2, feed_dict={X: X_valid}) \n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_idx = np.random.permutation(len(X_train))\n",
    "        hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n",
    "        y_batches = np.array_split(y_train[shuffled_idx], n_batches)\n",
    "        \n",
    "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "            sess.run(training_op, feed_dict={hidden2: hidden2_batch, y: y_batch})\n",
    "            \n",
    "        accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_valid, y: y_valid})             \n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)               \n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking, Dropping, or Replacing the Upper Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layer of the original model should usually be replaced since it is most likely not useful at all for the new task, and it may not even have the right number of outputs for the new task.\n",
    "\n",
    "Similarly, the upper hidden layers of the original model are less likely to be as useful as the lower layers, since the high-level features that are most useful for the new task may differ significantly from the ones that were most useful for the original task. You want to find the right number of layers to reuse.\n",
    "\n",
    "Try freezing all the copied layers first, then train your model and see how it performs. Then try unfreezing one or two of the top hidden layers to let backpropagation tweak them and see if performance improves. The more training data you have, the more layers you can unfreeze.\n",
    "\n",
    "If you still cannot get good performance, and you have little training data, try dropping the top hidden layer( s) and freeze all remaining hidden layers again. You can iterate until you find the right number of layers to reuse. If you have plenty of training data, you may try replacing the top hidden layers instead of dropping them, and even add more hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum optimization\n",
    "\n",
    "Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start out slowly, but it will quickly pick up momentum until it eventually reaches terminal velocity (if there is some friction or air resistance). This is the very simple idea behind *Momentum optimization*, proposed by Boris Polyak in 1964.10 In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one drawback of Momentum optimization is that it adds yet another hyperparameter to tune. However, the momentum value of 0.9 usually works well in practice and almost always goes faster than Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient\n",
    "\n",
    "One small variant to Momentum optimization, proposed by Yurii Nesterov in 1983, is almost always faster than vanilla Momentum optimization. The idea of *Nesterov Momentum optimization*, or *Nesterov Accelerated Gradient* (NAG), is to measure the gradient of the cost function not at the local position but slightly ahead in the direction of the momentum. The only difference from vanilla Momentum optimization is that the gradient is measured at θ + βm rather than at θ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9, use_nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "Consider the elongated bowl problem again: Gradient Descent starts by quickly going down the steepest slope, then slowly goes down the bottom of the valley. It would be nice if the algorithm could detect this early on and correct its direction to point a bit more toward the global optimum.\n",
    "\n",
    "The *AdaGrad* algorithm achieves this by scaling down the gradient vector along the steepest dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "\n",
    "Although AdaGrad slows down a bit too fast and ends up never converging to the global optimum, the RMSProp algorithm fixes this by accumulating only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training). It does so by using exponential decay in the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, momentum=0.9, decay=0.9, epsilon=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Optimization\n",
    "\n",
    "Adam, which stands for *adaptive moment estimation*, combines the ideas of Momentum optimization and RMSProp: just like Momentum optimization it keeps track of an exponentially decaying average of past gradients, and just like RMSProp it keeps track of an exponentially decaying average of past squared gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the optimization techniques discussed so far only rely on the *first-order partial derivatives* (*Jacobians*). The optimization literature contains amazing algorithms based on the *second-order partial derivatives* (*the Hessians*). Unfortunately, these algorithms are very hard to apply to deep neural networks because there are n2 Hessians per output (where n is the number of parameters), as opposed to just n Jacobians per output. Since DNNs typically have tens of thousands of parameters, the second-order optimization algorithms often don’t even fit in memory, and even when they do, computing the Hessians is just too slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduling\n",
    "\n",
    "Finding a good learning rate can be tricky. If you set it way too high, training may actually diverge. If you set it too low, training will eventually converge to the optimum, but it will take a very long time. If you set it slightly too high, it will make progress very quickly at first, but it will end up dancing around the optimum, never settling down. If you have a limited computing budget, you may have to interrupt training before it has converged properly, yielding a suboptimal solution.\n",
    "\n",
    "You may be able to find a fairly good learning rate by training your network several times during just a few epochs using various learning rates and comparing the learning curves. The ideal learning rate will learn quickly and \n",
    "converge to good solution.\n",
    "\n",
    "However, you can do better than a constant learning rate: if you start with a high learning rate and then reduce it once it stops making fast progress, you can reach a good solution faster than with the optimal constant learning rate. There are many different strategies to reduce the learning rate during training. These strategies are called *learning schedules*, the most common of which are:\n",
    "- *Predetermined piecewise constant learning rate*. For example, set the learning rate to η0 = 0.1 at first, then to η1 = 0.001 after 50 epochs. Although this solution can work very well, it often requires fiddling around to figure out the right learning rates and when to use them.\n",
    "- *Performance scheduling*. Measure the validation error every N steps (just likefor early stopping) and reduce the learning rate by a factor of λ when the error stops dropping.\n",
    "- *Exponential scheduling*. Set the learning rate to a function of the iteration number t: η( t) = η0 10– t/ r. This works great, but it requires tuning η0 and r. The learning rate will drop by a factor of   10 every r steps.\n",
    "- *Power scheduling*. Set the learning rate to η( t) = η0 (1 + t/ r)– c. The hyperparameter c is typically set to   1. This is similar to exponential scheduling, but the learning rate drops much more slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 validation accuracy 0.959\n",
      "1 validation accuracy 0.9688\n",
      "2 validation accuracy 0.9726\n",
      "3 validation accuracy 0.9804\n",
      "4 validation accuracy 0.982\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"validation accuracy\", accuracy_val)\n",
    "        \n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\ell_1$ and $\\ell_2$ regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement $\\ell_1$ regularization manually. First, we create the model, as usual (with just one hidden layer this time, for simplicity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get a handle on the layer weights, and we compute the total loss, which is equal to the sum of the usual cross entropy loss and the $\\ell_1$ loss (i.e., the absolute values of the weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "\n",
    "scale = 0.001 # l1 regularization hyperparameter\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
    "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is just as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.831\n",
      "1 Validation accuracy: 0.871\n",
      "2 Validation accuracy: 0.8838\n",
      "3 Validation accuracy: 0.8934\n",
      "4 Validation accuracy: 0.8966\n",
      "5 Validation accuracy: 0.8988\n",
      "6 Validation accuracy: 0.9016\n",
      "7 Validation accuracy: 0.9044\n",
      "8 Validation accuracy: 0.9058\n",
      "9 Validation accuracy: 0.906\n",
      "10 Validation accuracy: 0.9068\n",
      "11 Validation accuracy: 0.9054\n",
      "12 Validation accuracy: 0.907\n",
      "13 Validation accuracy: 0.9084\n",
      "14 Validation accuracy: 0.9088\n",
      "15 Validation accuracy: 0.9064\n",
      "16 Validation accuracy: 0.9066\n",
      "17 Validation accuracy: 0.9066\n",
      "18 Validation accuracy: 0.9066\n",
      "19 Validation accuracy: 0.9052\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can pass a regularization function to the `tf.layers.dense()` function, which will use it to create operations that will compute the regularization loss, and it adds these operations to the collection of regularization losses. The beginning is the same as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use Python's `partial()` function to avoid repeating the same arguments over and over again. Note that we set the `kernel_regularizer` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer = partial(\n",
    "    tf.layers.dense,\n",
    "    activation=tf.nn.relu,\n",
    "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale)\n",
    ")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = dense_layer(hidden2, n_outputs, activation=None, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we must add the regularization losses to the base loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    \n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.8274\n",
      "1 Validation accuracy: 0.8766\n",
      "2 Validation accuracy: 0.8952\n",
      "3 Validation accuracy: 0.9016\n",
      "4 Validation accuracy: 0.908\n",
      "5 Validation accuracy: 0.9096\n",
      "6 Validation accuracy: 0.9126\n",
      "7 Validation accuracy: 0.9154\n",
      "8 Validation accuracy: 0.9178\n",
      "9 Validation accuracy: 0.919\n",
      "10 Validation accuracy: 0.92\n",
      "11 Validation accuracy: 0.9224\n",
      "12 Validation accuracy: 0.9212\n",
      "13 Validation accuracy: 0.9228\n",
      "14 Validation accuracy: 0.9224\n",
      "15 Validation accuracy: 0.9216\n",
      "16 Validation accuracy: 0.9218\n",
      "17 Validation accuracy: 0.9228\n",
      "18 Validation accuracy: 0.9216\n",
      "19 Validation accuracy: 0.9214\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most popular regularization technique for deep neural networks is *arguably dropout*. It was proposed by G. E. Hinton in 2012 and further detailed in a paper by Nitish Srivastava et al., and it has proven to be highly successful: even the state-of-the-art neural networks got a 1– 2% accuracy boost simply by adding dropout. This may not sound like a lot, but when a model already has 95% accuracy, getting a 2% accuracy boost means dropping the error rate by almost 40% (going from 5% error to roughly 3%).\n",
    "\n",
    "It is a fairly simple algorithm: at every training step, every neuron (including the input neurons but excluding the output neurons) has a probability p of being temporarily “dropped out,” meaning it will be entirely ignored during this training step, but it may be active during the next step. The hyperparameter *p* is called the dropout rate, and it is typically set to 50%. After training, neurons don’t get dropped anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the book uses `tf.contrib.layers.dropout()` rather than `tf.layers.dropout()` (which did not exist when this chapter was written). It is now preferable to use `tf.layers.dropout()`, because anything in the contrib module may change or be deleted without notice. The `tf.layers.dropout()` function is almost identical to the `tf.contrib.layers.dropout()` function, except for a few minor differences. Most importantly:\n",
    "* you must specify the dropout rate (`rate`) rather than the keep probability (`keep_prob`), where `rate` is simply equal to `1 - keep_prob`,\n",
    "* the `is_training` parameter is renamed to `training`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "dropout_rate = 0.5 # 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy 0.9584\n",
      "1 Validation accuracy 0.9722\n",
      "2 Validation accuracy 0.968\n",
      "3 Validation accuracy 0.9772\n",
      "4 Validation accuracy 0.9782\n",
      "5 Validation accuracy 0.9772\n",
      "6 Validation accuracy 0.9822\n",
      "7 Validation accuracy 0.9824\n",
      "8 Validation accuracy 0.9798\n",
      "9 Validation accuracy 0.982\n",
      "10 Validation accuracy 0.9834\n",
      "11 Validation accuracy 0.9834\n",
      "12 Validation accuracy 0.9846\n",
      "13 Validation accuracy 0.9838\n",
      "14 Validation accuracy 0.9844\n",
      "15 Validation accuracy 0.9846\n",
      "16 Validation accuracy 0.9842\n",
      "17 Validation accuracy 0.9846\n",
      "18 Validation accuracy 0.9848\n",
      "19 Validation accuracy 0.984\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy\", accuracy_val)\n",
    "        \n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Max-norm regularization*: for each neuron, it constrains the weights **w** of the incoming connections such that **∥ w ∥** *2 ≤ r*, where **r** is the max-norm hyperparameter and **∥ · ∥ 2** is the **ℓ2** norm. We typically implement this constraint by computing **∥ w ∥ 2** after each training step and clipping **w** if needed. Reducing *r* increases the amount of regularization and helps reduce overfitting. Max-norm regularization can also help alleviate the vanishing/ exploding gradients problems (if you are not using Batch Normalization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to a plain and simple neural net for MNIST with just 2 hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's get a handle on the first hidden layer's weight and create an operation that will compute the clipped weights using the `clip_by_norm()` function. Then we create an assignment operation to assign the clipped weights to the weights variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "weights = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n",
    "clip_weights = tf.assign(weights, clipped_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this as well for the second hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\n",
    "clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n",
    "clip_weights2 = tf.assign(weights2, clipped_weights2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add an initializer and a saver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can train the model. It's pretty much as usual, except that right after running the `training_op`, we run the `clip_weights` and `clip_weights2` operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy 0.9568\n",
      "1 Validation accuracy 0.9696\n",
      "2 Validation accuracy 0.972\n",
      "3 Validation accuracy 0.9768\n",
      "4 Validation accuracy 0.9784\n",
      "5 Validation accuracy 0.9786\n",
      "6 Validation accuracy 0.9816\n",
      "7 Validation accuracy 0.9808\n",
      "8 Validation accuracy 0.981\n",
      "9 Validation accuracy 0.983\n",
      "10 Validation accuracy 0.9822\n",
      "11 Validation accuracy 0.9854\n",
      "12 Validation accuracy 0.9822\n",
      "13 Validation accuracy 0.9842\n",
      "14 Validation accuracy 0.984\n",
      "15 Validation accuracy 0.9852\n",
      "16 Validation accuracy 0.984\n",
      "17 Validation accuracy 0.9844\n",
      "18 Validation accuracy 0.9844\n",
      "19 Validation accuracy 0.9844\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            clip_weights.eval()\n",
    "            clip_weights2.eval()\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy\", accuracy_val)\n",
    "        \n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation above is straightforward and it works fine, but it is a bit messy. A better approach is to define a `max_norm_regularizer()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\", collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        \n",
    "        # There is no regularization loss term.\n",
    "        return None\n",
    "    \n",
    "    return max_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can call this function to get a max norm regularizer (with the threshold you want). When you create a hidden layer, you can pass this regularizer to the `kernel_regularizer` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, kernel_regularizer=max_norm_reg, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, kernel_regularizer=max_norm_reg,\n",
    "                              name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is as usual, except you must run the weights clipping operations after each training operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9556\n",
      "1 Validation accuracy: 0.9706\n",
      "2 Validation accuracy: 0.9686\n",
      "3 Validation accuracy: 0.9738\n",
      "4 Validation accuracy: 0.9764\n",
      "5 Validation accuracy: 0.976\n",
      "6 Validation accuracy: 0.9806\n",
      "7 Validation accuracy: 0.9796\n",
      "8 Validation accuracy: 0.9832\n",
      "9 Validation accuracy: 0.981\n",
      "10 Validation accuracy: 0.9818\n",
      "11 Validation accuracy: 0.983\n",
      "12 Validation accuracy: 0.9826\n",
      "13 Validation accuracy: 0.9826\n",
      "14 Validation accuracy: 0.983\n",
      "15 Validation accuracy: 0.984\n",
      "16 Validation accuracy: 0.9832\n",
      "17 Validation accuracy: 0.983\n",
      "18 Validation accuracy: 0.9834\n",
      "19 Validation accuracy: 0.9838\n"
     ]
    }
   ],
   "source": [
    "clip_all_weights = tf.get_collection(\"max_norm\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            sess.run(clip_all_weights)\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid}) \n",
    "        print(epoch, \"Validation accuracy:\", acc_valid)               \n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise: Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need similar DNNs in the next exercises, so let's create a function to build this DNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_init = tf.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn(inputs, n_hidden_layers=5, n_neurons=100, name=None, activation=tf.nn.elu, initializer=he_init):\n",
    "    with tf.name_scope(name, \"dnn\"):\n",
    "        for layer in range(n_hidden_layers):\n",
    "            inputs = tf.layers.dense(inputs, n_neurons, activation=activation, kernel_initializer=initializer,\n",
    "                                    name=\"hidden%d\" % (layer + 1))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_outputs = 10\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "dnn_outputs = dnn(X)\n",
    "logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"logits\")\n",
    "Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise: Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later._\n",
    "\n",
    "Let's complete the graph with the cost function, the training op, and all the other usual components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss, name=\"training_op\")\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the training set, validation and test set (we need the validation set to implement early stopping):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train[y_train < 5]\n",
    "y_train1 = y_train[y_train < 5]\n",
    "X_valid1 = X_valid[y_valid < 5]\n",
    "y_valid1 = y_valid[y_valid < 5]\n",
    "X_test1 = X_test[y_test < 5]\n",
    "y_test1 = y_test[y_test < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.092821\tBest loss: 0.092821\tAccuracy: 97.46%\n",
      "1\tValidation loss: 0.233841\tBest loss: 0.092821\tAccuracy: 94.68%\n",
      "2\tValidation loss: 0.962701\tBest loss: 0.092821\tAccuracy: 55.94%\n",
      "3\tValidation loss: 0.238051\tBest loss: 0.092821\tAccuracy: 96.09%\n",
      "4\tValidation loss: 1.667996\tBest loss: 0.092821\tAccuracy: 39.21%\n",
      "5\tValidation loss: 1.332677\tBest loss: 0.092821\tAccuracy: 38.86%\n",
      "6\tValidation loss: 1.279799\tBest loss: 0.092821\tAccuracy: 38.66%\n",
      "7\tValidation loss: 1.536807\tBest loss: 0.092821\tAccuracy: 40.46%\n",
      "8\tValidation loss: 1.301057\tBest loss: 0.092821\tAccuracy: 36.55%\n",
      "9\tValidation loss: 1.787939\tBest loss: 0.092821\tAccuracy: 20.91%\n",
      "10\tValidation loss: 1.621812\tBest loss: 0.092821\tAccuracy: 22.01%\n",
      "11\tValidation loss: 1.785381\tBest loss: 0.092821\tAccuracy: 22.01%\n",
      "12\tValidation loss: 1.629946\tBest loss: 0.092821\tAccuracy: 19.08%\n",
      "13\tValidation loss: 1.621241\tBest loss: 0.092821\tAccuracy: 20.91%\n",
      "14\tValidation loss: 1.649095\tBest loss: 0.092821\tAccuracy: 20.91%\n",
      "15\tValidation loss: 1.633090\tBest loss: 0.092821\tAccuracy: 20.91%\n",
      "16\tValidation loss: 1.719113\tBest loss: 0.092821\tAccuracy: 19.08%\n",
      "17\tValidation loss: 1.682555\tBest loss: 0.092821\tAccuracy: 19.27%\n",
      "18\tValidation loss: 1.675346\tBest loss: 0.092821\tAccuracy: 18.73%\n",
      "19\tValidation loss: 1.645804\tBest loss: 0.092821\tAccuracy: 19.08%\n",
      "20\tValidation loss: 1.722339\tBest loss: 0.092821\tAccuracy: 22.01%\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train1))\n",
    "        \n",
    "        for rnd_indicies in np.array_split(rnd_idx, len(X_train1) // batch_size):\n",
    "            X_batch, y_batch = X_train1[rnd_indicies], y_train1[rnd_indicies]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        \n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid1, y: y_valid1})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = saver.save(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        \n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_mnist_model_0_to_4.ckpt\n",
      "Final test accuracy: 97.90%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can do better by tuning the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise: Tune the hyperparameters using cross-validation and see what precision you can achieve._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `DNNClassifier` class, compatible with Scikit-Learn's `RandomizedSearchCV` class, to perform hyperparameter tuning. Here are the key points of this implementation:\n",
    "* the `__init__()` method (constructor) does nothing more than create instance variables for each of the hyperparameters.\n",
    "* the `fit()` method creates the graph, starts a session and trains the model:\n",
    "  * it calls the `_build_graph()` method to build the graph (much lile the graph we defined earlier). Once this method is done creating the graph, it saves all the important operations as instance variables for easy access by other methods.\n",
    "  * the `_dnn()` method builds the hidden layers, just like the `dnn()` function above, but also with support for batch normalization and dropout (for the next exercises).\n",
    "  * if the `fit()` method is given a validation set (`X_valid` and `y_valid`), then it implements early stopping. This implementation does not save the best model to disk, but rather to memory: it uses the `_get_model_params()` method to get all the graph's variables and their values, and the `_restore_model_params()` method to restore the variable values (of the best model found). This trick helps speed up training.\n",
    "  * After the `fit()` method has finished training the model, it keeps the session open so that predictions can be made quickly, without having to save a model to disk and restore it for every prediction. You can close the session by calling the `close_session()` method.\n",
    "* the `predict_proba()` method uses the trained model to predict the class probabilities.\n",
    "* the `predict()` method calls `predict_proba()` and returns the class with the highest probability, for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden_layers=5, n_neurons=100, optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, batch_size=20, activation=tf.nn.elu, initializer=he_init,\n",
    "                 batch_norm_momentum=None, dropout_rate=None, random_state=None):\n",
    "        \"\"\"Initialize the DNNClassifier by simply storing all the hyperparameters.\"\"\"\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.random_state = random_state\n",
    "        self._session = None\n",
    "\n",
    "    def _dnn(self, inputs):\n",
    "        \"\"\"Build the hidden layers, with support for batch normalization and dropout.\"\"\"\n",
    "        for layer in range(self.n_hidden_layers):\n",
    "            if self.dropout_rate:\n",
    "                inputs = tf.layers.dropout(inputs, self.dropout_rate, training=self._training)\n",
    "            inputs = tf.layers.dense(inputs, self.n_neurons,\n",
    "                                     kernel_initializer=self.initializer,\n",
    "                                     name=\"hidden%d\" % (layer + 1))\n",
    "            if self.batch_norm_momentum:\n",
    "                inputs = tf.layers.batch_normalization(inputs, momentum=self.batch_norm_momentum,\n",
    "                                                       training=self._training)\n",
    "            inputs = self.activation(inputs, name=\"hidden%d_out\" % (layer + 1))\n",
    "        return inputs\n",
    "\n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        \"\"\"Build the same model as earlier\"\"\"\n",
    "        if self.random_state is not None:\n",
    "            tf.set_random_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "        y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "        if self.batch_norm_momentum or self.dropout_rate:\n",
    "            self._training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "        else:\n",
    "            self._training = None\n",
    "\n",
    "        dnn_outputs = self._dnn(X)\n",
    "\n",
    "        logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"logits\")\n",
    "        Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                                  logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "        optimizer = self.optimizer_class(learning_rate=self.learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Make the important operations available easily through instance variables\n",
    "        self._X, self._y = X, y\n",
    "        self._Y_proba, self._loss = Y_proba, loss\n",
    "        self._training_op, self._accuracy = training_op, accuracy\n",
    "        self._init, self._saver = init, saver\n",
    "\n",
    "    def close_session(self):\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "\n",
    "    def _get_model_params(self):\n",
    "        \"\"\"Get all variable values (used for early stopping, faster than saving to disk)\"\"\"\n",
    "        with self._graph.as_default():\n",
    "            gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        return {gvar.op.name: value for gvar, value in zip(gvars, self._session.run(gvars))}\n",
    "\n",
    "    def _restore_model_params(self, model_params):\n",
    "        \"\"\"Set all variables to the given values (for early stopping, faster than loading from disk)\"\"\"\n",
    "        gvar_names = list(model_params.keys())\n",
    "        assign_ops = {gvar_name: self._graph.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                      for gvar_name in gvar_names}\n",
    "        init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "        feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "        self._session.run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "    def fit(self, X, y, n_epochs=100, X_valid=None, y_valid=None):\n",
    "        \"\"\"Fit the model to the training set. If X_valid and y_valid are provided, use early stopping.\"\"\"\n",
    "        self.close_session()\n",
    "\n",
    "        # infer n_inputs and n_outputs from the training set.\n",
    "        n_inputs = X.shape[1]\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_outputs = len(self.classes_)\n",
    "        \n",
    "        # Translate the labels vector to a vector of sorted class indices, containing\n",
    "        # integers from 0 to n_outputs - 1.\n",
    "        # For example, if y is equal to [8, 8, 9, 5, 7, 6, 6, 6], then the sorted class\n",
    "        # labels (self.classes_) will be equal to [5, 6, 7, 8, 9], and the labels vector\n",
    "        # will be translated to [3, 3, 4, 0, 2, 1, 1, 1]\n",
    "        self.class_to_index_ = {label: index\n",
    "                                for index, label in enumerate(self.classes_)}\n",
    "        y = np.array([self.class_to_index_[label]\n",
    "                      for label in y], dtype=np.int32)\n",
    "        \n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "            # extra ops for batch normalization\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "        # needed in case of early stopping\n",
    "        max_checks_without_progress = 20\n",
    "        checks_without_progress = 0\n",
    "        best_loss = np.infty\n",
    "        best_params = None\n",
    "        \n",
    "        # Now train the model!\n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        with self._session.as_default() as sess:\n",
    "            self._init.run()\n",
    "            for epoch in range(n_epochs):\n",
    "                rnd_idx = np.random.permutation(len(X))\n",
    "                for rnd_indices in np.array_split(rnd_idx, len(X) // self.batch_size):\n",
    "                    X_batch, y_batch = X[rnd_indices], y[rnd_indices]\n",
    "                    feed_dict = {self._X: X_batch, self._y: y_batch}\n",
    "                    if self._training is not None:\n",
    "                        feed_dict[self._training] = True\n",
    "                    sess.run(self._training_op, feed_dict=feed_dict)\n",
    "                    if extra_update_ops:\n",
    "                        sess.run(extra_update_ops, feed_dict=feed_dict)\n",
    "                if X_valid is not None and y_valid is not None:\n",
    "                    loss_val, acc_val = sess.run([self._loss, self._accuracy],\n",
    "                                                 feed_dict={self._X: X_valid,\n",
    "                                                            self._y: y_valid})\n",
    "                    if loss_val < best_loss:\n",
    "                        best_params = self._get_model_params()\n",
    "                        best_loss = loss_val\n",
    "                        checks_without_progress = 0\n",
    "                    else:\n",
    "                        checks_without_progress += 1\n",
    "                    print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        epoch, loss_val, best_loss, acc_val * 100))\n",
    "                    if checks_without_progress > max_checks_without_progress:\n",
    "                        print(\"Early stopping!\")\n",
    "                        break\n",
    "                else:\n",
    "                    loss_train, acc_train = sess.run([self._loss, self._accuracy],\n",
    "                                                     feed_dict={self._X: X_batch,\n",
    "                                                                self._y: y_batch})\n",
    "                    print(\"{}\\tLast training batch loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        epoch, loss_train, acc_train * 100))\n",
    "            # If we used early stopping then rollback to the best model found\n",
    "            if best_params:\n",
    "                self._restore_model_params(best_params)\n",
    "            return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if not self._session:\n",
    "            raise NotFittedError(\"This %s instance is not fitted yet\" % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            return self._Y_proba.eval(feed_dict={self._X: X})\n",
    "\n",
    "    def predict(self, X):\n",
    "        class_indices = np.argmax(self.predict_proba(X), axis=1)\n",
    "        return np.array([[self.classes_[class_index]]\n",
    "                         for class_index in class_indices], np.int32)\n",
    "\n",
    "    def save(self, path):\n",
    "        self._saver.save(self._session, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we get the exact same accuracy as earlier using this class (without dropout or batch norm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.126721\tBest loss: 0.126721\tAccuracy: 96.99%\n",
      "1\tValidation loss: 0.152275\tBest loss: 0.126721\tAccuracy: 97.42%\n",
      "2\tValidation loss: 0.233682\tBest loss: 0.126721\tAccuracy: 96.13%\n",
      "3\tValidation loss: 0.438649\tBest loss: 0.126721\tAccuracy: 88.74%\n",
      "4\tValidation loss: 0.258861\tBest loss: 0.126721\tAccuracy: 97.07%\n",
      "5\tValidation loss: 0.120900\tBest loss: 0.120900\tAccuracy: 97.97%\n",
      "6\tValidation loss: 1.633393\tBest loss: 0.120900\tAccuracy: 18.73%\n",
      "7\tValidation loss: 1.631511\tBest loss: 0.120900\tAccuracy: 18.73%\n",
      "8\tValidation loss: 1.703525\tBest loss: 0.120900\tAccuracy: 19.27%\n",
      "9\tValidation loss: 1.663757\tBest loss: 0.120900\tAccuracy: 19.08%\n",
      "10\tValidation loss: 1.638141\tBest loss: 0.120900\tAccuracy: 22.01%\n",
      "11\tValidation loss: 1.691426\tBest loss: 0.120900\tAccuracy: 19.27%\n",
      "12\tValidation loss: 1.671014\tBest loss: 0.120900\tAccuracy: 18.73%\n",
      "13\tValidation loss: 1.621311\tBest loss: 0.120900\tAccuracy: 20.91%\n",
      "14\tValidation loss: 1.638140\tBest loss: 0.120900\tAccuracy: 20.91%\n",
      "15\tValidation loss: 1.638748\tBest loss: 0.120900\tAccuracy: 20.91%\n",
      "16\tValidation loss: 1.718888\tBest loss: 0.120900\tAccuracy: 19.08%\n",
      "17\tValidation loss: 1.682493\tBest loss: 0.120900\tAccuracy: 19.27%\n",
      "18\tValidation loss: 1.675364\tBest loss: 0.120900\tAccuracy: 18.73%\n",
      "19\tValidation loss: 1.645801\tBest loss: 0.120900\tAccuracy: 19.08%\n",
      "20\tValidation loss: 1.722334\tBest loss: 0.120900\tAccuracy: 22.01%\n",
      "21\tValidation loss: 1.656415\tBest loss: 0.120900\tAccuracy: 22.01%\n",
      "22\tValidation loss: 1.643530\tBest loss: 0.120900\tAccuracy: 18.73%\n",
      "23\tValidation loss: 1.644233\tBest loss: 0.120900\tAccuracy: 19.27%\n",
      "24\tValidation loss: 1.690035\tBest loss: 0.120900\tAccuracy: 18.73%\n",
      "25\tValidation loss: 1.718947\tBest loss: 0.120900\tAccuracy: 18.73%\n",
      "26\tValidation loss: 1.672112\tBest loss: 0.120900\tAccuracy: 19.27%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function elu at 0x129284e18>,\n",
       "       batch_norm_momentum=None, batch_size=20, dropout_rate=None,\n",
       "       initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0xb39b8fda0>,\n",
       "       learning_rate=0.01, n_hidden_layers=5, n_neurons=100,\n",
       "       optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "       random_state=42)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = DNNClassifier(random_state=42)\n",
    "dnn_clf.fit(X_train1, y_train1, n_epochs=100, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9780112862424596"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = dnn_clf.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep! Working fine. Now we can use Scikit-Learn's `RandomizedSearchCV` class to search for better hyperparameters (this may take over an hour, depending on your system):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/model_selection/_search.py:584: DeprecationWarning: \"fit_params\" as a constructor argument was deprecated in version 0.19 and will be removed in version 0.21. Pass fit parameters to the \"fit\" method instead.\n",
      "  '\"fit\" method instead.', DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "[CV] activation=<function elu at 0x129284e18>, learning_rate=0.05, n_neurons=10, batch_size=100 \n",
      "0\tValidation loss: 0.163489\tBest loss: 0.163489\tAccuracy: 95.04%\n",
      "1\tValidation loss: 0.133876\tBest loss: 0.133876\tAccuracy: 96.40%\n",
      "2\tValidation loss: 0.119736\tBest loss: 0.119736\tAccuracy: 97.22%\n",
      "3\tValidation loss: 0.769582\tBest loss: 0.119736\tAccuracy: 70.33%\n",
      "4\tValidation loss: 0.505063\tBest loss: 0.119736\tAccuracy: 75.80%\n",
      "5\tValidation loss: 0.393608\tBest loss: 0.119736\tAccuracy: 79.12%\n",
      "6\tValidation loss: 0.367128\tBest loss: 0.119736\tAccuracy: 79.36%\n",
      "7\tValidation loss: 0.387097\tBest loss: 0.119736\tAccuracy: 79.98%\n",
      "8\tValidation loss: 0.385775\tBest loss: 0.119736\tAccuracy: 80.02%\n",
      "9\tValidation loss: 0.418006\tBest loss: 0.119736\tAccuracy: 77.60%\n",
      "10\tValidation loss: 0.374809\tBest loss: 0.119736\tAccuracy: 80.22%\n",
      "11\tValidation loss: 0.482855\tBest loss: 0.119736\tAccuracy: 77.05%\n",
      "12\tValidation loss: 0.391005\tBest loss: 0.119736\tAccuracy: 78.50%\n",
      "13\tValidation loss: 0.359630\tBest loss: 0.119736\tAccuracy: 78.85%\n",
      "14\tValidation loss: 0.421922\tBest loss: 0.119736\tAccuracy: 77.60%\n",
      "15\tValidation loss: 0.361048\tBest loss: 0.119736\tAccuracy: 78.46%\n",
      "16\tValidation loss: 0.424473\tBest loss: 0.119736\tAccuracy: 78.93%\n",
      "17\tValidation loss: 1.420146\tBest loss: 0.119736\tAccuracy: 32.13%\n",
      "18\tValidation loss: 1.390117\tBest loss: 0.119736\tAccuracy: 31.35%\n",
      "19\tValidation loss: 1.307044\tBest loss: 0.119736\tAccuracy: 36.00%\n",
      "20\tValidation loss: 1.302098\tBest loss: 0.119736\tAccuracy: 36.08%\n",
      "21\tValidation loss: 1.203369\tBest loss: 0.119736\tAccuracy: 38.94%\n",
      "22\tValidation loss: 1.199787\tBest loss: 0.119736\tAccuracy: 38.62%\n",
      "23\tValidation loss: 1.212798\tBest loss: 0.119736\tAccuracy: 39.09%\n",
      "Early stopping!\n",
      "[CV]  activation=<function elu at 0x129284e18>, learning_rate=0.05, n_neurons=10, batch_size=100, total=   8.2s\n",
      "[CV] activation=<function elu at 0x129284e18>, learning_rate=0.05, n_neurons=10, batch_size=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.135237\tBest loss: 0.135237\tAccuracy: 96.60%\n",
      "1\tValidation loss: 0.145759\tBest loss: 0.135237\tAccuracy: 95.54%\n",
      "2\tValidation loss: 0.164811\tBest loss: 0.135237\tAccuracy: 96.05%\n",
      "3\tValidation loss: 0.185358\tBest loss: 0.135237\tAccuracy: 96.33%\n",
      "4\tValidation loss: 0.139947\tBest loss: 0.135237\tAccuracy: 96.95%\n",
      "5\tValidation loss: 0.298661\tBest loss: 0.135237\tAccuracy: 93.00%\n",
      "6\tValidation loss: 1.642965\tBest loss: 0.135237\tAccuracy: 19.08%\n",
      "7\tValidation loss: 1.627335\tBest loss: 0.135237\tAccuracy: 18.73%\n",
      "8\tValidation loss: 1.614714\tBest loss: 0.135237\tAccuracy: 19.27%\n",
      "9\tValidation loss: 1.618463\tBest loss: 0.135237\tAccuracy: 22.01%\n",
      "10\tValidation loss: 1.609363\tBest loss: 0.135237\tAccuracy: 22.01%\n",
      "11\tValidation loss: 1.649849\tBest loss: 0.135237\tAccuracy: 19.08%\n",
      "12\tValidation loss: 1.641160\tBest loss: 0.135237\tAccuracy: 19.08%\n",
      "13\tValidation loss: 1.630665\tBest loss: 0.135237\tAccuracy: 19.27%\n",
      "14\tValidation loss: 1.618600\tBest loss: 0.135237\tAccuracy: 22.01%\n",
      "15\tValidation loss: 1.614877\tBest loss: 0.135237\tAccuracy: 22.01%\n",
      "16\tValidation loss: 1.624016\tBest loss: 0.135237\tAccuracy: 22.01%\n",
      "17\tValidation loss: 1.616499\tBest loss: 0.135237\tAccuracy: 20.91%\n",
      "18\tValidation loss: 1.616123\tBest loss: 0.135237\tAccuracy: 22.01%\n",
      "19\tValidation loss: 1.612977\tBest loss: 0.135237\tAccuracy: 19.27%\n",
      "20\tValidation loss: 1.615308\tBest loss: 0.135237\tAccuracy: 19.27%\n",
      "21\tValidation loss: 1.664066\tBest loss: 0.135237\tAccuracy: 22.01%\n",
      "Early stopping!\n",
      "[CV]  activation=<function elu at 0x129284e18>, learning_rate=0.05, n_neurons=10, batch_size=100, total=   8.2s\n",
      "[CV] activation=<function elu at 0x129284e18>, learning_rate=0.05, n_neurons=10, batch_size=100 \n",
      "0\tValidation loss: 0.132428\tBest loss: 0.132428\tAccuracy: 96.21%\n",
      "1\tValidation loss: 0.133276\tBest loss: 0.132428\tAccuracy: 96.68%\n",
      "2\tValidation loss: 0.137844\tBest loss: 0.132428\tAccuracy: 96.64%\n",
      "3\tValidation loss: 0.147014\tBest loss: 0.132428\tAccuracy: 96.56%\n",
      "4\tValidation loss: 0.154728\tBest loss: 0.132428\tAccuracy: 96.40%\n",
      "5\tValidation loss: 0.109898\tBest loss: 0.109898\tAccuracy: 96.79%\n",
      "6\tValidation loss: 0.120111\tBest loss: 0.109898\tAccuracy: 96.99%\n",
      "7\tValidation loss: 1.439282\tBest loss: 0.109898\tAccuracy: 41.83%\n",
      "8\tValidation loss: 0.946822\tBest loss: 0.109898\tAccuracy: 56.25%\n",
      "9\tValidation loss: 0.779816\tBest loss: 0.109898\tAccuracy: 56.72%\n",
      "10\tValidation loss: 0.797473\tBest loss: 0.109898\tAccuracy: 56.65%\n",
      "11\tValidation loss: 0.694416\tBest loss: 0.109898\tAccuracy: 73.03%\n",
      "12\tValidation loss: 0.581611\tBest loss: 0.109898\tAccuracy: 73.10%\n",
      "13\tValidation loss: 0.503349\tBest loss: 0.109898\tAccuracy: 76.15%\n",
      "14\tValidation loss: 0.509365\tBest loss: 0.109898\tAccuracy: 77.09%\n",
      "15\tValidation loss: 0.496259\tBest loss: 0.109898\tAccuracy: 76.35%\n",
      "16\tValidation loss: 0.508826\tBest loss: 0.109898\tAccuracy: 78.38%\n",
      "17\tValidation loss: 0.622025\tBest loss: 0.109898\tAccuracy: 69.82%\n",
      "18\tValidation loss: 0.471002\tBest loss: 0.109898\tAccuracy: 77.17%\n",
      "19\tValidation loss: 0.875533\tBest loss: 0.109898\tAccuracy: 59.11%\n",
      "20\tValidation loss: 0.551760\tBest loss: 0.109898\tAccuracy: 77.05%\n",
      "21\tValidation loss: 0.520151\tBest loss: 0.109898\tAccuracy: 77.60%\n",
      "22\tValidation loss: 0.497015\tBest loss: 0.109898\tAccuracy: 77.91%\n",
      "23\tValidation loss: 0.455520\tBest loss: 0.109898\tAccuracy: 77.80%\n",
      "24\tValidation loss: 0.454755\tBest loss: 0.109898\tAccuracy: 77.33%\n",
      "25\tValidation loss: 0.606302\tBest loss: 0.109898\tAccuracy: 77.01%\n",
      "26\tValidation loss: 0.485146\tBest loss: 0.109898\tAccuracy: 78.81%\n",
      "Early stopping!\n",
      "[CV]  activation=<function elu at 0x129284e18>, learning_rate=0.05, n_neurons=10, batch_size=100, total=   9.5s\n",
      "[CV] activation=<function relu at 0x12920c1e0>, learning_rate=0.02, n_neurons=30, batch_size=500 \n",
      "0\tValidation loss: 0.123844\tBest loss: 0.123844\tAccuracy: 96.48%\n",
      "1\tValidation loss: 0.081994\tBest loss: 0.081994\tAccuracy: 97.58%\n",
      "2\tValidation loss: 0.081470\tBest loss: 0.081470\tAccuracy: 97.89%\n",
      "3\tValidation loss: 0.075482\tBest loss: 0.075482\tAccuracy: 98.01%\n",
      "4\tValidation loss: 0.084189\tBest loss: 0.075482\tAccuracy: 97.69%\n",
      "5\tValidation loss: 0.067171\tBest loss: 0.067171\tAccuracy: 97.93%\n",
      "6\tValidation loss: 0.065851\tBest loss: 0.065851\tAccuracy: 98.08%\n",
      "7\tValidation loss: 0.080103\tBest loss: 0.065851\tAccuracy: 98.08%\n",
      "8\tValidation loss: 0.082318\tBest loss: 0.065851\tAccuracy: 97.93%\n",
      "9\tValidation loss: 0.071792\tBest loss: 0.065851\tAccuracy: 98.40%\n",
      "10\tValidation loss: 0.080456\tBest loss: 0.065851\tAccuracy: 98.32%\n",
      "11\tValidation loss: 0.071958\tBest loss: 0.065851\tAccuracy: 98.48%\n",
      "12\tValidation loss: 0.077912\tBest loss: 0.065851\tAccuracy: 98.16%\n",
      "13\tValidation loss: 0.073074\tBest loss: 0.065851\tAccuracy: 98.44%\n",
      "14\tValidation loss: 0.079857\tBest loss: 0.065851\tAccuracy: 97.93%\n",
      "15\tValidation loss: 0.074989\tBest loss: 0.065851\tAccuracy: 98.08%\n",
      "16\tValidation loss: 0.071820\tBest loss: 0.065851\tAccuracy: 98.08%\n",
      "17\tValidation loss: 0.093496\tBest loss: 0.065851\tAccuracy: 98.12%\n",
      "18\tValidation loss: 0.064486\tBest loss: 0.064486\tAccuracy: 98.67%\n",
      "19\tValidation loss: 0.089902\tBest loss: 0.064486\tAccuracy: 98.12%\n",
      "20\tValidation loss: 0.092854\tBest loss: 0.064486\tAccuracy: 98.01%\n",
      "21\tValidation loss: 0.094933\tBest loss: 0.064486\tAccuracy: 98.05%\n",
      "22\tValidation loss: 0.076796\tBest loss: 0.064486\tAccuracy: 98.51%\n",
      "23\tValidation loss: 0.095581\tBest loss: 0.064486\tAccuracy: 98.32%\n",
      "24\tValidation loss: 0.082184\tBest loss: 0.064486\tAccuracy: 98.28%\n",
      "25\tValidation loss: 0.096895\tBest loss: 0.064486\tAccuracy: 98.05%\n",
      "26\tValidation loss: 0.078305\tBest loss: 0.064486\tAccuracy: 98.44%\n",
      "27\tValidation loss: 0.084345\tBest loss: 0.064486\tAccuracy: 98.24%\n",
      "28\tValidation loss: 0.075331\tBest loss: 0.064486\tAccuracy: 98.55%\n",
      "29\tValidation loss: 0.104153\tBest loss: 0.064486\tAccuracy: 98.28%\n",
      "30\tValidation loss: 0.105435\tBest loss: 0.064486\tAccuracy: 98.32%\n",
      "31\tValidation loss: 0.095090\tBest loss: 0.064486\tAccuracy: 98.44%\n",
      "32\tValidation loss: 0.134710\tBest loss: 0.064486\tAccuracy: 98.12%\n",
      "33\tValidation loss: 0.139016\tBest loss: 0.064486\tAccuracy: 97.54%\n",
      "34\tValidation loss: 0.126024\tBest loss: 0.064486\tAccuracy: 98.24%\n",
      "35\tValidation loss: 0.132409\tBest loss: 0.064486\tAccuracy: 98.05%\n",
      "36\tValidation loss: 0.109510\tBest loss: 0.064486\tAccuracy: 98.48%\n",
      "37\tValidation loss: 0.121012\tBest loss: 0.064486\tAccuracy: 98.20%\n",
      "38\tValidation loss: 0.121399\tBest loss: 0.064486\tAccuracy: 97.93%\n",
      "39\tValidation loss: 0.135389\tBest loss: 0.064486\tAccuracy: 97.26%\n",
      "Early stopping!\n",
      "[CV]  activation=<function relu at 0x12920c1e0>, learning_rate=0.02, n_neurons=30, batch_size=500, total=   9.7s\n",
      "[CV] activation=<function relu at 0x12920c1e0>, learning_rate=0.02, n_neurons=30, batch_size=500 \n",
      "0\tValidation loss: 0.118566\tBest loss: 0.118566\tAccuracy: 96.72%\n",
      "1\tValidation loss: 0.080641\tBest loss: 0.080641\tAccuracy: 97.85%\n",
      "2\tValidation loss: 0.063092\tBest loss: 0.063092\tAccuracy: 98.05%\n",
      "3\tValidation loss: 0.059220\tBest loss: 0.059220\tAccuracy: 98.48%\n",
      "4\tValidation loss: 0.075472\tBest loss: 0.059220\tAccuracy: 97.93%\n",
      "5\tValidation loss: 0.061205\tBest loss: 0.059220\tAccuracy: 98.55%\n",
      "6\tValidation loss: 0.063930\tBest loss: 0.059220\tAccuracy: 98.48%\n",
      "7\tValidation loss: 0.063106\tBest loss: 0.059220\tAccuracy: 98.32%\n",
      "8\tValidation loss: 0.076880\tBest loss: 0.059220\tAccuracy: 97.93%\n",
      "9\tValidation loss: 0.075046\tBest loss: 0.059220\tAccuracy: 98.16%\n",
      "10\tValidation loss: 0.062531\tBest loss: 0.059220\tAccuracy: 98.55%\n",
      "11\tValidation loss: 0.069311\tBest loss: 0.059220\tAccuracy: 98.51%\n",
      "12\tValidation loss: 0.068718\tBest loss: 0.059220\tAccuracy: 98.36%\n",
      "13\tValidation loss: 0.060551\tBest loss: 0.059220\tAccuracy: 98.67%\n",
      "14\tValidation loss: 0.059445\tBest loss: 0.059220\tAccuracy: 98.63%\n",
      "15\tValidation loss: 0.072511\tBest loss: 0.059220\tAccuracy: 98.55%\n",
      "16\tValidation loss: 0.054440\tBest loss: 0.054440\tAccuracy: 98.98%\n",
      "17\tValidation loss: 0.066542\tBest loss: 0.054440\tAccuracy: 98.36%\n",
      "18\tValidation loss: 0.088445\tBest loss: 0.054440\tAccuracy: 98.36%\n",
      "19\tValidation loss: 0.095281\tBest loss: 0.054440\tAccuracy: 97.93%\n",
      "20\tValidation loss: 0.082575\tBest loss: 0.054440\tAccuracy: 98.51%\n",
      "21\tValidation loss: 0.077274\tBest loss: 0.054440\tAccuracy: 98.44%\n",
      "22\tValidation loss: 0.069299\tBest loss: 0.054440\tAccuracy: 98.51%\n",
      "23\tValidation loss: 0.071802\tBest loss: 0.054440\tAccuracy: 98.75%\n",
      "24\tValidation loss: 0.083413\tBest loss: 0.054440\tAccuracy: 98.55%\n",
      "25\tValidation loss: 0.103696\tBest loss: 0.054440\tAccuracy: 98.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\tValidation loss: 0.111898\tBest loss: 0.054440\tAccuracy: 98.63%\n",
      "27\tValidation loss: 0.137866\tBest loss: 0.054440\tAccuracy: 97.77%\n",
      "28\tValidation loss: 0.102089\tBest loss: 0.054440\tAccuracy: 97.97%\n",
      "29\tValidation loss: 0.079758\tBest loss: 0.054440\tAccuracy: 98.44%\n",
      "30\tValidation loss: 0.096021\tBest loss: 0.054440\tAccuracy: 98.59%\n",
      "31\tValidation loss: 0.074475\tBest loss: 0.054440\tAccuracy: 98.79%\n",
      "32\tValidation loss: 0.119091\tBest loss: 0.054440\tAccuracy: 98.71%\n",
      "33\tValidation loss: 0.106834\tBest loss: 0.054440\tAccuracy: 98.55%\n",
      "34\tValidation loss: 0.093877\tBest loss: 0.054440\tAccuracy: 98.59%\n",
      "35\tValidation loss: 0.080391\tBest loss: 0.054440\tAccuracy: 98.51%\n",
      "36\tValidation loss: 0.108990\tBest loss: 0.054440\tAccuracy: 98.71%\n",
      "37\tValidation loss: 0.090491\tBest loss: 0.054440\tAccuracy: 98.59%\n",
      "Early stopping!\n",
      "[CV]  activation=<function relu at 0x12920c1e0>, learning_rate=0.02, n_neurons=30, batch_size=500, total=   8.9s\n",
      "[CV] activation=<function relu at 0x12920c1e0>, learning_rate=0.02, n_neurons=30, batch_size=500 \n",
      "0\tValidation loss: 0.115925\tBest loss: 0.115925\tAccuracy: 96.36%\n",
      "1\tValidation loss: 0.076872\tBest loss: 0.076872\tAccuracy: 97.58%\n",
      "2\tValidation loss: 0.067116\tBest loss: 0.067116\tAccuracy: 98.01%\n",
      "3\tValidation loss: 0.058144\tBest loss: 0.058144\tAccuracy: 98.36%\n",
      "4\tValidation loss: 0.074453\tBest loss: 0.058144\tAccuracy: 98.16%\n",
      "5\tValidation loss: 0.054237\tBest loss: 0.054237\tAccuracy: 98.40%\n",
      "6\tValidation loss: 0.061965\tBest loss: 0.054237\tAccuracy: 98.48%\n",
      "7\tValidation loss: 0.090012\tBest loss: 0.054237\tAccuracy: 97.77%\n",
      "8\tValidation loss: 0.075915\tBest loss: 0.054237\tAccuracy: 98.16%\n",
      "9\tValidation loss: 0.080275\tBest loss: 0.054237\tAccuracy: 98.12%\n",
      "10\tValidation loss: 0.067947\tBest loss: 0.054237\tAccuracy: 98.24%\n",
      "11\tValidation loss: 0.068969\tBest loss: 0.054237\tAccuracy: 98.20%\n",
      "12\tValidation loss: 0.084179\tBest loss: 0.054237\tAccuracy: 98.28%\n",
      "13\tValidation loss: 0.070090\tBest loss: 0.054237\tAccuracy: 98.36%\n",
      "14\tValidation loss: 0.075025\tBest loss: 0.054237\tAccuracy: 98.40%\n",
      "15\tValidation loss: 0.066218\tBest loss: 0.054237\tAccuracy: 98.44%\n",
      "16\tValidation loss: 0.070396\tBest loss: 0.054237\tAccuracy: 98.59%\n",
      "17\tValidation loss: 0.069024\tBest loss: 0.054237\tAccuracy: 98.63%\n",
      "18\tValidation loss: 0.071814\tBest loss: 0.054237\tAccuracy: 98.51%\n",
      "19\tValidation loss: 0.076222\tBest loss: 0.054237\tAccuracy: 98.51%\n",
      "20\tValidation loss: 0.059068\tBest loss: 0.054237\tAccuracy: 98.44%\n",
      "21\tValidation loss: 0.083172\tBest loss: 0.054237\tAccuracy: 98.48%\n",
      "22\tValidation loss: 0.067008\tBest loss: 0.054237\tAccuracy: 98.63%\n",
      "23\tValidation loss: 0.090917\tBest loss: 0.054237\tAccuracy: 98.40%\n",
      "24\tValidation loss: 0.074512\tBest loss: 0.054237\tAccuracy: 98.63%\n",
      "25\tValidation loss: 0.094484\tBest loss: 0.054237\tAccuracy: 98.55%\n",
      "26\tValidation loss: 0.089215\tBest loss: 0.054237\tAccuracy: 98.48%\n",
      "Early stopping!\n",
      "[CV]  activation=<function relu at 0x12920c1e0>, learning_rate=0.02, n_neurons=30, batch_size=500, total=   6.3s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.05, n_neurons=90, batch_size=50 \n",
      "0\tValidation loss: 103.228485\tBest loss: 103.228485\tAccuracy: 55.36%\n",
      "1\tValidation loss: 14.093122\tBest loss: 14.093122\tAccuracy: 60.71%\n",
      "2\tValidation loss: 2.197721\tBest loss: 2.197721\tAccuracy: 83.70%\n",
      "3\tValidation loss: 1.159829\tBest loss: 1.159829\tAccuracy: 82.06%\n",
      "4\tValidation loss: 1.771994\tBest loss: 1.159829\tAccuracy: 78.19%\n",
      "5\tValidation loss: 1.802980\tBest loss: 1.159829\tAccuracy: 81.20%\n",
      "6\tValidation loss: 0.654191\tBest loss: 0.654191\tAccuracy: 90.93%\n",
      "7\tValidation loss: 0.854373\tBest loss: 0.654191\tAccuracy: 87.76%\n",
      "8\tValidation loss: 0.430180\tBest loss: 0.430180\tAccuracy: 92.34%\n",
      "9\tValidation loss: 5745.417480\tBest loss: 0.430180\tAccuracy: 19.12%\n",
      "10\tValidation loss: 167.091766\tBest loss: 0.430180\tAccuracy: 34.48%\n",
      "11\tValidation loss: 35.349941\tBest loss: 0.430180\tAccuracy: 36.90%\n",
      "12\tValidation loss: 17.548634\tBest loss: 0.430180\tAccuracy: 57.23%\n",
      "13\tValidation loss: 21.009371\tBest loss: 0.430180\tAccuracy: 45.47%\n",
      "14\tValidation loss: 44.470722\tBest loss: 0.430180\tAccuracy: 34.17%\n",
      "15\tValidation loss: 15.480438\tBest loss: 0.430180\tAccuracy: 58.87%\n",
      "16\tValidation loss: 22.483549\tBest loss: 0.430180\tAccuracy: 63.76%\n",
      "17\tValidation loss: 124.452682\tBest loss: 0.430180\tAccuracy: 36.71%\n",
      "18\tValidation loss: 401.870270\tBest loss: 0.430180\tAccuracy: 39.33%\n",
      "19\tValidation loss: 366.740997\tBest loss: 0.430180\tAccuracy: 50.82%\n",
      "20\tValidation loss: 83.906197\tBest loss: 0.430180\tAccuracy: 67.71%\n",
      "21\tValidation loss: 138.810883\tBest loss: 0.430180\tAccuracy: 76.00%\n",
      "22\tValidation loss: 49.128784\tBest loss: 0.430180\tAccuracy: 77.29%\n",
      "23\tValidation loss: 51.494808\tBest loss: 0.430180\tAccuracy: 76.04%\n",
      "24\tValidation loss: 35.646675\tBest loss: 0.430180\tAccuracy: 90.30%\n",
      "25\tValidation loss: 1696.727295\tBest loss: 0.430180\tAccuracy: 34.05%\n",
      "26\tValidation loss: 41.309216\tBest loss: 0.430180\tAccuracy: 86.43%\n",
      "27\tValidation loss: 50.966393\tBest loss: 0.430180\tAccuracy: 83.46%\n",
      "28\tValidation loss: 35.008610\tBest loss: 0.430180\tAccuracy: 92.89%\n",
      "29\tValidation loss: 41.314182\tBest loss: 0.430180\tAccuracy: 87.10%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.05, n_neurons=90, batch_size=50, total=  34.2s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.05, n_neurons=90, batch_size=50 \n",
      "0\tValidation loss: 63.474274\tBest loss: 63.474274\tAccuracy: 26.74%\n",
      "1\tValidation loss: 3.977245\tBest loss: 3.977245\tAccuracy: 71.97%\n",
      "2\tValidation loss: 2.550418\tBest loss: 2.550418\tAccuracy: 74.82%\n",
      "3\tValidation loss: 7.759871\tBest loss: 2.550418\tAccuracy: 73.92%\n",
      "4\tValidation loss: 2.533108\tBest loss: 2.533108\tAccuracy: 85.69%\n",
      "5\tValidation loss: 3.037193\tBest loss: 2.533108\tAccuracy: 84.40%\n",
      "6\tValidation loss: 1.596668\tBest loss: 1.596668\tAccuracy: 88.70%\n",
      "7\tValidation loss: 1.293652\tBest loss: 1.293652\tAccuracy: 89.95%\n",
      "8\tValidation loss: 1.359713\tBest loss: 1.293652\tAccuracy: 87.33%\n",
      "9\tValidation loss: 0.782853\tBest loss: 0.782853\tAccuracy: 91.13%\n",
      "10\tValidation loss: 1.022293\tBest loss: 0.782853\tAccuracy: 86.40%\n",
      "11\tValidation loss: 3069370.750000\tBest loss: 0.782853\tAccuracy: 39.87%\n",
      "12\tValidation loss: 499.849976\tBest loss: 0.782853\tAccuracy: 65.56%\n",
      "13\tValidation loss: 254.824905\tBest loss: 0.782853\tAccuracy: 73.77%\n",
      "14\tValidation loss: 110.816422\tBest loss: 0.782853\tAccuracy: 88.58%\n",
      "15\tValidation loss: 166.025940\tBest loss: 0.782853\tAccuracy: 78.58%\n",
      "16\tValidation loss: 99.183884\tBest loss: 0.782853\tAccuracy: 91.13%\n",
      "17\tValidation loss: 104.939003\tBest loss: 0.782853\tAccuracy: 88.58%\n",
      "18\tValidation loss: 79.433899\tBest loss: 0.782853\tAccuracy: 84.17%\n",
      "19\tValidation loss: 75.041679\tBest loss: 0.782853\tAccuracy: 89.72%\n",
      "20\tValidation loss: 555.179016\tBest loss: 0.782853\tAccuracy: 77.52%\n",
      "21\tValidation loss: 353.123596\tBest loss: 0.782853\tAccuracy: 79.91%\n",
      "22\tValidation loss: 543.602905\tBest loss: 0.782853\tAccuracy: 70.25%\n",
      "23\tValidation loss: 163.708237\tBest loss: 0.782853\tAccuracy: 82.84%\n",
      "24\tValidation loss: 353.898743\tBest loss: 0.782853\tAccuracy: 74.12%\n",
      "25\tValidation loss: 97.458717\tBest loss: 0.782853\tAccuracy: 87.96%\n",
      "26\tValidation loss: 108.462303\tBest loss: 0.782853\tAccuracy: 91.87%\n",
      "27\tValidation loss: 90.522072\tBest loss: 0.782853\tAccuracy: 92.14%\n",
      "28\tValidation loss: 112.898895\tBest loss: 0.782853\tAccuracy: 94.33%\n",
      "29\tValidation loss: 324.916840\tBest loss: 0.782853\tAccuracy: 89.56%\n",
      "30\tValidation loss: 111.023628\tBest loss: 0.782853\tAccuracy: 95.35%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.05, n_neurons=90, batch_size=50, total=  33.4s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.05, n_neurons=90, batch_size=50 \n",
      "0\tValidation loss: 0.815395\tBest loss: 0.815395\tAccuracy: 73.14%\n",
      "1\tValidation loss: 0.506267\tBest loss: 0.506267\tAccuracy: 85.38%\n",
      "2\tValidation loss: 0.462600\tBest loss: 0.462600\tAccuracy: 85.50%\n",
      "3\tValidation loss: 0.314935\tBest loss: 0.314935\tAccuracy: 90.34%\n",
      "4\tValidation loss: 0.458779\tBest loss: 0.314935\tAccuracy: 88.90%\n",
      "5\tValidation loss: 88.917892\tBest loss: 0.314935\tAccuracy: 48.75%\n",
      "6\tValidation loss: 33.100758\tBest loss: 0.314935\tAccuracy: 53.71%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\tValidation loss: 27.591993\tBest loss: 0.314935\tAccuracy: 58.09%\n",
      "8\tValidation loss: 20.542816\tBest loss: 0.314935\tAccuracy: 67.36%\n",
      "9\tValidation loss: 8.394818\tBest loss: 0.314935\tAccuracy: 74.78%\n",
      "10\tValidation loss: 7.288506\tBest loss: 0.314935\tAccuracy: 79.16%\n",
      "11\tValidation loss: 410.676849\tBest loss: 0.314935\tAccuracy: 37.57%\n",
      "12\tValidation loss: 267.917236\tBest loss: 0.314935\tAccuracy: 48.75%\n",
      "13\tValidation loss: 73.418243\tBest loss: 0.314935\tAccuracy: 64.07%\n",
      "14\tValidation loss: 44.319416\tBest loss: 0.314935\tAccuracy: 78.54%\n",
      "15\tValidation loss: 19.304142\tBest loss: 0.314935\tAccuracy: 86.00%\n",
      "16\tValidation loss: 36.966705\tBest loss: 0.314935\tAccuracy: 79.91%\n",
      "17\tValidation loss: 311.287140\tBest loss: 0.314935\tAccuracy: 71.34%\n",
      "18\tValidation loss: 72.757744\tBest loss: 0.314935\tAccuracy: 84.60%\n",
      "19\tValidation loss: 50.592232\tBest loss: 0.314935\tAccuracy: 83.89%\n",
      "20\tValidation loss: 46.770527\tBest loss: 0.314935\tAccuracy: 85.61%\n",
      "21\tValidation loss: 131.143295\tBest loss: 0.314935\tAccuracy: 73.06%\n",
      "22\tValidation loss: 26.369406\tBest loss: 0.314935\tAccuracy: 86.28%\n",
      "23\tValidation loss: 662.716797\tBest loss: 0.314935\tAccuracy: 74.90%\n",
      "24\tValidation loss: 118.011497\tBest loss: 0.314935\tAccuracy: 87.92%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.05, n_neurons=90, batch_size=50, total=  26.5s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.1, n_neurons=70, batch_size=50 \n",
      "0\tValidation loss: 16223.305664\tBest loss: 16223.305664\tAccuracy: 51.06%\n",
      "1\tValidation loss: 1355.662964\tBest loss: 1355.662964\tAccuracy: 79.98%\n",
      "2\tValidation loss: 805.149597\tBest loss: 805.149597\tAccuracy: 88.47%\n",
      "3\tValidation loss: 517.710266\tBest loss: 517.710266\tAccuracy: 87.14%\n",
      "4\tValidation loss: 1311.818115\tBest loss: 517.710266\tAccuracy: 88.00%\n",
      "5\tValidation loss: 815.264282\tBest loss: 517.710266\tAccuracy: 93.28%\n",
      "6\tValidation loss: 380.903442\tBest loss: 380.903442\tAccuracy: 93.51%\n",
      "7\tValidation loss: 1243.561768\tBest loss: 380.903442\tAccuracy: 91.67%\n",
      "8\tValidation loss: 384.050842\tBest loss: 380.903442\tAccuracy: 94.21%\n",
      "9\tValidation loss: 536.970825\tBest loss: 380.903442\tAccuracy: 91.99%\n",
      "10\tValidation loss: 223.109222\tBest loss: 223.109222\tAccuracy: 95.70%\n",
      "11\tValidation loss: 204.216034\tBest loss: 204.216034\tAccuracy: 95.93%\n",
      "12\tValidation loss: 119.605721\tBest loss: 119.605721\tAccuracy: 95.70%\n",
      "13\tValidation loss: 126.952560\tBest loss: 119.605721\tAccuracy: 95.31%\n",
      "14\tValidation loss: 153.579224\tBest loss: 119.605721\tAccuracy: 95.78%\n",
      "15\tValidation loss: 275.548706\tBest loss: 119.605721\tAccuracy: 95.39%\n",
      "16\tValidation loss: 206543.453125\tBest loss: 119.605721\tAccuracy: 90.07%\n",
      "17\tValidation loss: 58357.761719\tBest loss: 119.605721\tAccuracy: 95.78%\n",
      "18\tValidation loss: 45411.082031\tBest loss: 119.605721\tAccuracy: 96.33%\n",
      "19\tValidation loss: 57540.773438\tBest loss: 119.605721\tAccuracy: 95.58%\n",
      "20\tValidation loss: 21072.326172\tBest loss: 119.605721\tAccuracy: 96.95%\n",
      "21\tValidation loss: 23686.539062\tBest loss: 119.605721\tAccuracy: 96.95%\n",
      "22\tValidation loss: 30830.160156\tBest loss: 119.605721\tAccuracy: 97.26%\n",
      "23\tValidation loss: 30027.941406\tBest loss: 119.605721\tAccuracy: 96.25%\n",
      "24\tValidation loss: 24693.312500\tBest loss: 119.605721\tAccuracy: 96.52%\n",
      "25\tValidation loss: 16194.788086\tBest loss: 119.605721\tAccuracy: 97.11%\n",
      "26\tValidation loss: 66107.695312\tBest loss: 119.605721\tAccuracy: 97.30%\n",
      "27\tValidation loss: 25529.050781\tBest loss: 119.605721\tAccuracy: 97.42%\n",
      "28\tValidation loss: 21368.962891\tBest loss: 119.605721\tAccuracy: 97.65%\n",
      "29\tValidation loss: 21861.281250\tBest loss: 119.605721\tAccuracy: 97.46%\n",
      "30\tValidation loss: 14579.310547\tBest loss: 119.605721\tAccuracy: 97.85%\n",
      "31\tValidation loss: 24451.386719\tBest loss: 119.605721\tAccuracy: 94.88%\n",
      "32\tValidation loss: 60555.390625\tBest loss: 119.605721\tAccuracy: 95.54%\n",
      "33\tValidation loss: 20489.765625\tBest loss: 119.605721\tAccuracy: 96.95%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.1, n_neurons=70, batch_size=50, total=  32.3s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.1, n_neurons=70, batch_size=50 \n",
      "0\tValidation loss: 1.301469\tBest loss: 1.301469\tAccuracy: 87.06%\n",
      "1\tValidation loss: 4.634439\tBest loss: 1.301469\tAccuracy: 63.02%\n",
      "2\tValidation loss: 0.774507\tBest loss: 0.774507\tAccuracy: 90.81%\n",
      "3\tValidation loss: 0.304559\tBest loss: 0.304559\tAccuracy: 95.15%\n",
      "4\tValidation loss: 21435.705078\tBest loss: 0.304559\tAccuracy: 87.57%\n",
      "5\tValidation loss: 6712.982910\tBest loss: 0.304559\tAccuracy: 92.57%\n",
      "6\tValidation loss: 8440.402344\tBest loss: 0.304559\tAccuracy: 93.04%\n",
      "7\tValidation loss: 11355.163086\tBest loss: 0.304559\tAccuracy: 87.84%\n",
      "8\tValidation loss: 7980.890625\tBest loss: 0.304559\tAccuracy: 92.49%\n",
      "9\tValidation loss: 2905.217285\tBest loss: 0.304559\tAccuracy: 96.17%\n",
      "10\tValidation loss: 18352.687500\tBest loss: 0.304559\tAccuracy: 91.13%\n",
      "11\tValidation loss: 3524.953857\tBest loss: 0.304559\tAccuracy: 95.90%\n",
      "12\tValidation loss: 2032.030884\tBest loss: 0.304559\tAccuracy: 95.23%\n",
      "13\tValidation loss: 3354.635254\tBest loss: 0.304559\tAccuracy: 92.18%\n",
      "14\tValidation loss: 330121.218750\tBest loss: 0.304559\tAccuracy: 32.56%\n",
      "15\tValidation loss: 5139.753906\tBest loss: 0.304559\tAccuracy: 94.64%\n",
      "16\tValidation loss: 2227.692139\tBest loss: 0.304559\tAccuracy: 96.40%\n",
      "17\tValidation loss: 1824.999268\tBest loss: 0.304559\tAccuracy: 95.19%\n",
      "18\tValidation loss: 1173.721069\tBest loss: 0.304559\tAccuracy: 96.83%\n",
      "19\tValidation loss: 6299.407227\tBest loss: 0.304559\tAccuracy: 94.64%\n",
      "20\tValidation loss: 2473.865234\tBest loss: 0.304559\tAccuracy: 95.97%\n",
      "21\tValidation loss: 855.907654\tBest loss: 0.304559\tAccuracy: 96.76%\n",
      "22\tValidation loss: 20811.222656\tBest loss: 0.304559\tAccuracy: 88.35%\n",
      "23\tValidation loss: 2773.717041\tBest loss: 0.304559\tAccuracy: 89.33%\n",
      "24\tValidation loss: 1171.208984\tBest loss: 0.304559\tAccuracy: 95.50%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.1, n_neurons=70, batch_size=50, total=  23.7s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.1, n_neurons=70, batch_size=50 \n",
      "0\tValidation loss: 331.822754\tBest loss: 331.822754\tAccuracy: 90.07%\n",
      "1\tValidation loss: 329.824249\tBest loss: 329.824249\tAccuracy: 86.71%\n",
      "2\tValidation loss: 89.438110\tBest loss: 89.438110\tAccuracy: 90.73%\n",
      "3\tValidation loss: 59989.546875\tBest loss: 89.438110\tAccuracy: 89.99%\n",
      "4\tValidation loss: 34912.218750\tBest loss: 89.438110\tAccuracy: 92.77%\n",
      "5\tValidation loss: 28058.324219\tBest loss: 89.438110\tAccuracy: 90.42%\n",
      "6\tValidation loss: 11321.753906\tBest loss: 89.438110\tAccuracy: 93.67%\n",
      "7\tValidation loss: 11707.875000\tBest loss: 89.438110\tAccuracy: 91.99%\n",
      "8\tValidation loss: 12724.045898\tBest loss: 89.438110\tAccuracy: 93.04%\n",
      "9\tValidation loss: 7067.854004\tBest loss: 89.438110\tAccuracy: 94.64%\n",
      "10\tValidation loss: 7288.822754\tBest loss: 89.438110\tAccuracy: 95.78%\n",
      "11\tValidation loss: 7629.655762\tBest loss: 89.438110\tAccuracy: 95.70%\n",
      "12\tValidation loss: 7157.807617\tBest loss: 89.438110\tAccuracy: 94.76%\n",
      "13\tValidation loss: 7325.032715\tBest loss: 89.438110\tAccuracy: 94.10%\n",
      "14\tValidation loss: 5875.416016\tBest loss: 89.438110\tAccuracy: 94.88%\n",
      "15\tValidation loss: 7990.090820\tBest loss: 89.438110\tAccuracy: 94.02%\n",
      "16\tValidation loss: 2225.986328\tBest loss: 89.438110\tAccuracy: 96.56%\n",
      "17\tValidation loss: 3523448.000000\tBest loss: 89.438110\tAccuracy: 93.08%\n",
      "18\tValidation loss: 189600.671875\tBest loss: 89.438110\tAccuracy: 96.17%\n",
      "19\tValidation loss: 180422.562500\tBest loss: 89.438110\tAccuracy: 95.04%\n",
      "20\tValidation loss: 102715.046875\tBest loss: 89.438110\tAccuracy: 94.18%\n",
      "21\tValidation loss: 95772.921875\tBest loss: 89.438110\tAccuracy: 95.78%\n",
      "22\tValidation loss: 74888.742188\tBest loss: 89.438110\tAccuracy: 96.29%\n",
      "23\tValidation loss: 90773.695312\tBest loss: 89.438110\tAccuracy: 96.09%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.1, n_neurons=70, batch_size=50, total=  22.7s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.01, n_neurons=120, batch_size=500 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.109824\tBest loss: 0.109824\tAccuracy: 96.79%\n",
      "1\tValidation loss: 0.062282\tBest loss: 0.062282\tAccuracy: 98.08%\n",
      "2\tValidation loss: 0.067534\tBest loss: 0.062282\tAccuracy: 98.16%\n",
      "3\tValidation loss: 0.057334\tBest loss: 0.057334\tAccuracy: 98.40%\n",
      "4\tValidation loss: 0.051163\tBest loss: 0.051163\tAccuracy: 98.59%\n",
      "5\tValidation loss: 0.056141\tBest loss: 0.051163\tAccuracy: 98.67%\n",
      "6\tValidation loss: 0.059677\tBest loss: 0.051163\tAccuracy: 98.32%\n",
      "7\tValidation loss: 0.056883\tBest loss: 0.051163\tAccuracy: 98.40%\n",
      "8\tValidation loss: 0.065996\tBest loss: 0.051163\tAccuracy: 98.51%\n",
      "9\tValidation loss: 0.063311\tBest loss: 0.051163\tAccuracy: 98.44%\n",
      "10\tValidation loss: 0.063939\tBest loss: 0.051163\tAccuracy: 98.71%\n",
      "11\tValidation loss: 0.071703\tBest loss: 0.051163\tAccuracy: 98.51%\n",
      "12\tValidation loss: 0.066804\tBest loss: 0.051163\tAccuracy: 98.55%\n",
      "13\tValidation loss: 0.085716\tBest loss: 0.051163\tAccuracy: 98.44%\n",
      "14\tValidation loss: 0.079161\tBest loss: 0.051163\tAccuracy: 98.32%\n",
      "15\tValidation loss: 0.098848\tBest loss: 0.051163\tAccuracy: 98.40%\n",
      "16\tValidation loss: 0.057709\tBest loss: 0.051163\tAccuracy: 98.67%\n",
      "17\tValidation loss: 0.075228\tBest loss: 0.051163\tAccuracy: 98.79%\n",
      "18\tValidation loss: 0.082543\tBest loss: 0.051163\tAccuracy: 98.63%\n",
      "19\tValidation loss: 0.068890\tBest loss: 0.051163\tAccuracy: 98.87%\n",
      "20\tValidation loss: 0.086662\tBest loss: 0.051163\tAccuracy: 98.55%\n",
      "21\tValidation loss: 0.069746\tBest loss: 0.051163\tAccuracy: 98.55%\n",
      "22\tValidation loss: 0.082476\tBest loss: 0.051163\tAccuracy: 98.59%\n",
      "23\tValidation loss: 0.098263\tBest loss: 0.051163\tAccuracy: 98.71%\n",
      "24\tValidation loss: 0.078280\tBest loss: 0.051163\tAccuracy: 99.02%\n",
      "25\tValidation loss: 0.091679\tBest loss: 0.051163\tAccuracy: 98.71%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.01, n_neurons=120, batch_size=500, total=  13.8s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.01, n_neurons=120, batch_size=500 \n",
      "0\tValidation loss: 0.105513\tBest loss: 0.105513\tAccuracy: 96.76%\n",
      "1\tValidation loss: 0.071428\tBest loss: 0.071428\tAccuracy: 97.93%\n",
      "2\tValidation loss: 0.060122\tBest loss: 0.060122\tAccuracy: 98.36%\n",
      "3\tValidation loss: 0.056448\tBest loss: 0.056448\tAccuracy: 98.40%\n",
      "4\tValidation loss: 0.057042\tBest loss: 0.056448\tAccuracy: 98.20%\n",
      "5\tValidation loss: 0.064108\tBest loss: 0.056448\tAccuracy: 98.36%\n",
      "6\tValidation loss: 0.070088\tBest loss: 0.056448\tAccuracy: 98.16%\n",
      "7\tValidation loss: 0.050303\tBest loss: 0.050303\tAccuracy: 98.55%\n",
      "8\tValidation loss: 0.047453\tBest loss: 0.047453\tAccuracy: 98.79%\n",
      "9\tValidation loss: 0.069805\tBest loss: 0.047453\tAccuracy: 98.55%\n",
      "10\tValidation loss: 0.073442\tBest loss: 0.047453\tAccuracy: 98.40%\n",
      "11\tValidation loss: 0.057435\tBest loss: 0.047453\tAccuracy: 98.71%\n",
      "12\tValidation loss: 0.062045\tBest loss: 0.047453\tAccuracy: 98.71%\n",
      "13\tValidation loss: 0.067335\tBest loss: 0.047453\tAccuracy: 98.28%\n",
      "14\tValidation loss: 0.067299\tBest loss: 0.047453\tAccuracy: 98.67%\n",
      "15\tValidation loss: 0.078219\tBest loss: 0.047453\tAccuracy: 98.59%\n",
      "16\tValidation loss: 0.086242\tBest loss: 0.047453\tAccuracy: 98.63%\n",
      "17\tValidation loss: 0.068894\tBest loss: 0.047453\tAccuracy: 98.71%\n",
      "18\tValidation loss: 0.089507\tBest loss: 0.047453\tAccuracy: 98.40%\n",
      "19\tValidation loss: 0.095097\tBest loss: 0.047453\tAccuracy: 98.16%\n",
      "20\tValidation loss: 0.099970\tBest loss: 0.047453\tAccuracy: 98.67%\n",
      "21\tValidation loss: 0.073843\tBest loss: 0.047453\tAccuracy: 98.67%\n",
      "22\tValidation loss: 0.073286\tBest loss: 0.047453\tAccuracy: 98.67%\n",
      "23\tValidation loss: 0.086557\tBest loss: 0.047453\tAccuracy: 98.59%\n",
      "24\tValidation loss: 0.074675\tBest loss: 0.047453\tAccuracy: 98.71%\n",
      "25\tValidation loss: 0.077939\tBest loss: 0.047453\tAccuracy: 98.91%\n",
      "26\tValidation loss: 0.089621\tBest loss: 0.047453\tAccuracy: 98.94%\n",
      "27\tValidation loss: 0.085165\tBest loss: 0.047453\tAccuracy: 98.98%\n",
      "28\tValidation loss: 0.113488\tBest loss: 0.047453\tAccuracy: 98.87%\n",
      "29\tValidation loss: 0.091195\tBest loss: 0.047453\tAccuracy: 98.94%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.01, n_neurons=120, batch_size=500, total=  16.2s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.01, n_neurons=120, batch_size=500 \n",
      "0\tValidation loss: 0.121900\tBest loss: 0.121900\tAccuracy: 96.05%\n",
      "1\tValidation loss: 0.070490\tBest loss: 0.070490\tAccuracy: 97.97%\n",
      "2\tValidation loss: 0.059113\tBest loss: 0.059113\tAccuracy: 98.12%\n",
      "3\tValidation loss: 0.064580\tBest loss: 0.059113\tAccuracy: 98.51%\n",
      "4\tValidation loss: 0.049428\tBest loss: 0.049428\tAccuracy: 98.55%\n",
      "5\tValidation loss: 0.059353\tBest loss: 0.049428\tAccuracy: 98.12%\n",
      "6\tValidation loss: 0.048794\tBest loss: 0.048794\tAccuracy: 98.51%\n",
      "7\tValidation loss: 0.050422\tBest loss: 0.048794\tAccuracy: 98.51%\n",
      "8\tValidation loss: 0.065305\tBest loss: 0.048794\tAccuracy: 98.40%\n",
      "9\tValidation loss: 0.039833\tBest loss: 0.039833\tAccuracy: 98.87%\n",
      "10\tValidation loss: 0.064861\tBest loss: 0.039833\tAccuracy: 98.67%\n",
      "11\tValidation loss: 0.077416\tBest loss: 0.039833\tAccuracy: 98.44%\n",
      "12\tValidation loss: 0.059295\tBest loss: 0.039833\tAccuracy: 98.67%\n",
      "13\tValidation loss: 0.069597\tBest loss: 0.039833\tAccuracy: 98.55%\n",
      "14\tValidation loss: 0.083519\tBest loss: 0.039833\tAccuracy: 98.48%\n",
      "15\tValidation loss: 0.052377\tBest loss: 0.039833\tAccuracy: 98.83%\n",
      "16\tValidation loss: 0.045651\tBest loss: 0.039833\tAccuracy: 98.94%\n",
      "17\tValidation loss: 0.052411\tBest loss: 0.039833\tAccuracy: 98.87%\n",
      "18\tValidation loss: 0.060380\tBest loss: 0.039833\tAccuracy: 98.71%\n",
      "19\tValidation loss: 0.113459\tBest loss: 0.039833\tAccuracy: 98.51%\n",
      "20\tValidation loss: 0.093996\tBest loss: 0.039833\tAccuracy: 98.48%\n",
      "21\tValidation loss: 0.073720\tBest loss: 0.039833\tAccuracy: 98.63%\n",
      "22\tValidation loss: 0.061989\tBest loss: 0.039833\tAccuracy: 99.10%\n",
      "23\tValidation loss: 0.067228\tBest loss: 0.039833\tAccuracy: 98.71%\n",
      "24\tValidation loss: 0.067302\tBest loss: 0.039833\tAccuracy: 98.28%\n",
      "25\tValidation loss: 0.074416\tBest loss: 0.039833\tAccuracy: 98.83%\n",
      "26\tValidation loss: 0.058904\tBest loss: 0.039833\tAccuracy: 98.75%\n",
      "27\tValidation loss: 0.064219\tBest loss: 0.039833\tAccuracy: 98.36%\n",
      "28\tValidation loss: 0.071315\tBest loss: 0.039833\tAccuracy: 98.59%\n",
      "29\tValidation loss: 0.061257\tBest loss: 0.039833\tAccuracy: 98.67%\n",
      "30\tValidation loss: 0.066078\tBest loss: 0.039833\tAccuracy: 99.02%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.01, n_neurons=120, batch_size=500, total=  16.5s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.01, n_neurons=90, batch_size=500 \n",
      "0\tValidation loss: 0.105184\tBest loss: 0.105184\tAccuracy: 96.68%\n",
      "1\tValidation loss: 0.061807\tBest loss: 0.061807\tAccuracy: 98.20%\n",
      "2\tValidation loss: 0.054473\tBest loss: 0.054473\tAccuracy: 98.40%\n",
      "3\tValidation loss: 0.051237\tBest loss: 0.051237\tAccuracy: 98.28%\n",
      "4\tValidation loss: 0.070407\tBest loss: 0.051237\tAccuracy: 97.93%\n",
      "5\tValidation loss: 0.056388\tBest loss: 0.051237\tAccuracy: 98.71%\n",
      "6\tValidation loss: 0.063715\tBest loss: 0.051237\tAccuracy: 98.48%\n",
      "7\tValidation loss: 0.047094\tBest loss: 0.047094\tAccuracy: 98.75%\n",
      "8\tValidation loss: 0.070861\tBest loss: 0.047094\tAccuracy: 98.24%\n",
      "9\tValidation loss: 0.061751\tBest loss: 0.047094\tAccuracy: 98.44%\n",
      "10\tValidation loss: 0.048057\tBest loss: 0.047094\tAccuracy: 98.83%\n",
      "11\tValidation loss: 0.072979\tBest loss: 0.047094\tAccuracy: 98.36%\n",
      "12\tValidation loss: 0.055018\tBest loss: 0.047094\tAccuracy: 98.91%\n",
      "13\tValidation loss: 0.056399\tBest loss: 0.047094\tAccuracy: 98.67%\n",
      "14\tValidation loss: 0.061171\tBest loss: 0.047094\tAccuracy: 98.71%\n",
      "15\tValidation loss: 0.059716\tBest loss: 0.047094\tAccuracy: 98.32%\n",
      "16\tValidation loss: 0.065243\tBest loss: 0.047094\tAccuracy: 98.63%\n",
      "17\tValidation loss: 0.076371\tBest loss: 0.047094\tAccuracy: 98.83%\n",
      "18\tValidation loss: 0.094339\tBest loss: 0.047094\tAccuracy: 98.40%\n",
      "19\tValidation loss: 0.067080\tBest loss: 0.047094\tAccuracy: 98.63%\n",
      "20\tValidation loss: 0.075241\tBest loss: 0.047094\tAccuracy: 98.75%\n",
      "21\tValidation loss: 0.082193\tBest loss: 0.047094\tAccuracy: 98.51%\n",
      "22\tValidation loss: 0.071474\tBest loss: 0.047094\tAccuracy: 98.79%\n",
      "23\tValidation loss: 0.060503\tBest loss: 0.047094\tAccuracy: 98.75%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\tValidation loss: 0.090367\tBest loss: 0.047094\tAccuracy: 98.79%\n",
      "25\tValidation loss: 0.065086\tBest loss: 0.047094\tAccuracy: 98.75%\n",
      "26\tValidation loss: 0.080702\tBest loss: 0.047094\tAccuracy: 98.83%\n",
      "27\tValidation loss: 0.068190\tBest loss: 0.047094\tAccuracy: 98.91%\n",
      "28\tValidation loss: 0.093204\tBest loss: 0.047094\tAccuracy: 98.75%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.01, n_neurons=90, batch_size=500, total=  12.8s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.01, n_neurons=90, batch_size=500 \n",
      "0\tValidation loss: 0.096790\tBest loss: 0.096790\tAccuracy: 97.19%\n",
      "1\tValidation loss: 0.063598\tBest loss: 0.063598\tAccuracy: 98.20%\n",
      "2\tValidation loss: 0.054796\tBest loss: 0.054796\tAccuracy: 98.08%\n",
      "3\tValidation loss: 0.046267\tBest loss: 0.046267\tAccuracy: 98.36%\n",
      "4\tValidation loss: 0.061056\tBest loss: 0.046267\tAccuracy: 98.32%\n",
      "5\tValidation loss: 0.049244\tBest loss: 0.046267\tAccuracy: 98.55%\n",
      "6\tValidation loss: 0.043670\tBest loss: 0.043670\tAccuracy: 98.79%\n",
      "7\tValidation loss: 0.045760\tBest loss: 0.043670\tAccuracy: 98.71%\n",
      "8\tValidation loss: 0.040150\tBest loss: 0.040150\tAccuracy: 98.98%\n",
      "9\tValidation loss: 0.065572\tBest loss: 0.040150\tAccuracy: 98.71%\n",
      "10\tValidation loss: 0.052363\tBest loss: 0.040150\tAccuracy: 98.79%\n",
      "11\tValidation loss: 0.057260\tBest loss: 0.040150\tAccuracy: 98.79%\n",
      "12\tValidation loss: 0.068283\tBest loss: 0.040150\tAccuracy: 98.48%\n",
      "13\tValidation loss: 0.049587\tBest loss: 0.040150\tAccuracy: 98.71%\n",
      "14\tValidation loss: 0.037502\tBest loss: 0.037502\tAccuracy: 99.18%\n",
      "15\tValidation loss: 0.052206\tBest loss: 0.037502\tAccuracy: 98.87%\n",
      "16\tValidation loss: 0.047586\tBest loss: 0.037502\tAccuracy: 98.94%\n",
      "17\tValidation loss: 0.038252\tBest loss: 0.037502\tAccuracy: 98.83%\n",
      "18\tValidation loss: 0.050415\tBest loss: 0.037502\tAccuracy: 99.14%\n",
      "19\tValidation loss: 0.045626\tBest loss: 0.037502\tAccuracy: 99.22%\n",
      "20\tValidation loss: 0.086421\tBest loss: 0.037502\tAccuracy: 98.67%\n",
      "21\tValidation loss: 0.054106\tBest loss: 0.037502\tAccuracy: 99.14%\n",
      "22\tValidation loss: 0.052226\tBest loss: 0.037502\tAccuracy: 98.98%\n",
      "23\tValidation loss: 0.061322\tBest loss: 0.037502\tAccuracy: 99.02%\n",
      "24\tValidation loss: 0.053045\tBest loss: 0.037502\tAccuracy: 99.06%\n",
      "25\tValidation loss: 0.102305\tBest loss: 0.037502\tAccuracy: 98.71%\n",
      "26\tValidation loss: 0.093162\tBest loss: 0.037502\tAccuracy: 98.51%\n",
      "27\tValidation loss: 0.061474\tBest loss: 0.037502\tAccuracy: 98.87%\n",
      "28\tValidation loss: 0.070722\tBest loss: 0.037502\tAccuracy: 98.98%\n",
      "29\tValidation loss: 0.129934\tBest loss: 0.037502\tAccuracy: 98.24%\n",
      "30\tValidation loss: 0.086056\tBest loss: 0.037502\tAccuracy: 98.24%\n",
      "31\tValidation loss: 0.070668\tBest loss: 0.037502\tAccuracy: 98.91%\n",
      "32\tValidation loss: 0.042944\tBest loss: 0.037502\tAccuracy: 98.91%\n",
      "33\tValidation loss: 0.057584\tBest loss: 0.037502\tAccuracy: 99.14%\n",
      "34\tValidation loss: 0.064911\tBest loss: 0.037502\tAccuracy: 98.59%\n",
      "35\tValidation loss: 0.096891\tBest loss: 0.037502\tAccuracy: 98.98%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.01, n_neurons=90, batch_size=500, total=  15.7s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.01, n_neurons=90, batch_size=500 \n",
      "0\tValidation loss: 0.082425\tBest loss: 0.082425\tAccuracy: 97.50%\n",
      "1\tValidation loss: 0.064244\tBest loss: 0.064244\tAccuracy: 98.24%\n",
      "2\tValidation loss: 0.059379\tBest loss: 0.059379\tAccuracy: 98.24%\n",
      "3\tValidation loss: 0.044422\tBest loss: 0.044422\tAccuracy: 98.63%\n",
      "4\tValidation loss: 0.040435\tBest loss: 0.040435\tAccuracy: 98.79%\n",
      "5\tValidation loss: 0.048679\tBest loss: 0.040435\tAccuracy: 98.79%\n",
      "6\tValidation loss: 0.053436\tBest loss: 0.040435\tAccuracy: 98.83%\n",
      "7\tValidation loss: 0.054028\tBest loss: 0.040435\tAccuracy: 98.63%\n",
      "8\tValidation loss: 0.051359\tBest loss: 0.040435\tAccuracy: 98.48%\n",
      "9\tValidation loss: 0.061104\tBest loss: 0.040435\tAccuracy: 98.79%\n",
      "10\tValidation loss: 0.062384\tBest loss: 0.040435\tAccuracy: 98.51%\n",
      "11\tValidation loss: 0.062546\tBest loss: 0.040435\tAccuracy: 98.63%\n",
      "12\tValidation loss: 0.065473\tBest loss: 0.040435\tAccuracy: 98.40%\n",
      "13\tValidation loss: 0.053102\tBest loss: 0.040435\tAccuracy: 98.91%\n",
      "14\tValidation loss: 0.060751\tBest loss: 0.040435\tAccuracy: 98.91%\n",
      "15\tValidation loss: 0.035675\tBest loss: 0.035675\tAccuracy: 98.98%\n",
      "16\tValidation loss: 0.045283\tBest loss: 0.035675\tAccuracy: 99.02%\n",
      "17\tValidation loss: 0.044211\tBest loss: 0.035675\tAccuracy: 99.06%\n",
      "18\tValidation loss: 0.052031\tBest loss: 0.035675\tAccuracy: 99.14%\n",
      "19\tValidation loss: 0.063395\tBest loss: 0.035675\tAccuracy: 98.79%\n",
      "20\tValidation loss: 0.065780\tBest loss: 0.035675\tAccuracy: 98.83%\n",
      "21\tValidation loss: 0.065048\tBest loss: 0.035675\tAccuracy: 98.91%\n",
      "22\tValidation loss: 0.063028\tBest loss: 0.035675\tAccuracy: 98.87%\n",
      "23\tValidation loss: 0.058373\tBest loss: 0.035675\tAccuracy: 98.67%\n",
      "24\tValidation loss: 0.078830\tBest loss: 0.035675\tAccuracy: 98.83%\n",
      "25\tValidation loss: 0.077306\tBest loss: 0.035675\tAccuracy: 98.75%\n",
      "26\tValidation loss: 0.089936\tBest loss: 0.035675\tAccuracy: 98.63%\n",
      "27\tValidation loss: 0.065968\tBest loss: 0.035675\tAccuracy: 98.79%\n",
      "28\tValidation loss: 0.054141\tBest loss: 0.035675\tAccuracy: 98.94%\n",
      "29\tValidation loss: 0.061560\tBest loss: 0.035675\tAccuracy: 98.75%\n",
      "30\tValidation loss: 0.047242\tBest loss: 0.035675\tAccuracy: 99.10%\n",
      "31\tValidation loss: 0.042253\tBest loss: 0.035675\tAccuracy: 99.26%\n",
      "32\tValidation loss: 0.072907\tBest loss: 0.035675\tAccuracy: 98.98%\n",
      "33\tValidation loss: 0.060336\tBest loss: 0.035675\tAccuracy: 99.14%\n",
      "34\tValidation loss: 0.056209\tBest loss: 0.035675\tAccuracy: 99.14%\n",
      "35\tValidation loss: 0.053985\tBest loss: 0.035675\tAccuracy: 99.06%\n",
      "36\tValidation loss: 0.058075\tBest loss: 0.035675\tAccuracy: 99.34%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.01, n_neurons=90, batch_size=500, total=  15.7s\n",
      "[CV] activation=<function elu at 0x129284e18>, learning_rate=0.01, n_neurons=140, batch_size=500 \n",
      "0\tValidation loss: 0.125781\tBest loss: 0.125781\tAccuracy: 95.90%\n",
      "1\tValidation loss: 0.085375\tBest loss: 0.085375\tAccuracy: 97.38%\n",
      "2\tValidation loss: 0.083951\tBest loss: 0.083951\tAccuracy: 97.46%\n",
      "3\tValidation loss: 0.063494\tBest loss: 0.063494\tAccuracy: 97.97%\n",
      "4\tValidation loss: 0.067941\tBest loss: 0.063494\tAccuracy: 97.85%\n",
      "5\tValidation loss: 0.056756\tBest loss: 0.056756\tAccuracy: 98.63%\n",
      "6\tValidation loss: 0.059947\tBest loss: 0.056756\tAccuracy: 98.16%\n",
      "7\tValidation loss: 0.046515\tBest loss: 0.046515\tAccuracy: 98.59%\n",
      "8\tValidation loss: 0.054861\tBest loss: 0.046515\tAccuracy: 98.28%\n",
      "9\tValidation loss: 0.044641\tBest loss: 0.044641\tAccuracy: 98.87%\n",
      "10\tValidation loss: 0.055060\tBest loss: 0.044641\tAccuracy: 98.63%\n",
      "11\tValidation loss: 0.046966\tBest loss: 0.044641\tAccuracy: 98.67%\n",
      "12\tValidation loss: 0.067704\tBest loss: 0.044641\tAccuracy: 98.40%\n",
      "13\tValidation loss: 0.063182\tBest loss: 0.044641\tAccuracy: 98.44%\n",
      "14\tValidation loss: 0.046961\tBest loss: 0.044641\tAccuracy: 98.32%\n",
      "15\tValidation loss: 0.041618\tBest loss: 0.041618\tAccuracy: 98.98%\n",
      "16\tValidation loss: 0.117282\tBest loss: 0.041618\tAccuracy: 97.62%\n",
      "17\tValidation loss: 0.045226\tBest loss: 0.041618\tAccuracy: 98.63%\n",
      "18\tValidation loss: 0.045561\tBest loss: 0.041618\tAccuracy: 98.91%\n",
      "19\tValidation loss: 0.056779\tBest loss: 0.041618\tAccuracy: 98.91%\n",
      "20\tValidation loss: 0.051047\tBest loss: 0.041618\tAccuracy: 99.02%\n",
      "21\tValidation loss: 0.076791\tBest loss: 0.041618\tAccuracy: 98.79%\n",
      "22\tValidation loss: 0.060512\tBest loss: 0.041618\tAccuracy: 98.44%\n",
      "23\tValidation loss: 0.063563\tBest loss: 0.041618\tAccuracy: 98.67%\n",
      "24\tValidation loss: 0.070973\tBest loss: 0.041618\tAccuracy: 98.71%\n",
      "25\tValidation loss: 0.060670\tBest loss: 0.041618\tAccuracy: 98.83%\n",
      "26\tValidation loss: 0.054350\tBest loss: 0.041618\tAccuracy: 98.91%\n",
      "27\tValidation loss: 0.089539\tBest loss: 0.041618\tAccuracy: 98.40%\n",
      "28\tValidation loss: 0.057984\tBest loss: 0.041618\tAccuracy: 98.63%\n",
      "29\tValidation loss: 0.055766\tBest loss: 0.041618\tAccuracy: 98.71%\n",
      "30\tValidation loss: 0.062304\tBest loss: 0.041618\tAccuracy: 98.94%\n",
      "31\tValidation loss: 0.065844\tBest loss: 0.041618\tAccuracy: 98.91%\n",
      "32\tValidation loss: 0.082125\tBest loss: 0.041618\tAccuracy: 98.59%\n",
      "33\tValidation loss: 0.066515\tBest loss: 0.041618\tAccuracy: 98.83%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\tValidation loss: 0.064701\tBest loss: 0.041618\tAccuracy: 99.02%\n",
      "35\tValidation loss: 0.079409\tBest loss: 0.041618\tAccuracy: 98.75%\n",
      "36\tValidation loss: 0.165110\tBest loss: 0.041618\tAccuracy: 96.87%\n",
      "Early stopping!\n",
      "[CV]  activation=<function elu at 0x129284e18>, learning_rate=0.01, n_neurons=140, batch_size=500, total=  22.6s\n",
      "[CV] activation=<function elu at 0x129284e18>, learning_rate=0.01, n_neurons=140, batch_size=500 \n",
      "0\tValidation loss: 0.133312\tBest loss: 0.133312\tAccuracy: 95.70%\n",
      "1\tValidation loss: 0.081172\tBest loss: 0.081172\tAccuracy: 97.46%\n",
      "2\tValidation loss: 0.067674\tBest loss: 0.067674\tAccuracy: 97.89%\n",
      "3\tValidation loss: 0.058299\tBest loss: 0.058299\tAccuracy: 98.28%\n",
      "4\tValidation loss: 0.065549\tBest loss: 0.058299\tAccuracy: 98.08%\n",
      "5\tValidation loss: 0.056146\tBest loss: 0.056146\tAccuracy: 98.51%\n",
      "6\tValidation loss: 0.057568\tBest loss: 0.056146\tAccuracy: 98.48%\n",
      "7\tValidation loss: 0.051621\tBest loss: 0.051621\tAccuracy: 98.71%\n",
      "8\tValidation loss: 0.043155\tBest loss: 0.043155\tAccuracy: 98.67%\n",
      "9\tValidation loss: 0.062305\tBest loss: 0.043155\tAccuracy: 98.48%\n",
      "10\tValidation loss: 0.051858\tBest loss: 0.043155\tAccuracy: 98.71%\n",
      "11\tValidation loss: 0.060294\tBest loss: 0.043155\tAccuracy: 98.59%\n",
      "12\tValidation loss: 0.044002\tBest loss: 0.043155\tAccuracy: 98.87%\n",
      "13\tValidation loss: 0.055607\tBest loss: 0.043155\tAccuracy: 98.79%\n",
      "14\tValidation loss: 0.072784\tBest loss: 0.043155\tAccuracy: 98.59%\n",
      "15\tValidation loss: 0.056146\tBest loss: 0.043155\tAccuracy: 98.67%\n",
      "16\tValidation loss: 0.054196\tBest loss: 0.043155\tAccuracy: 98.71%\n",
      "17\tValidation loss: 0.053629\tBest loss: 0.043155\tAccuracy: 98.87%\n",
      "18\tValidation loss: 0.101757\tBest loss: 0.043155\tAccuracy: 98.16%\n",
      "19\tValidation loss: 0.068454\tBest loss: 0.043155\tAccuracy: 98.79%\n",
      "20\tValidation loss: 0.056099\tBest loss: 0.043155\tAccuracy: 98.87%\n",
      "21\tValidation loss: 0.061212\tBest loss: 0.043155\tAccuracy: 98.48%\n",
      "22\tValidation loss: 0.057540\tBest loss: 0.043155\tAccuracy: 98.67%\n",
      "23\tValidation loss: 0.085453\tBest loss: 0.043155\tAccuracy: 98.87%\n",
      "24\tValidation loss: 0.061336\tBest loss: 0.043155\tAccuracy: 98.83%\n",
      "25\tValidation loss: 0.059987\tBest loss: 0.043155\tAccuracy: 98.98%\n",
      "26\tValidation loss: 0.059171\tBest loss: 0.043155\tAccuracy: 99.02%\n",
      "27\tValidation loss: 0.069646\tBest loss: 0.043155\tAccuracy: 98.83%\n",
      "28\tValidation loss: 0.060068\tBest loss: 0.043155\tAccuracy: 98.94%\n",
      "29\tValidation loss: 0.062383\tBest loss: 0.043155\tAccuracy: 98.63%\n",
      "Early stopping!\n",
      "[CV]  activation=<function elu at 0x129284e18>, learning_rate=0.01, n_neurons=140, batch_size=500, total=  17.7s\n",
      "[CV] activation=<function elu at 0x129284e18>, learning_rate=0.01, n_neurons=140, batch_size=500 \n",
      "0\tValidation loss: 0.127920\tBest loss: 0.127920\tAccuracy: 96.33%\n",
      "1\tValidation loss: 0.078285\tBest loss: 0.078285\tAccuracy: 97.69%\n",
      "2\tValidation loss: 0.063931\tBest loss: 0.063931\tAccuracy: 98.08%\n",
      "3\tValidation loss: 0.061658\tBest loss: 0.061658\tAccuracy: 98.01%\n",
      "4\tValidation loss: 0.049285\tBest loss: 0.049285\tAccuracy: 98.44%\n",
      "5\tValidation loss: 0.047061\tBest loss: 0.047061\tAccuracy: 98.59%\n",
      "6\tValidation loss: 0.059377\tBest loss: 0.047061\tAccuracy: 98.40%\n",
      "7\tValidation loss: 0.045424\tBest loss: 0.045424\tAccuracy: 98.48%\n",
      "8\tValidation loss: 0.046367\tBest loss: 0.045424\tAccuracy: 98.40%\n",
      "9\tValidation loss: 0.050366\tBest loss: 0.045424\tAccuracy: 98.59%\n",
      "10\tValidation loss: 0.043057\tBest loss: 0.043057\tAccuracy: 98.79%\n",
      "11\tValidation loss: 0.049350\tBest loss: 0.043057\tAccuracy: 98.87%\n",
      "12\tValidation loss: 0.054511\tBest loss: 0.043057\tAccuracy: 98.48%\n",
      "13\tValidation loss: 0.048814\tBest loss: 0.043057\tAccuracy: 98.87%\n",
      "14\tValidation loss: 0.054113\tBest loss: 0.043057\tAccuracy: 98.71%\n",
      "15\tValidation loss: 0.052198\tBest loss: 0.043057\tAccuracy: 98.83%\n",
      "16\tValidation loss: 0.069093\tBest loss: 0.043057\tAccuracy: 98.36%\n",
      "17\tValidation loss: 0.064760\tBest loss: 0.043057\tAccuracy: 98.71%\n",
      "18\tValidation loss: 0.065226\tBest loss: 0.043057\tAccuracy: 98.75%\n",
      "19\tValidation loss: 0.047936\tBest loss: 0.043057\tAccuracy: 98.79%\n",
      "20\tValidation loss: 0.059210\tBest loss: 0.043057\tAccuracy: 98.83%\n",
      "21\tValidation loss: 0.062265\tBest loss: 0.043057\tAccuracy: 98.55%\n",
      "22\tValidation loss: 0.049235\tBest loss: 0.043057\tAccuracy: 98.94%\n",
      "23\tValidation loss: 0.066963\tBest loss: 0.043057\tAccuracy: 98.87%\n",
      "24\tValidation loss: 0.069564\tBest loss: 0.043057\tAccuracy: 98.94%\n",
      "25\tValidation loss: 0.064174\tBest loss: 0.043057\tAccuracy: 98.94%\n",
      "26\tValidation loss: 0.071398\tBest loss: 0.043057\tAccuracy: 98.83%\n",
      "27\tValidation loss: 0.070311\tBest loss: 0.043057\tAccuracy: 98.83%\n",
      "28\tValidation loss: 0.072504\tBest loss: 0.043057\tAccuracy: 98.79%\n",
      "29\tValidation loss: 0.069921\tBest loss: 0.043057\tAccuracy: 98.83%\n",
      "30\tValidation loss: 0.057379\tBest loss: 0.043057\tAccuracy: 98.87%\n",
      "31\tValidation loss: 0.058410\tBest loss: 0.043057\tAccuracy: 98.87%\n",
      "Early stopping!\n",
      "[CV]  activation=<function elu at 0x129284e18>, learning_rate=0.01, n_neurons=140, batch_size=500, total=  18.5s\n",
      "[CV] activation=<function relu at 0x12920c1e0>, learning_rate=0.1, n_neurons=50, batch_size=10 \n",
      "0\tValidation loss: 1.637288\tBest loss: 1.637288\tAccuracy: 19.31%\n",
      "1\tValidation loss: 1.621568\tBest loss: 1.621568\tAccuracy: 19.31%\n",
      "2\tValidation loss: 1.619519\tBest loss: 1.619519\tAccuracy: 19.31%\n",
      "3\tValidation loss: 1.616241\tBest loss: 1.616241\tAccuracy: 19.31%\n",
      "4\tValidation loss: 1.654658\tBest loss: 1.616241\tAccuracy: 22.05%\n",
      "5\tValidation loss: 1.621189\tBest loss: 1.616241\tAccuracy: 22.05%\n",
      "6\tValidation loss: 1.632401\tBest loss: 1.616241\tAccuracy: 19.31%\n",
      "7\tValidation loss: 1.614565\tBest loss: 1.614565\tAccuracy: 19.31%\n",
      "8\tValidation loss: 1.642283\tBest loss: 1.614565\tAccuracy: 19.31%\n",
      "9\tValidation loss: 1.639493\tBest loss: 1.614565\tAccuracy: 19.31%\n",
      "10\tValidation loss: 1.615069\tBest loss: 1.614565\tAccuracy: 22.05%\n",
      "11\tValidation loss: 1.630250\tBest loss: 1.614565\tAccuracy: 22.05%\n",
      "12\tValidation loss: 1.636419\tBest loss: 1.614565\tAccuracy: 19.12%\n",
      "13\tValidation loss: 1.612812\tBest loss: 1.612812\tAccuracy: 19.31%\n",
      "14\tValidation loss: 1.628912\tBest loss: 1.612812\tAccuracy: 19.12%\n",
      "15\tValidation loss: 1.616823\tBest loss: 1.612812\tAccuracy: 22.05%\n",
      "16\tValidation loss: 1.626637\tBest loss: 1.612812\tAccuracy: 18.73%\n",
      "17\tValidation loss: 1.608286\tBest loss: 1.608286\tAccuracy: 22.05%\n",
      "18\tValidation loss: 1.624446\tBest loss: 1.608286\tAccuracy: 19.31%\n",
      "19\tValidation loss: 1.623746\tBest loss: 1.608286\tAccuracy: 20.95%\n",
      "20\tValidation loss: 1.615125\tBest loss: 1.608286\tAccuracy: 19.31%\n",
      "21\tValidation loss: 1.629332\tBest loss: 1.608286\tAccuracy: 19.31%\n",
      "22\tValidation loss: 1.618768\tBest loss: 1.608286\tAccuracy: 19.31%\n",
      "23\tValidation loss: 1.620750\tBest loss: 1.608286\tAccuracy: 22.05%\n",
      "24\tValidation loss: 1.633706\tBest loss: 1.608286\tAccuracy: 22.05%\n",
      "25\tValidation loss: 1.646806\tBest loss: 1.608286\tAccuracy: 22.01%\n",
      "26\tValidation loss: 1.613813\tBest loss: 1.608286\tAccuracy: 19.12%\n",
      "27\tValidation loss: 1.638241\tBest loss: 1.608286\tAccuracy: 19.31%\n",
      "28\tValidation loss: 1.655873\tBest loss: 1.608286\tAccuracy: 22.05%\n",
      "29\tValidation loss: 1.617473\tBest loss: 1.608286\tAccuracy: 18.73%\n",
      "30\tValidation loss: 1.641590\tBest loss: 1.608286\tAccuracy: 19.31%\n",
      "31\tValidation loss: 1.631204\tBest loss: 1.608286\tAccuracy: 18.73%\n",
      "32\tValidation loss: 1.640918\tBest loss: 1.608286\tAccuracy: 18.73%\n",
      "33\tValidation loss: 1.647130\tBest loss: 1.608286\tAccuracy: 19.12%\n",
      "34\tValidation loss: 1.609976\tBest loss: 1.608286\tAccuracy: 22.05%\n",
      "35\tValidation loss: 1.624349\tBest loss: 1.608286\tAccuracy: 19.12%\n",
      "36\tValidation loss: 1.630581\tBest loss: 1.608286\tAccuracy: 18.73%\n",
      "37\tValidation loss: 1.632025\tBest loss: 1.608286\tAccuracy: 18.73%\n",
      "38\tValidation loss: 1.630729\tBest loss: 1.608286\tAccuracy: 22.05%\n",
      "Early stopping!\n",
      "[CV]  activation=<function relu at 0x12920c1e0>, learning_rate=0.1, n_neurons=50, batch_size=10, total= 1.4min\n",
      "[CV] activation=<function relu at 0x12920c1e0>, learning_rate=0.1, n_neurons=50, batch_size=10 \n",
      "0\tValidation loss: 1.631880\tBest loss: 1.631880\tAccuracy: 22.01%\n",
      "1\tValidation loss: 1.644810\tBest loss: 1.631880\tAccuracy: 19.08%\n",
      "2\tValidation loss: 1.611657\tBest loss: 1.611657\tAccuracy: 22.01%\n",
      "3\tValidation loss: 1.614184\tBest loss: 1.611657\tAccuracy: 22.01%\n",
      "4\tValidation loss: 1.617815\tBest loss: 1.611657\tAccuracy: 22.01%\n",
      "5\tValidation loss: 1.624095\tBest loss: 1.611657\tAccuracy: 22.01%\n",
      "6\tValidation loss: 1.635101\tBest loss: 1.611657\tAccuracy: 19.27%\n",
      "7\tValidation loss: 1.630914\tBest loss: 1.611657\tAccuracy: 18.73%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\tValidation loss: 1.639863\tBest loss: 1.611657\tAccuracy: 22.01%\n",
      "9\tValidation loss: 1.611894\tBest loss: 1.611657\tAccuracy: 20.91%\n",
      "10\tValidation loss: 1.613292\tBest loss: 1.611657\tAccuracy: 22.01%\n",
      "11\tValidation loss: 1.614465\tBest loss: 1.611657\tAccuracy: 22.01%\n",
      "12\tValidation loss: 1.615519\tBest loss: 1.611657\tAccuracy: 20.91%\n",
      "13\tValidation loss: 1.609593\tBest loss: 1.609593\tAccuracy: 22.01%\n",
      "14\tValidation loss: 1.628134\tBest loss: 1.609593\tAccuracy: 22.01%\n",
      "15\tValidation loss: 1.613723\tBest loss: 1.609593\tAccuracy: 19.08%\n",
      "16\tValidation loss: 1.621646\tBest loss: 1.609593\tAccuracy: 22.01%\n",
      "17\tValidation loss: 1.612428\tBest loss: 1.609593\tAccuracy: 19.08%\n",
      "18\tValidation loss: 1.622170\tBest loss: 1.609593\tAccuracy: 22.01%\n",
      "19\tValidation loss: 1.611825\tBest loss: 1.609593\tAccuracy: 22.01%\n",
      "20\tValidation loss: 1.612158\tBest loss: 1.609593\tAccuracy: 20.91%\n",
      "21\tValidation loss: 1.657981\tBest loss: 1.609593\tAccuracy: 22.01%\n",
      "22\tValidation loss: 1.643383\tBest loss: 1.609593\tAccuracy: 19.27%\n",
      "23\tValidation loss: 1.647554\tBest loss: 1.609593\tAccuracy: 22.01%\n",
      "24\tValidation loss: 1.652203\tBest loss: 1.609593\tAccuracy: 18.73%\n",
      "25\tValidation loss: 1.610976\tBest loss: 1.609593\tAccuracy: 20.91%\n",
      "26\tValidation loss: 1.614098\tBest loss: 1.609593\tAccuracy: 20.91%\n",
      "27\tValidation loss: 1.614371\tBest loss: 1.609593\tAccuracy: 20.91%\n",
      "28\tValidation loss: 1.642984\tBest loss: 1.609593\tAccuracy: 19.08%\n",
      "29\tValidation loss: 1.614041\tBest loss: 1.609593\tAccuracy: 19.08%\n",
      "30\tValidation loss: 1.609285\tBest loss: 1.609285\tAccuracy: 22.01%\n",
      "31\tValidation loss: 1.634894\tBest loss: 1.609285\tAccuracy: 18.73%\n",
      "32\tValidation loss: 1.614006\tBest loss: 1.609285\tAccuracy: 19.08%\n",
      "33\tValidation loss: 1.619314\tBest loss: 1.609285\tAccuracy: 19.08%\n",
      "34\tValidation loss: 1.611677\tBest loss: 1.609285\tAccuracy: 22.01%\n",
      "35\tValidation loss: 1.638075\tBest loss: 1.609285\tAccuracy: 20.91%\n",
      "36\tValidation loss: 1.621402\tBest loss: 1.609285\tAccuracy: 22.01%\n",
      "37\tValidation loss: 1.617570\tBest loss: 1.609285\tAccuracy: 22.01%\n",
      "38\tValidation loss: 1.620770\tBest loss: 1.609285\tAccuracy: 20.91%\n",
      "39\tValidation loss: 1.627340\tBest loss: 1.609285\tAccuracy: 19.27%\n",
      "40\tValidation loss: 1.607887\tBest loss: 1.607887\tAccuracy: 22.01%\n",
      "41\tValidation loss: 1.634036\tBest loss: 1.607887\tAccuracy: 22.01%\n",
      "42\tValidation loss: 1.618335\tBest loss: 1.607887\tAccuracy: 20.91%\n",
      "43\tValidation loss: 1.618744\tBest loss: 1.607887\tAccuracy: 22.01%\n",
      "44\tValidation loss: 1.621666\tBest loss: 1.607887\tAccuracy: 19.08%\n",
      "45\tValidation loss: 1.629387\tBest loss: 1.607887\tAccuracy: 22.01%\n",
      "46\tValidation loss: 1.637660\tBest loss: 1.607887\tAccuracy: 18.73%\n",
      "47\tValidation loss: 1.609609\tBest loss: 1.607887\tAccuracy: 22.01%\n",
      "48\tValidation loss: 1.612161\tBest loss: 1.607887\tAccuracy: 22.01%\n",
      "49\tValidation loss: 1.643130\tBest loss: 1.607887\tAccuracy: 19.27%\n",
      "50\tValidation loss: 1.611989\tBest loss: 1.607887\tAccuracy: 22.01%\n",
      "51\tValidation loss: 1.619973\tBest loss: 1.607887\tAccuracy: 19.08%\n",
      "52\tValidation loss: 1.627019\tBest loss: 1.607887\tAccuracy: 19.08%\n",
      "53\tValidation loss: 1.618522\tBest loss: 1.607887\tAccuracy: 22.01%\n",
      "54\tValidation loss: 1.654810\tBest loss: 1.607887\tAccuracy: 18.73%\n",
      "55\tValidation loss: 1.645115\tBest loss: 1.607887\tAccuracy: 18.73%\n",
      "56\tValidation loss: 1.669473\tBest loss: 1.607887\tAccuracy: 19.08%\n",
      "57\tValidation loss: 1.619665\tBest loss: 1.607887\tAccuracy: 18.73%\n",
      "58\tValidation loss: 1.612547\tBest loss: 1.607887\tAccuracy: 22.01%\n",
      "59\tValidation loss: 1.619512\tBest loss: 1.607887\tAccuracy: 22.01%\n",
      "60\tValidation loss: 1.635963\tBest loss: 1.607887\tAccuracy: 18.73%\n",
      "61\tValidation loss: 1.627398\tBest loss: 1.607887\tAccuracy: 22.01%\n",
      "Early stopping!\n",
      "[CV]  activation=<function relu at 0x12920c1e0>, learning_rate=0.1, n_neurons=50, batch_size=10, total= 2.3min\n",
      "[CV] activation=<function relu at 0x12920c1e0>, learning_rate=0.1, n_neurons=50, batch_size=10 \n",
      "0\tValidation loss: 1.644697\tBest loss: 1.644697\tAccuracy: 19.27%\n",
      "1\tValidation loss: 1.624601\tBest loss: 1.624601\tAccuracy: 19.27%\n",
      "2\tValidation loss: 1.614165\tBest loss: 1.614165\tAccuracy: 19.27%\n",
      "3\tValidation loss: 1.616999\tBest loss: 1.614165\tAccuracy: 19.27%\n",
      "4\tValidation loss: 1.614503\tBest loss: 1.614165\tAccuracy: 22.01%\n",
      "5\tValidation loss: 1.627699\tBest loss: 1.614165\tAccuracy: 22.01%\n",
      "6\tValidation loss: 1.648675\tBest loss: 1.614165\tAccuracy: 19.27%\n",
      "7\tValidation loss: 1.627600\tBest loss: 1.614165\tAccuracy: 18.73%\n",
      "8\tValidation loss: 1.625684\tBest loss: 1.614165\tAccuracy: 22.01%\n",
      "9\tValidation loss: 1.615554\tBest loss: 1.614165\tAccuracy: 20.91%\n",
      "10\tValidation loss: 1.619121\tBest loss: 1.614165\tAccuracy: 22.01%\n",
      "11\tValidation loss: 1.639597\tBest loss: 1.614165\tAccuracy: 22.01%\n",
      "12\tValidation loss: 1.627611\tBest loss: 1.614165\tAccuracy: 19.08%\n",
      "13\tValidation loss: 1.615012\tBest loss: 1.614165\tAccuracy: 19.08%\n",
      "14\tValidation loss: 1.624332\tBest loss: 1.614165\tAccuracy: 19.08%\n",
      "15\tValidation loss: 1.612443\tBest loss: 1.612443\tAccuracy: 22.01%\n",
      "16\tValidation loss: 1.627838\tBest loss: 1.612443\tAccuracy: 22.01%\n",
      "17\tValidation loss: 1.613231\tBest loss: 1.612443\tAccuracy: 18.73%\n",
      "18\tValidation loss: 1.615750\tBest loss: 1.612443\tAccuracy: 22.01%\n",
      "19\tValidation loss: 1.619094\tBest loss: 1.612443\tAccuracy: 22.01%\n",
      "20\tValidation loss: 1.609112\tBest loss: 1.609112\tAccuracy: 22.01%\n",
      "21\tValidation loss: 1.689015\tBest loss: 1.609112\tAccuracy: 22.01%\n",
      "22\tValidation loss: 1.618887\tBest loss: 1.609112\tAccuracy: 18.73%\n",
      "23\tValidation loss: 1.638923\tBest loss: 1.609112\tAccuracy: 22.01%\n",
      "24\tValidation loss: 1.660611\tBest loss: 1.609112\tAccuracy: 20.91%\n",
      "25\tValidation loss: 1.619327\tBest loss: 1.609112\tAccuracy: 19.27%\n",
      "26\tValidation loss: 1.611328\tBest loss: 1.609112\tAccuracy: 22.01%\n",
      "27\tValidation loss: 1.614297\tBest loss: 1.609112\tAccuracy: 20.91%\n",
      "28\tValidation loss: 1.628115\tBest loss: 1.609112\tAccuracy: 19.08%\n",
      "29\tValidation loss: 1.610351\tBest loss: 1.609112\tAccuracy: 20.91%\n",
      "30\tValidation loss: 1.626940\tBest loss: 1.609112\tAccuracy: 20.91%\n",
      "31\tValidation loss: 1.633271\tBest loss: 1.609112\tAccuracy: 18.73%\n",
      "32\tValidation loss: 1.609059\tBest loss: 1.609059\tAccuracy: 22.01%\n",
      "33\tValidation loss: 1.610649\tBest loss: 1.609059\tAccuracy: 19.08%\n",
      "34\tValidation loss: 1.616985\tBest loss: 1.609059\tAccuracy: 22.01%\n",
      "35\tValidation loss: 1.627558\tBest loss: 1.609059\tAccuracy: 19.27%\n",
      "36\tValidation loss: 1.609164\tBest loss: 1.609059\tAccuracy: 20.91%\n",
      "37\tValidation loss: 1.626446\tBest loss: 1.609059\tAccuracy: 19.27%\n",
      "38\tValidation loss: 1.614503\tBest loss: 1.609059\tAccuracy: 22.01%\n",
      "39\tValidation loss: 1.617748\tBest loss: 1.609059\tAccuracy: 19.27%\n",
      "40\tValidation loss: 1.623124\tBest loss: 1.609059\tAccuracy: 22.01%\n",
      "41\tValidation loss: 1.615785\tBest loss: 1.609059\tAccuracy: 19.27%\n",
      "42\tValidation loss: 1.617481\tBest loss: 1.609059\tAccuracy: 19.27%\n",
      "43\tValidation loss: 1.617776\tBest loss: 1.609059\tAccuracy: 19.08%\n",
      "44\tValidation loss: 1.626857\tBest loss: 1.609059\tAccuracy: 22.01%\n",
      "45\tValidation loss: 1.613781\tBest loss: 1.609059\tAccuracy: 22.01%\n",
      "46\tValidation loss: 1.618688\tBest loss: 1.609059\tAccuracy: 22.01%\n",
      "47\tValidation loss: 1.610628\tBest loss: 1.609059\tAccuracy: 22.01%\n",
      "48\tValidation loss: 1.614394\tBest loss: 1.609059\tAccuracy: 18.73%\n",
      "49\tValidation loss: 1.612177\tBest loss: 1.609059\tAccuracy: 19.27%\n",
      "50\tValidation loss: 1.623884\tBest loss: 1.609059\tAccuracy: 19.27%\n",
      "51\tValidation loss: 1.616774\tBest loss: 1.609059\tAccuracy: 22.01%\n",
      "52\tValidation loss: 1.620593\tBest loss: 1.609059\tAccuracy: 18.73%\n",
      "53\tValidation loss: 1.617683\tBest loss: 1.609059\tAccuracy: 22.01%\n",
      "Early stopping!\n",
      "[CV]  activation=<function relu at 0x12920c1e0>, learning_rate=0.1, n_neurons=50, batch_size=10, total= 2.1min\n",
      "[CV] activation=<function relu at 0x12920c1e0>, learning_rate=0.02, n_neurons=30, batch_size=100 \n",
      "0\tValidation loss: 0.118921\tBest loss: 0.118921\tAccuracy: 96.99%\n",
      "1\tValidation loss: 0.106589\tBest loss: 0.106589\tAccuracy: 96.79%\n",
      "2\tValidation loss: 0.099934\tBest loss: 0.099934\tAccuracy: 97.26%\n",
      "3\tValidation loss: 0.074603\tBest loss: 0.074603\tAccuracy: 97.65%\n",
      "4\tValidation loss: 0.081001\tBest loss: 0.074603\tAccuracy: 97.85%\n",
      "5\tValidation loss: 0.077539\tBest loss: 0.074603\tAccuracy: 97.89%\n",
      "6\tValidation loss: 0.095467\tBest loss: 0.074603\tAccuracy: 97.54%\n",
      "7\tValidation loss: 0.095072\tBest loss: 0.074603\tAccuracy: 96.95%\n",
      "8\tValidation loss: 0.091399\tBest loss: 0.074603\tAccuracy: 97.42%\n",
      "9\tValidation loss: 0.079363\tBest loss: 0.074603\tAccuracy: 98.01%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\tValidation loss: 0.093491\tBest loss: 0.074603\tAccuracy: 97.58%\n",
      "11\tValidation loss: 0.127638\tBest loss: 0.074603\tAccuracy: 97.65%\n",
      "12\tValidation loss: 0.251176\tBest loss: 0.074603\tAccuracy: 95.74%\n",
      "13\tValidation loss: 0.099040\tBest loss: 0.074603\tAccuracy: 96.99%\n",
      "14\tValidation loss: 0.098636\tBest loss: 0.074603\tAccuracy: 97.89%\n",
      "15\tValidation loss: 0.093644\tBest loss: 0.074603\tAccuracy: 97.81%\n",
      "16\tValidation loss: 0.071533\tBest loss: 0.071533\tAccuracy: 98.12%\n",
      "17\tValidation loss: 0.064240\tBest loss: 0.064240\tAccuracy: 98.24%\n",
      "18\tValidation loss: 0.085714\tBest loss: 0.064240\tAccuracy: 98.48%\n",
      "19\tValidation loss: 0.087115\tBest loss: 0.064240\tAccuracy: 98.01%\n",
      "20\tValidation loss: 0.072025\tBest loss: 0.064240\tAccuracy: 98.12%\n",
      "21\tValidation loss: 0.092169\tBest loss: 0.064240\tAccuracy: 98.16%\n",
      "22\tValidation loss: 0.108946\tBest loss: 0.064240\tAccuracy: 97.58%\n",
      "23\tValidation loss: 0.079442\tBest loss: 0.064240\tAccuracy: 97.93%\n",
      "24\tValidation loss: 0.066063\tBest loss: 0.064240\tAccuracy: 98.32%\n",
      "25\tValidation loss: 0.092293\tBest loss: 0.064240\tAccuracy: 98.12%\n",
      "26\tValidation loss: 0.072347\tBest loss: 0.064240\tAccuracy: 98.28%\n",
      "27\tValidation loss: 0.086003\tBest loss: 0.064240\tAccuracy: 98.32%\n",
      "28\tValidation loss: 0.093282\tBest loss: 0.064240\tAccuracy: 98.32%\n",
      "29\tValidation loss: 0.074296\tBest loss: 0.064240\tAccuracy: 97.85%\n",
      "30\tValidation loss: 0.075789\tBest loss: 0.064240\tAccuracy: 98.36%\n",
      "31\tValidation loss: 0.081741\tBest loss: 0.064240\tAccuracy: 98.24%\n",
      "32\tValidation loss: 0.107874\tBest loss: 0.064240\tAccuracy: 97.85%\n",
      "33\tValidation loss: 0.137245\tBest loss: 0.064240\tAccuracy: 97.26%\n",
      "34\tValidation loss: 0.157246\tBest loss: 0.064240\tAccuracy: 96.44%\n",
      "35\tValidation loss: 0.103930\tBest loss: 0.064240\tAccuracy: 98.28%\n",
      "36\tValidation loss: 0.091799\tBest loss: 0.064240\tAccuracy: 97.81%\n",
      "37\tValidation loss: 0.086077\tBest loss: 0.064240\tAccuracy: 97.65%\n",
      "38\tValidation loss: 0.091710\tBest loss: 0.064240\tAccuracy: 98.28%\n",
      "Early stopping!\n",
      "[CV]  activation=<function relu at 0x12920c1e0>, learning_rate=0.02, n_neurons=30, batch_size=100, total=  15.1s\n",
      "[CV] activation=<function relu at 0x12920c1e0>, learning_rate=0.02, n_neurons=30, batch_size=100 \n",
      "0\tValidation loss: 0.133327\tBest loss: 0.133327\tAccuracy: 96.52%\n",
      "1\tValidation loss: 0.081075\tBest loss: 0.081075\tAccuracy: 97.73%\n",
      "2\tValidation loss: 0.107819\tBest loss: 0.081075\tAccuracy: 97.42%\n",
      "3\tValidation loss: 0.083619\tBest loss: 0.081075\tAccuracy: 97.89%\n",
      "4\tValidation loss: 0.092492\tBest loss: 0.081075\tAccuracy: 97.93%\n",
      "5\tValidation loss: 0.090558\tBest loss: 0.081075\tAccuracy: 97.69%\n",
      "6\tValidation loss: 0.084504\tBest loss: 0.081075\tAccuracy: 98.20%\n",
      "7\tValidation loss: 0.176296\tBest loss: 0.081075\tAccuracy: 97.65%\n",
      "8\tValidation loss: 0.072894\tBest loss: 0.072894\tAccuracy: 98.16%\n",
      "9\tValidation loss: 0.066363\tBest loss: 0.066363\tAccuracy: 98.20%\n",
      "10\tValidation loss: 0.128927\tBest loss: 0.066363\tAccuracy: 97.22%\n",
      "11\tValidation loss: 0.101561\tBest loss: 0.066363\tAccuracy: 97.89%\n",
      "12\tValidation loss: 0.073856\tBest loss: 0.066363\tAccuracy: 98.01%\n",
      "13\tValidation loss: 0.101441\tBest loss: 0.066363\tAccuracy: 97.97%\n",
      "14\tValidation loss: 0.084836\tBest loss: 0.066363\tAccuracy: 98.28%\n",
      "15\tValidation loss: 0.093611\tBest loss: 0.066363\tAccuracy: 97.81%\n",
      "16\tValidation loss: 0.158119\tBest loss: 0.066363\tAccuracy: 96.64%\n",
      "17\tValidation loss: 0.150022\tBest loss: 0.066363\tAccuracy: 95.35%\n",
      "18\tValidation loss: 0.247747\tBest loss: 0.066363\tAccuracy: 93.86%\n",
      "19\tValidation loss: 0.164140\tBest loss: 0.066363\tAccuracy: 95.97%\n",
      "20\tValidation loss: 0.146967\tBest loss: 0.066363\tAccuracy: 96.76%\n",
      "21\tValidation loss: 0.176260\tBest loss: 0.066363\tAccuracy: 96.68%\n",
      "22\tValidation loss: 0.120816\tBest loss: 0.066363\tAccuracy: 97.30%\n",
      "23\tValidation loss: 0.118285\tBest loss: 0.066363\tAccuracy: 97.19%\n",
      "24\tValidation loss: 0.130137\tBest loss: 0.066363\tAccuracy: 97.38%\n",
      "25\tValidation loss: 0.103822\tBest loss: 0.066363\tAccuracy: 97.46%\n",
      "26\tValidation loss: 0.102675\tBest loss: 0.066363\tAccuracy: 97.85%\n",
      "27\tValidation loss: 0.098549\tBest loss: 0.066363\tAccuracy: 97.85%\n",
      "28\tValidation loss: 0.094263\tBest loss: 0.066363\tAccuracy: 97.69%\n",
      "29\tValidation loss: 0.082549\tBest loss: 0.066363\tAccuracy: 97.65%\n",
      "30\tValidation loss: 0.094590\tBest loss: 0.066363\tAccuracy: 97.77%\n",
      "Early stopping!\n",
      "[CV]  activation=<function relu at 0x12920c1e0>, learning_rate=0.02, n_neurons=30, batch_size=100, total=  11.7s\n",
      "[CV] activation=<function relu at 0x12920c1e0>, learning_rate=0.02, n_neurons=30, batch_size=100 \n",
      "0\tValidation loss: 0.098785\tBest loss: 0.098785\tAccuracy: 97.22%\n",
      "1\tValidation loss: 0.087356\tBest loss: 0.087356\tAccuracy: 97.50%\n",
      "2\tValidation loss: 0.076462\tBest loss: 0.076462\tAccuracy: 97.85%\n",
      "3\tValidation loss: 0.072690\tBest loss: 0.072690\tAccuracy: 98.20%\n",
      "4\tValidation loss: 0.099704\tBest loss: 0.072690\tAccuracy: 98.16%\n",
      "5\tValidation loss: 0.097050\tBest loss: 0.072690\tAccuracy: 97.50%\n",
      "6\tValidation loss: 0.103612\tBest loss: 0.072690\tAccuracy: 98.20%\n",
      "7\tValidation loss: 0.113036\tBest loss: 0.072690\tAccuracy: 97.07%\n",
      "8\tValidation loss: 0.068501\tBest loss: 0.068501\tAccuracy: 98.48%\n",
      "9\tValidation loss: 0.057257\tBest loss: 0.057257\tAccuracy: 98.44%\n",
      "10\tValidation loss: 0.066725\tBest loss: 0.057257\tAccuracy: 98.16%\n",
      "11\tValidation loss: 0.071067\tBest loss: 0.057257\tAccuracy: 98.51%\n",
      "12\tValidation loss: 0.093298\tBest loss: 0.057257\tAccuracy: 98.28%\n",
      "13\tValidation loss: 0.102210\tBest loss: 0.057257\tAccuracy: 97.85%\n",
      "14\tValidation loss: 0.109813\tBest loss: 0.057257\tAccuracy: 98.16%\n",
      "15\tValidation loss: 0.081265\tBest loss: 0.057257\tAccuracy: 98.28%\n",
      "16\tValidation loss: 0.068880\tBest loss: 0.057257\tAccuracy: 98.51%\n",
      "17\tValidation loss: 0.061760\tBest loss: 0.057257\tAccuracy: 98.28%\n",
      "18\tValidation loss: 0.063024\tBest loss: 0.057257\tAccuracy: 98.40%\n",
      "19\tValidation loss: 0.078393\tBest loss: 0.057257\tAccuracy: 98.32%\n",
      "20\tValidation loss: 0.101915\tBest loss: 0.057257\tAccuracy: 97.97%\n",
      "21\tValidation loss: 0.089251\tBest loss: 0.057257\tAccuracy: 98.12%\n",
      "22\tValidation loss: 0.065621\tBest loss: 0.057257\tAccuracy: 98.59%\n",
      "23\tValidation loss: 0.092477\tBest loss: 0.057257\tAccuracy: 97.77%\n",
      "24\tValidation loss: 0.146573\tBest loss: 0.057257\tAccuracy: 97.22%\n",
      "25\tValidation loss: 0.140315\tBest loss: 0.057257\tAccuracy: 97.34%\n",
      "26\tValidation loss: 0.168294\tBest loss: 0.057257\tAccuracy: 98.05%\n",
      "27\tValidation loss: 0.077650\tBest loss: 0.057257\tAccuracy: 98.08%\n",
      "28\tValidation loss: 0.088081\tBest loss: 0.057257\tAccuracy: 98.59%\n",
      "29\tValidation loss: 0.126362\tBest loss: 0.057257\tAccuracy: 97.89%\n",
      "30\tValidation loss: 0.086914\tBest loss: 0.057257\tAccuracy: 97.81%\n",
      "Early stopping!\n",
      "[CV]  activation=<function relu at 0x12920c1e0>, learning_rate=0.02, n_neurons=30, batch_size=100, total=  12.0s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.05, n_neurons=50, batch_size=100 \n",
      "0\tValidation loss: 0.316550\tBest loss: 0.316550\tAccuracy: 92.73%\n",
      "1\tValidation loss: 0.147735\tBest loss: 0.147735\tAccuracy: 95.35%\n",
      "2\tValidation loss: 218.851425\tBest loss: 0.147735\tAccuracy: 19.08%\n",
      "3\tValidation loss: 23.185188\tBest loss: 0.147735\tAccuracy: 15.05%\n",
      "4\tValidation loss: 7.107602\tBest loss: 0.147735\tAccuracy: 36.12%\n",
      "5\tValidation loss: 3.668442\tBest loss: 0.147735\tAccuracy: 49.69%\n",
      "6\tValidation loss: 2.199088\tBest loss: 0.147735\tAccuracy: 47.30%\n",
      "7\tValidation loss: 1.708365\tBest loss: 0.147735\tAccuracy: 51.37%\n",
      "8\tValidation loss: 2.916120\tBest loss: 0.147735\tAccuracy: 41.59%\n",
      "9\tValidation loss: 1.618981\tBest loss: 0.147735\tAccuracy: 54.81%\n",
      "10\tValidation loss: 0.867457\tBest loss: 0.147735\tAccuracy: 68.57%\n",
      "11\tValidation loss: 0.926033\tBest loss: 0.147735\tAccuracy: 65.75%\n",
      "12\tValidation loss: 0.772806\tBest loss: 0.147735\tAccuracy: 72.79%\n",
      "13\tValidation loss: 1.057465\tBest loss: 0.147735\tAccuracy: 63.96%\n",
      "14\tValidation loss: 0.750527\tBest loss: 0.147735\tAccuracy: 72.09%\n",
      "15\tValidation loss: 0.747062\tBest loss: 0.147735\tAccuracy: 74.08%\n",
      "16\tValidation loss: 0.973286\tBest loss: 0.147735\tAccuracy: 65.87%\n",
      "17\tValidation loss: 0.737982\tBest loss: 0.147735\tAccuracy: 73.77%\n",
      "18\tValidation loss: 0.579276\tBest loss: 0.147735\tAccuracy: 78.81%\n",
      "19\tValidation loss: 0.588034\tBest loss: 0.147735\tAccuracy: 78.38%\n",
      "20\tValidation loss: 0.573857\tBest loss: 0.147735\tAccuracy: 79.95%\n",
      "21\tValidation loss: 0.519905\tBest loss: 0.147735\tAccuracy: 82.17%\n",
      "22\tValidation loss: 0.467676\tBest loss: 0.147735\tAccuracy: 84.32%\n",
      "Early stopping!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.05, n_neurons=50, batch_size=100, total=  13.0s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.05, n_neurons=50, batch_size=100 \n",
      "0\tValidation loss: 0.144153\tBest loss: 0.144153\tAccuracy: 96.36%\n",
      "1\tValidation loss: 0.169146\tBest loss: 0.144153\tAccuracy: 95.97%\n",
      "2\tValidation loss: 18.746487\tBest loss: 0.144153\tAccuracy: 38.39%\n",
      "3\tValidation loss: 1.160032\tBest loss: 0.144153\tAccuracy: 69.90%\n",
      "4\tValidation loss: 5.571653\tBest loss: 0.144153\tAccuracy: 57.94%\n",
      "5\tValidation loss: 1.778084\tBest loss: 0.144153\tAccuracy: 66.46%\n",
      "6\tValidation loss: 0.667185\tBest loss: 0.144153\tAccuracy: 79.75%\n",
      "7\tValidation loss: 0.715547\tBest loss: 0.144153\tAccuracy: 75.92%\n",
      "8\tValidation loss: 0.682914\tBest loss: 0.144153\tAccuracy: 81.67%\n",
      "9\tValidation loss: 0.519801\tBest loss: 0.144153\tAccuracy: 81.55%\n",
      "10\tValidation loss: 0.973413\tBest loss: 0.144153\tAccuracy: 67.59%\n",
      "11\tValidation loss: 0.323697\tBest loss: 0.144153\tAccuracy: 90.19%\n",
      "12\tValidation loss: 0.311125\tBest loss: 0.144153\tAccuracy: 91.83%\n",
      "13\tValidation loss: 2.221849\tBest loss: 0.144153\tAccuracy: 66.07%\n",
      "14\tValidation loss: 432.431671\tBest loss: 0.144153\tAccuracy: 30.77%\n",
      "15\tValidation loss: 2007.683960\tBest loss: 0.144153\tAccuracy: 5.28%\n",
      "16\tValidation loss: 92.090652\tBest loss: 0.144153\tAccuracy: 21.07%\n",
      "17\tValidation loss: 42.261051\tBest loss: 0.144153\tAccuracy: 24.59%\n",
      "18\tValidation loss: 56.199028\tBest loss: 0.144153\tAccuracy: 29.09%\n",
      "19\tValidation loss: 14.273826\tBest loss: 0.144153\tAccuracy: 43.12%\n",
      "20\tValidation loss: 12.071667\tBest loss: 0.144153\tAccuracy: 53.52%\n",
      "21\tValidation loss: 19.017210\tBest loss: 0.144153\tAccuracy: 44.64%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.05, n_neurons=50, batch_size=100, total=  12.3s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.05, n_neurons=50, batch_size=100 \n",
      "0\tValidation loss: 0.301551\tBest loss: 0.301551\tAccuracy: 93.71%\n",
      "1\tValidation loss: 0.829887\tBest loss: 0.301551\tAccuracy: 68.61%\n",
      "2\tValidation loss: 0.471309\tBest loss: 0.301551\tAccuracy: 80.41%\n",
      "3\tValidation loss: 0.248580\tBest loss: 0.248580\tAccuracy: 92.85%\n",
      "4\tValidation loss: 0.219082\tBest loss: 0.219082\tAccuracy: 93.55%\n",
      "5\tValidation loss: 0.193752\tBest loss: 0.193752\tAccuracy: 94.84%\n",
      "6\tValidation loss: 0.162231\tBest loss: 0.162231\tAccuracy: 95.50%\n",
      "7\tValidation loss: 1798.457397\tBest loss: 0.162231\tAccuracy: 20.88%\n",
      "8\tValidation loss: 13.882420\tBest loss: 0.162231\tAccuracy: 59.93%\n",
      "9\tValidation loss: 14.642152\tBest loss: 0.162231\tAccuracy: 58.56%\n",
      "10\tValidation loss: 5.846180\tBest loss: 0.162231\tAccuracy: 70.72%\n",
      "11\tValidation loss: 3.986018\tBest loss: 0.162231\tAccuracy: 76.58%\n",
      "12\tValidation loss: 4.476730\tBest loss: 0.162231\tAccuracy: 75.02%\n",
      "13\tValidation loss: 4.769586\tBest loss: 0.162231\tAccuracy: 72.20%\n",
      "14\tValidation loss: 2.152272\tBest loss: 0.162231\tAccuracy: 81.70%\n",
      "15\tValidation loss: 2.528306\tBest loss: 0.162231\tAccuracy: 82.84%\n",
      "16\tValidation loss: 1.771733\tBest loss: 0.162231\tAccuracy: 83.42%\n",
      "17\tValidation loss: 2.804108\tBest loss: 0.162231\tAccuracy: 75.49%\n",
      "18\tValidation loss: 1.187025\tBest loss: 0.162231\tAccuracy: 86.67%\n",
      "19\tValidation loss: 34.293293\tBest loss: 0.162231\tAccuracy: 38.70%\n",
      "20\tValidation loss: 3.409877\tBest loss: 0.162231\tAccuracy: 69.55%\n",
      "21\tValidation loss: 5.752376\tBest loss: 0.162231\tAccuracy: 69.74%\n",
      "22\tValidation loss: 2.181871\tBest loss: 0.162231\tAccuracy: 75.61%\n",
      "23\tValidation loss: 1.204932\tBest loss: 0.162231\tAccuracy: 82.92%\n",
      "24\tValidation loss: 0.851700\tBest loss: 0.162231\tAccuracy: 88.39%\n",
      "25\tValidation loss: 0.834377\tBest loss: 0.162231\tAccuracy: 86.94%\n",
      "26\tValidation loss: 2.957385\tBest loss: 0.162231\tAccuracy: 66.89%\n",
      "27\tValidation loss: 0.849596\tBest loss: 0.162231\tAccuracy: 88.82%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.05, n_neurons=50, batch_size=100, total=  15.9s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.01, n_neurons=50, batch_size=50 \n",
      "0\tValidation loss: 0.108575\tBest loss: 0.108575\tAccuracy: 97.03%\n",
      "1\tValidation loss: 0.086926\tBest loss: 0.086926\tAccuracy: 97.77%\n",
      "2\tValidation loss: 0.102755\tBest loss: 0.086926\tAccuracy: 97.73%\n",
      "3\tValidation loss: 0.264582\tBest loss: 0.086926\tAccuracy: 91.01%\n",
      "4\tValidation loss: 0.078609\tBest loss: 0.078609\tAccuracy: 97.65%\n",
      "5\tValidation loss: 0.107375\tBest loss: 0.078609\tAccuracy: 97.50%\n",
      "6\tValidation loss: 0.097018\tBest loss: 0.078609\tAccuracy: 97.42%\n",
      "7\tValidation loss: 0.616203\tBest loss: 0.078609\tAccuracy: 95.23%\n",
      "8\tValidation loss: 0.070472\tBest loss: 0.070472\tAccuracy: 97.73%\n",
      "9\tValidation loss: 0.058512\tBest loss: 0.058512\tAccuracy: 98.20%\n",
      "10\tValidation loss: 0.076184\tBest loss: 0.058512\tAccuracy: 98.28%\n",
      "11\tValidation loss: 0.072358\tBest loss: 0.058512\tAccuracy: 98.40%\n",
      "12\tValidation loss: 0.147265\tBest loss: 0.058512\tAccuracy: 98.08%\n",
      "13\tValidation loss: 0.158017\tBest loss: 0.058512\tAccuracy: 97.85%\n",
      "14\tValidation loss: 0.080950\tBest loss: 0.058512\tAccuracy: 98.40%\n",
      "15\tValidation loss: 0.192501\tBest loss: 0.058512\tAccuracy: 97.89%\n",
      "16\tValidation loss: 17.700747\tBest loss: 0.058512\tAccuracy: 75.92%\n",
      "17\tValidation loss: 0.158496\tBest loss: 0.058512\tAccuracy: 97.62%\n",
      "18\tValidation loss: 0.123997\tBest loss: 0.058512\tAccuracy: 97.89%\n",
      "19\tValidation loss: 0.123628\tBest loss: 0.058512\tAccuracy: 97.73%\n",
      "20\tValidation loss: 0.139569\tBest loss: 0.058512\tAccuracy: 97.89%\n",
      "21\tValidation loss: 0.254309\tBest loss: 0.058512\tAccuracy: 98.01%\n",
      "22\tValidation loss: 0.114146\tBest loss: 0.058512\tAccuracy: 98.05%\n",
      "23\tValidation loss: 0.148838\tBest loss: 0.058512\tAccuracy: 98.08%\n",
      "24\tValidation loss: 0.188802\tBest loss: 0.058512\tAccuracy: 97.93%\n",
      "25\tValidation loss: 0.266609\tBest loss: 0.058512\tAccuracy: 97.81%\n",
      "26\tValidation loss: 0.504884\tBest loss: 0.058512\tAccuracy: 97.11%\n",
      "27\tValidation loss: 0.227724\tBest loss: 0.058512\tAccuracy: 97.30%\n",
      "28\tValidation loss: 0.192531\tBest loss: 0.058512\tAccuracy: 98.08%\n",
      "29\tValidation loss: 0.268991\tBest loss: 0.058512\tAccuracy: 98.20%\n",
      "30\tValidation loss: 0.240519\tBest loss: 0.058512\tAccuracy: 97.30%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.01, n_neurons=50, batch_size=50, total=  25.9s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.01, n_neurons=50, batch_size=50 \n",
      "0\tValidation loss: 0.090241\tBest loss: 0.090241\tAccuracy: 97.54%\n",
      "1\tValidation loss: 0.069755\tBest loss: 0.069755\tAccuracy: 98.08%\n",
      "2\tValidation loss: 0.110229\tBest loss: 0.069755\tAccuracy: 96.95%\n",
      "3\tValidation loss: 0.077643\tBest loss: 0.069755\tAccuracy: 98.05%\n",
      "4\tValidation loss: 0.190037\tBest loss: 0.069755\tAccuracy: 95.58%\n",
      "5\tValidation loss: 0.111011\tBest loss: 0.069755\tAccuracy: 98.20%\n",
      "6\tValidation loss: 0.054039\tBest loss: 0.054039\tAccuracy: 98.48%\n",
      "7\tValidation loss: 0.075331\tBest loss: 0.054039\tAccuracy: 98.55%\n",
      "8\tValidation loss: 0.096330\tBest loss: 0.054039\tAccuracy: 98.51%\n",
      "9\tValidation loss: 1.939777\tBest loss: 0.054039\tAccuracy: 89.91%\n",
      "10\tValidation loss: 0.107613\tBest loss: 0.054039\tAccuracy: 97.50%\n",
      "11\tValidation loss: 0.069038\tBest loss: 0.054039\tAccuracy: 98.05%\n",
      "12\tValidation loss: 0.086694\tBest loss: 0.054039\tAccuracy: 98.24%\n",
      "13\tValidation loss: 0.074485\tBest loss: 0.054039\tAccuracy: 98.32%\n",
      "14\tValidation loss: 0.071177\tBest loss: 0.054039\tAccuracy: 98.28%\n",
      "15\tValidation loss: 0.134259\tBest loss: 0.054039\tAccuracy: 98.28%\n",
      "16\tValidation loss: 0.102239\tBest loss: 0.054039\tAccuracy: 98.40%\n",
      "17\tValidation loss: 0.193106\tBest loss: 0.054039\tAccuracy: 97.65%\n",
      "18\tValidation loss: 0.283021\tBest loss: 0.054039\tAccuracy: 96.64%\n",
      "19\tValidation loss: 0.126598\tBest loss: 0.054039\tAccuracy: 97.73%\n",
      "20\tValidation loss: 0.104652\tBest loss: 0.054039\tAccuracy: 97.93%\n",
      "21\tValidation loss: 0.110511\tBest loss: 0.054039\tAccuracy: 98.05%\n",
      "22\tValidation loss: 0.135268\tBest loss: 0.054039\tAccuracy: 98.08%\n",
      "23\tValidation loss: 0.113658\tBest loss: 0.054039\tAccuracy: 98.32%\n",
      "24\tValidation loss: 0.215093\tBest loss: 0.054039\tAccuracy: 97.26%\n",
      "25\tValidation loss: 0.937463\tBest loss: 0.054039\tAccuracy: 95.93%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\tValidation loss: 0.713008\tBest loss: 0.054039\tAccuracy: 97.65%\n",
      "27\tValidation loss: 0.375838\tBest loss: 0.054039\tAccuracy: 97.46%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.01, n_neurons=50, batch_size=50, total=  23.4s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.01, n_neurons=50, batch_size=50 \n",
      "0\tValidation loss: 0.102335\tBest loss: 0.102335\tAccuracy: 97.46%\n",
      "1\tValidation loss: 0.081099\tBest loss: 0.081099\tAccuracy: 97.97%\n",
      "2\tValidation loss: 0.106662\tBest loss: 0.081099\tAccuracy: 96.72%\n",
      "3\tValidation loss: 0.077179\tBest loss: 0.077179\tAccuracy: 98.05%\n",
      "4\tValidation loss: 0.099821\tBest loss: 0.077179\tAccuracy: 98.05%\n",
      "5\tValidation loss: 0.065434\tBest loss: 0.065434\tAccuracy: 98.44%\n",
      "6\tValidation loss: 0.272343\tBest loss: 0.065434\tAccuracy: 94.72%\n",
      "7\tValidation loss: 0.094835\tBest loss: 0.065434\tAccuracy: 97.97%\n",
      "8\tValidation loss: 0.139412\tBest loss: 0.065434\tAccuracy: 98.05%\n",
      "9\tValidation loss: 0.105553\tBest loss: 0.065434\tAccuracy: 97.58%\n",
      "10\tValidation loss: 0.117918\tBest loss: 0.065434\tAccuracy: 98.12%\n",
      "11\tValidation loss: 0.111141\tBest loss: 0.065434\tAccuracy: 98.16%\n",
      "12\tValidation loss: 0.106369\tBest loss: 0.065434\tAccuracy: 97.97%\n",
      "13\tValidation loss: 0.124023\tBest loss: 0.065434\tAccuracy: 97.81%\n",
      "14\tValidation loss: 0.115319\tBest loss: 0.065434\tAccuracy: 97.69%\n",
      "15\tValidation loss: 0.090186\tBest loss: 0.065434\tAccuracy: 98.32%\n",
      "16\tValidation loss: 0.130059\tBest loss: 0.065434\tAccuracy: 96.99%\n",
      "17\tValidation loss: 0.206691\tBest loss: 0.065434\tAccuracy: 97.26%\n",
      "18\tValidation loss: 0.181718\tBest loss: 0.065434\tAccuracy: 97.54%\n",
      "19\tValidation loss: 0.134391\tBest loss: 0.065434\tAccuracy: 97.85%\n",
      "20\tValidation loss: 0.131497\tBest loss: 0.065434\tAccuracy: 98.40%\n",
      "21\tValidation loss: 0.231735\tBest loss: 0.065434\tAccuracy: 98.40%\n",
      "22\tValidation loss: 6.114071\tBest loss: 0.065434\tAccuracy: 92.46%\n",
      "23\tValidation loss: 0.397521\tBest loss: 0.065434\tAccuracy: 97.46%\n",
      "24\tValidation loss: 0.195564\tBest loss: 0.065434\tAccuracy: 98.12%\n",
      "25\tValidation loss: 0.231222\tBest loss: 0.065434\tAccuracy: 97.97%\n",
      "26\tValidation loss: 0.276152\tBest loss: 0.065434\tAccuracy: 97.85%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.01, n_neurons=50, batch_size=50, total=  22.6s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.02, n_neurons=120, batch_size=10 \n",
      "0\tValidation loss: 0.542576\tBest loss: 0.542576\tAccuracy: 92.81%\n",
      "1\tValidation loss: 59.580692\tBest loss: 0.542576\tAccuracy: 96.25%\n",
      "2\tValidation loss: 7062.786621\tBest loss: 0.542576\tAccuracy: 64.78%\n",
      "3\tValidation loss: 390.682709\tBest loss: 0.542576\tAccuracy: 85.11%\n",
      "4\tValidation loss: 502.863129\tBest loss: 0.542576\tAccuracy: 82.56%\n",
      "5\tValidation loss: 21.947945\tBest loss: 0.542576\tAccuracy: 97.11%\n",
      "6\tValidation loss: 568.654724\tBest loss: 0.542576\tAccuracy: 96.83%\n",
      "7\tValidation loss: 907.364807\tBest loss: 0.542576\tAccuracy: 95.47%\n",
      "8\tValidation loss: 387.093231\tBest loss: 0.542576\tAccuracy: 94.61%\n",
      "9\tValidation loss: 1534.735962\tBest loss: 0.542576\tAccuracy: 96.36%\n",
      "10\tValidation loss: 2383.657715\tBest loss: 0.542576\tAccuracy: 96.09%\n",
      "11\tValidation loss: 810.884766\tBest loss: 0.542576\tAccuracy: 97.30%\n",
      "12\tValidation loss: 3385.096924\tBest loss: 0.542576\tAccuracy: 96.29%\n",
      "13\tValidation loss: 2114.335205\tBest loss: 0.542576\tAccuracy: 95.66%\n",
      "14\tValidation loss: 2056.609375\tBest loss: 0.542576\tAccuracy: 95.35%\n",
      "15\tValidation loss: 721.752930\tBest loss: 0.542576\tAccuracy: 97.03%\n",
      "16\tValidation loss: 7607.797363\tBest loss: 0.542576\tAccuracy: 96.48%\n",
      "17\tValidation loss: 15834.507812\tBest loss: 0.542576\tAccuracy: 94.25%\n",
      "18\tValidation loss: 4586.412598\tBest loss: 0.542576\tAccuracy: 95.39%\n",
      "19\tValidation loss: 8327.377930\tBest loss: 0.542576\tAccuracy: 96.79%\n",
      "20\tValidation loss: 8928.788086\tBest loss: 0.542576\tAccuracy: 94.02%\n",
      "21\tValidation loss: 3084.592529\tBest loss: 0.542576\tAccuracy: 97.19%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.02, n_neurons=120, batch_size=10, total= 1.6min\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.02, n_neurons=120, batch_size=10 \n",
      "0\tValidation loss: 159.449173\tBest loss: 159.449173\tAccuracy: 87.92%\n",
      "1\tValidation loss: 126.577896\tBest loss: 126.577896\tAccuracy: 91.48%\n",
      "2\tValidation loss: 23.481611\tBest loss: 23.481611\tAccuracy: 95.90%\n",
      "3\tValidation loss: 413.905243\tBest loss: 23.481611\tAccuracy: 94.14%\n",
      "4\tValidation loss: 310.632874\tBest loss: 23.481611\tAccuracy: 95.74%\n",
      "5\tValidation loss: 315.154114\tBest loss: 23.481611\tAccuracy: 97.34%\n",
      "6\tValidation loss: 657.239868\tBest loss: 23.481611\tAccuracy: 97.15%\n",
      "7\tValidation loss: 1435.209839\tBest loss: 23.481611\tAccuracy: 94.21%\n",
      "8\tValidation loss: 164.659760\tBest loss: 23.481611\tAccuracy: 96.99%\n",
      "9\tValidation loss: 162.957108\tBest loss: 23.481611\tAccuracy: 95.35%\n",
      "10\tValidation loss: 1374.459229\tBest loss: 23.481611\tAccuracy: 96.79%\n",
      "11\tValidation loss: 825.038574\tBest loss: 23.481611\tAccuracy: 97.30%\n",
      "12\tValidation loss: 1198.321533\tBest loss: 23.481611\tAccuracy: 93.75%\n",
      "13\tValidation loss: 4239.864258\tBest loss: 23.481611\tAccuracy: 95.82%\n",
      "14\tValidation loss: 1731.550415\tBest loss: 23.481611\tAccuracy: 93.94%\n",
      "15\tValidation loss: 1459.086548\tBest loss: 23.481611\tAccuracy: 94.10%\n",
      "16\tValidation loss: 75381.632812\tBest loss: 23.481611\tAccuracy: 89.25%\n",
      "17\tValidation loss: 65627.132812\tBest loss: 23.481611\tAccuracy: 97.22%\n",
      "18\tValidation loss: 57548.441406\tBest loss: 23.481611\tAccuracy: 93.86%\n",
      "19\tValidation loss: 2171.703613\tBest loss: 23.481611\tAccuracy: 97.19%\n",
      "20\tValidation loss: 3495.135742\tBest loss: 23.481611\tAccuracy: 97.19%\n",
      "21\tValidation loss: 7715.929688\tBest loss: 23.481611\tAccuracy: 97.11%\n",
      "22\tValidation loss: 4005.279785\tBest loss: 23.481611\tAccuracy: 98.12%\n",
      "23\tValidation loss: 8992.521484\tBest loss: 23.481611\tAccuracy: 97.15%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.02, n_neurons=120, batch_size=10, total= 1.7min\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.02, n_neurons=120, batch_size=10 \n",
      "0\tValidation loss: 1.040517\tBest loss: 1.040517\tAccuracy: 95.47%\n",
      "1\tValidation loss: 2626.832764\tBest loss: 1.040517\tAccuracy: 67.28%\n",
      "2\tValidation loss: 220.314423\tBest loss: 1.040517\tAccuracy: 94.49%\n",
      "3\tValidation loss: 398.574585\tBest loss: 1.040517\tAccuracy: 94.06%\n",
      "4\tValidation loss: 100.193817\tBest loss: 1.040517\tAccuracy: 95.54%\n",
      "5\tValidation loss: 4725.893555\tBest loss: 1.040517\tAccuracy: 94.10%\n",
      "6\tValidation loss: 629.311707\tBest loss: 1.040517\tAccuracy: 95.19%\n",
      "7\tValidation loss: 1446.989014\tBest loss: 1.040517\tAccuracy: 97.46%\n",
      "8\tValidation loss: 981.666748\tBest loss: 1.040517\tAccuracy: 94.21%\n",
      "9\tValidation loss: 13106.077148\tBest loss: 1.040517\tAccuracy: 90.46%\n",
      "10\tValidation loss: 2316.461670\tBest loss: 1.040517\tAccuracy: 96.99%\n",
      "11\tValidation loss: 3086.425293\tBest loss: 1.040517\tAccuracy: 96.68%\n",
      "12\tValidation loss: 4899.854492\tBest loss: 1.040517\tAccuracy: 95.47%\n",
      "13\tValidation loss: 645.753296\tBest loss: 1.040517\tAccuracy: 96.95%\n",
      "14\tValidation loss: 5827.896973\tBest loss: 1.040517\tAccuracy: 95.90%\n",
      "15\tValidation loss: 38206.945312\tBest loss: 1.040517\tAccuracy: 94.57%\n",
      "16\tValidation loss: 4448.940430\tBest loss: 1.040517\tAccuracy: 97.73%\n",
      "17\tValidation loss: 4144.960938\tBest loss: 1.040517\tAccuracy: 97.22%\n",
      "18\tValidation loss: 4411.196289\tBest loss: 1.040517\tAccuracy: 98.20%\n",
      "19\tValidation loss: 65369.820312\tBest loss: 1.040517\tAccuracy: 87.69%\n",
      "20\tValidation loss: 25324.531250\tBest loss: 1.040517\tAccuracy: 97.97%\n",
      "21\tValidation loss: 11040.542969\tBest loss: 1.040517\tAccuracy: 97.26%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.02, n_neurons=120, batch_size=10, total= 1.5min\n",
      "[CV] activation=<function elu at 0x129284e18>, learning_rate=0.1, n_neurons=50, batch_size=500 \n",
      "0\tValidation loss: 0.818072\tBest loss: 0.818072\tAccuracy: 62.39%\n",
      "1\tValidation loss: 0.519583\tBest loss: 0.519583\tAccuracy: 80.73%\n",
      "2\tValidation loss: 0.450057\tBest loss: 0.450057\tAccuracy: 85.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\tValidation loss: 0.317056\tBest loss: 0.317056\tAccuracy: 89.76%\n",
      "4\tValidation loss: 0.257634\tBest loss: 0.257634\tAccuracy: 92.49%\n",
      "5\tValidation loss: 0.199213\tBest loss: 0.199213\tAccuracy: 95.00%\n",
      "6\tValidation loss: 0.199480\tBest loss: 0.199213\tAccuracy: 95.07%\n",
      "7\tValidation loss: 0.192261\tBest loss: 0.192261\tAccuracy: 95.23%\n",
      "8\tValidation loss: 0.173756\tBest loss: 0.173756\tAccuracy: 95.86%\n",
      "9\tValidation loss: 0.168159\tBest loss: 0.168159\tAccuracy: 96.48%\n",
      "10\tValidation loss: 0.171228\tBest loss: 0.168159\tAccuracy: 95.86%\n",
      "11\tValidation loss: 0.161242\tBest loss: 0.161242\tAccuracy: 96.21%\n",
      "12\tValidation loss: 0.160804\tBest loss: 0.160804\tAccuracy: 95.70%\n",
      "13\tValidation loss: 0.128371\tBest loss: 0.128371\tAccuracy: 96.68%\n",
      "14\tValidation loss: 0.117058\tBest loss: 0.117058\tAccuracy: 96.79%\n",
      "15\tValidation loss: 0.111798\tBest loss: 0.111798\tAccuracy: 97.15%\n",
      "16\tValidation loss: 0.112581\tBest loss: 0.111798\tAccuracy: 97.19%\n",
      "17\tValidation loss: 0.126270\tBest loss: 0.111798\tAccuracy: 96.99%\n",
      "18\tValidation loss: 0.114703\tBest loss: 0.111798\tAccuracy: 97.15%\n",
      "19\tValidation loss: 0.117952\tBest loss: 0.111798\tAccuracy: 97.19%\n",
      "20\tValidation loss: 0.113590\tBest loss: 0.111798\tAccuracy: 97.19%\n",
      "21\tValidation loss: 0.120767\tBest loss: 0.111798\tAccuracy: 96.95%\n",
      "22\tValidation loss: 0.117355\tBest loss: 0.111798\tAccuracy: 97.11%\n",
      "23\tValidation loss: 0.132827\tBest loss: 0.111798\tAccuracy: 96.99%\n",
      "24\tValidation loss: 0.151890\tBest loss: 0.111798\tAccuracy: 96.52%\n",
      "25\tValidation loss: 0.220969\tBest loss: 0.111798\tAccuracy: 95.54%\n",
      "26\tValidation loss: 0.135212\tBest loss: 0.111798\tAccuracy: 96.52%\n",
      "27\tValidation loss: 0.128200\tBest loss: 0.111798\tAccuracy: 96.48%\n",
      "28\tValidation loss: 0.116586\tBest loss: 0.111798\tAccuracy: 96.91%\n",
      "29\tValidation loss: 0.117648\tBest loss: 0.111798\tAccuracy: 97.19%\n",
      "30\tValidation loss: 0.101755\tBest loss: 0.101755\tAccuracy: 97.22%\n",
      "31\tValidation loss: 0.109509\tBest loss: 0.101755\tAccuracy: 97.46%\n",
      "32\tValidation loss: 0.098990\tBest loss: 0.098990\tAccuracy: 97.62%\n",
      "33\tValidation loss: 0.114443\tBest loss: 0.098990\tAccuracy: 97.38%\n",
      "34\tValidation loss: 0.127990\tBest loss: 0.098990\tAccuracy: 96.64%\n",
      "35\tValidation loss: 0.116933\tBest loss: 0.098990\tAccuracy: 97.22%\n",
      "36\tValidation loss: 0.138003\tBest loss: 0.098990\tAccuracy: 96.91%\n",
      "37\tValidation loss: 0.112381\tBest loss: 0.098990\tAccuracy: 97.62%\n",
      "38\tValidation loss: 0.116895\tBest loss: 0.098990\tAccuracy: 97.46%\n",
      "39\tValidation loss: 0.136604\tBest loss: 0.098990\tAccuracy: 96.83%\n",
      "40\tValidation loss: 0.112282\tBest loss: 0.098990\tAccuracy: 97.58%\n",
      "41\tValidation loss: 0.136782\tBest loss: 0.098990\tAccuracy: 96.87%\n",
      "42\tValidation loss: 0.122612\tBest loss: 0.098990\tAccuracy: 97.54%\n",
      "43\tValidation loss: 0.124564\tBest loss: 0.098990\tAccuracy: 97.34%\n",
      "44\tValidation loss: 0.128526\tBest loss: 0.098990\tAccuracy: 97.38%\n",
      "45\tValidation loss: 0.108982\tBest loss: 0.098990\tAccuracy: 97.34%\n",
      "46\tValidation loss: 0.123273\tBest loss: 0.098990\tAccuracy: 97.34%\n",
      "47\tValidation loss: 0.091353\tBest loss: 0.091353\tAccuracy: 97.69%\n",
      "48\tValidation loss: 0.123777\tBest loss: 0.091353\tAccuracy: 97.54%\n",
      "49\tValidation loss: 0.109031\tBest loss: 0.091353\tAccuracy: 97.85%\n",
      "50\tValidation loss: 0.106593\tBest loss: 0.091353\tAccuracy: 97.69%\n",
      "51\tValidation loss: 0.157927\tBest loss: 0.091353\tAccuracy: 96.79%\n",
      "52\tValidation loss: 0.094092\tBest loss: 0.091353\tAccuracy: 97.77%\n",
      "53\tValidation loss: 0.098384\tBest loss: 0.091353\tAccuracy: 98.16%\n",
      "54\tValidation loss: 0.096725\tBest loss: 0.091353\tAccuracy: 97.97%\n",
      "55\tValidation loss: 0.128248\tBest loss: 0.091353\tAccuracy: 97.07%\n",
      "56\tValidation loss: 0.154612\tBest loss: 0.091353\tAccuracy: 96.72%\n",
      "57\tValidation loss: 0.140484\tBest loss: 0.091353\tAccuracy: 97.22%\n",
      "58\tValidation loss: 0.131865\tBest loss: 0.091353\tAccuracy: 97.50%\n",
      "59\tValidation loss: 0.128902\tBest loss: 0.091353\tAccuracy: 97.97%\n",
      "60\tValidation loss: 0.103537\tBest loss: 0.091353\tAccuracy: 97.77%\n",
      "61\tValidation loss: 0.156326\tBest loss: 0.091353\tAccuracy: 97.15%\n",
      "62\tValidation loss: 0.122738\tBest loss: 0.091353\tAccuracy: 97.58%\n",
      "63\tValidation loss: 0.132742\tBest loss: 0.091353\tAccuracy: 97.58%\n",
      "64\tValidation loss: 0.131847\tBest loss: 0.091353\tAccuracy: 97.11%\n",
      "65\tValidation loss: 0.156188\tBest loss: 0.091353\tAccuracy: 97.54%\n",
      "66\tValidation loss: 5.191550\tBest loss: 0.091353\tAccuracy: 18.73%\n",
      "67\tValidation loss: 1.699020\tBest loss: 0.091353\tAccuracy: 20.91%\n",
      "68\tValidation loss: 1.622510\tBest loss: 0.091353\tAccuracy: 22.01%\n",
      "Early stopping!\n",
      "[CV]  activation=<function elu at 0x129284e18>, learning_rate=0.1, n_neurons=50, batch_size=500, total=  20.6s\n",
      "[CV] activation=<function elu at 0x129284e18>, learning_rate=0.1, n_neurons=50, batch_size=500 \n",
      "0\tValidation loss: 0.798633\tBest loss: 0.798633\tAccuracy: 69.23%\n",
      "1\tValidation loss: 0.302374\tBest loss: 0.302374\tAccuracy: 90.73%\n",
      "2\tValidation loss: 0.200110\tBest loss: 0.200110\tAccuracy: 93.90%\n",
      "3\tValidation loss: 0.162017\tBest loss: 0.162017\tAccuracy: 95.15%\n",
      "4\tValidation loss: 0.149001\tBest loss: 0.149001\tAccuracy: 95.70%\n",
      "5\tValidation loss: 0.153190\tBest loss: 0.149001\tAccuracy: 95.43%\n",
      "6\tValidation loss: 0.123075\tBest loss: 0.123075\tAccuracy: 96.40%\n",
      "7\tValidation loss: 0.127688\tBest loss: 0.123075\tAccuracy: 96.44%\n",
      "8\tValidation loss: 0.130835\tBest loss: 0.123075\tAccuracy: 96.76%\n",
      "9\tValidation loss: 0.129153\tBest loss: 0.123075\tAccuracy: 96.87%\n",
      "10\tValidation loss: 0.137013\tBest loss: 0.123075\tAccuracy: 96.64%\n",
      "11\tValidation loss: 0.114397\tBest loss: 0.114397\tAccuracy: 97.15%\n",
      "12\tValidation loss: 0.150163\tBest loss: 0.114397\tAccuracy: 96.87%\n",
      "13\tValidation loss: 0.113617\tBest loss: 0.113617\tAccuracy: 96.95%\n",
      "14\tValidation loss: 0.139481\tBest loss: 0.113617\tAccuracy: 96.60%\n",
      "15\tValidation loss: 0.149066\tBest loss: 0.113617\tAccuracy: 96.44%\n",
      "16\tValidation loss: 0.121002\tBest loss: 0.113617\tAccuracy: 96.64%\n",
      "17\tValidation loss: 0.113568\tBest loss: 0.113568\tAccuracy: 96.95%\n",
      "18\tValidation loss: 0.109209\tBest loss: 0.109209\tAccuracy: 97.19%\n",
      "19\tValidation loss: 0.120750\tBest loss: 0.109209\tAccuracy: 97.15%\n",
      "20\tValidation loss: 0.110529\tBest loss: 0.109209\tAccuracy: 97.19%\n",
      "21\tValidation loss: 0.119861\tBest loss: 0.109209\tAccuracy: 97.07%\n",
      "22\tValidation loss: 0.130893\tBest loss: 0.109209\tAccuracy: 96.91%\n",
      "23\tValidation loss: 0.118014\tBest loss: 0.109209\tAccuracy: 97.22%\n",
      "24\tValidation loss: 0.109532\tBest loss: 0.109209\tAccuracy: 97.26%\n",
      "25\tValidation loss: 0.116598\tBest loss: 0.109209\tAccuracy: 97.15%\n",
      "26\tValidation loss: 0.115250\tBest loss: 0.109209\tAccuracy: 97.42%\n",
      "27\tValidation loss: 0.162725\tBest loss: 0.109209\tAccuracy: 97.22%\n",
      "28\tValidation loss: 0.161066\tBest loss: 0.109209\tAccuracy: 96.60%\n",
      "29\tValidation loss: 0.177051\tBest loss: 0.109209\tAccuracy: 96.56%\n",
      "30\tValidation loss: 0.152115\tBest loss: 0.109209\tAccuracy: 96.76%\n",
      "31\tValidation loss: 0.141355\tBest loss: 0.109209\tAccuracy: 97.15%\n",
      "32\tValidation loss: 0.131007\tBest loss: 0.109209\tAccuracy: 97.30%\n",
      "33\tValidation loss: 0.127842\tBest loss: 0.109209\tAccuracy: 97.26%\n",
      "34\tValidation loss: 0.109236\tBest loss: 0.109209\tAccuracy: 97.42%\n",
      "35\tValidation loss: 0.141628\tBest loss: 0.109209\tAccuracy: 97.11%\n",
      "36\tValidation loss: 0.115727\tBest loss: 0.109209\tAccuracy: 97.62%\n",
      "37\tValidation loss: 0.115584\tBest loss: 0.109209\tAccuracy: 97.54%\n",
      "38\tValidation loss: 0.114652\tBest loss: 0.109209\tAccuracy: 97.62%\n",
      "39\tValidation loss: 0.118872\tBest loss: 0.109209\tAccuracy: 97.54%\n",
      "Early stopping!\n",
      "[CV]  activation=<function elu at 0x129284e18>, learning_rate=0.1, n_neurons=50, batch_size=500, total=  11.1s\n",
      "[CV] activation=<function elu at 0x129284e18>, learning_rate=0.1, n_neurons=50, batch_size=500 \n",
      "0\tValidation loss: 0.979323\tBest loss: 0.979323\tAccuracy: 74.67%\n",
      "1\tValidation loss: 0.220529\tBest loss: 0.220529\tAccuracy: 94.41%\n",
      "2\tValidation loss: 0.187575\tBest loss: 0.187575\tAccuracy: 94.88%\n",
      "3\tValidation loss: 0.131708\tBest loss: 0.131708\tAccuracy: 96.25%\n",
      "4\tValidation loss: 0.119714\tBest loss: 0.119714\tAccuracy: 96.52%\n",
      "5\tValidation loss: 0.114192\tBest loss: 0.114192\tAccuracy: 96.68%\n",
      "6\tValidation loss: 0.121883\tBest loss: 0.114192\tAccuracy: 96.56%\n",
      "7\tValidation loss: 0.163887\tBest loss: 0.114192\tAccuracy: 95.70%\n",
      "8\tValidation loss: 0.137015\tBest loss: 0.114192\tAccuracy: 96.48%\n",
      "9\tValidation loss: 0.100888\tBest loss: 0.100888\tAccuracy: 97.30%\n",
      "10\tValidation loss: 0.096754\tBest loss: 0.096754\tAccuracy: 97.15%\n",
      "11\tValidation loss: 0.110300\tBest loss: 0.096754\tAccuracy: 97.19%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\tValidation loss: 0.138570\tBest loss: 0.096754\tAccuracy: 96.09%\n",
      "13\tValidation loss: 0.116385\tBest loss: 0.096754\tAccuracy: 97.46%\n",
      "14\tValidation loss: 0.115596\tBest loss: 0.096754\tAccuracy: 97.34%\n",
      "15\tValidation loss: 0.120082\tBest loss: 0.096754\tAccuracy: 96.95%\n",
      "16\tValidation loss: 0.114885\tBest loss: 0.096754\tAccuracy: 97.34%\n",
      "17\tValidation loss: 0.107821\tBest loss: 0.096754\tAccuracy: 97.15%\n",
      "18\tValidation loss: 0.151446\tBest loss: 0.096754\tAccuracy: 95.82%\n",
      "19\tValidation loss: 0.131202\tBest loss: 0.096754\tAccuracy: 96.99%\n",
      "20\tValidation loss: 0.138195\tBest loss: 0.096754\tAccuracy: 97.22%\n",
      "21\tValidation loss: 0.107345\tBest loss: 0.096754\tAccuracy: 97.58%\n",
      "22\tValidation loss: 0.105348\tBest loss: 0.096754\tAccuracy: 97.58%\n",
      "23\tValidation loss: 0.119292\tBest loss: 0.096754\tAccuracy: 97.69%\n",
      "24\tValidation loss: 0.132247\tBest loss: 0.096754\tAccuracy: 96.87%\n",
      "25\tValidation loss: 0.136653\tBest loss: 0.096754\tAccuracy: 97.26%\n",
      "26\tValidation loss: 0.128638\tBest loss: 0.096754\tAccuracy: 97.54%\n",
      "27\tValidation loss: 0.125228\tBest loss: 0.096754\tAccuracy: 97.62%\n",
      "28\tValidation loss: 0.162411\tBest loss: 0.096754\tAccuracy: 97.03%\n",
      "29\tValidation loss: 0.130869\tBest loss: 0.096754\tAccuracy: 97.62%\n",
      "30\tValidation loss: 0.115613\tBest loss: 0.096754\tAccuracy: 97.58%\n",
      "31\tValidation loss: 0.114957\tBest loss: 0.096754\tAccuracy: 97.65%\n",
      "Early stopping!\n",
      "[CV]  activation=<function elu at 0x129284e18>, learning_rate=0.1, n_neurons=50, batch_size=500, total=  10.7s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.1, n_neurons=100, batch_size=50 \n",
      "0\tValidation loss: 3136.043701\tBest loss: 3136.043701\tAccuracy: 61.96%\n",
      "1\tValidation loss: 643.084167\tBest loss: 643.084167\tAccuracy: 89.37%\n",
      "2\tValidation loss: 2017.171997\tBest loss: 643.084167\tAccuracy: 38.70%\n",
      "3\tValidation loss: 245.851303\tBest loss: 245.851303\tAccuracy: 61.81%\n",
      "4\tValidation loss: 229.907806\tBest loss: 229.907806\tAccuracy: 69.08%\n",
      "5\tValidation loss: 3259.735596\tBest loss: 229.907806\tAccuracy: 37.18%\n",
      "6\tValidation loss: 2628.657227\tBest loss: 229.907806\tAccuracy: 38.39%\n",
      "7\tValidation loss: 494.277374\tBest loss: 229.907806\tAccuracy: 48.32%\n",
      "8\tValidation loss: 64127.355469\tBest loss: 229.907806\tAccuracy: 50.39%\n",
      "9\tValidation loss: 15569.570312\tBest loss: 229.907806\tAccuracy: 74.75%\n",
      "10\tValidation loss: 18308.867188\tBest loss: 229.907806\tAccuracy: 79.16%\n",
      "11\tValidation loss: 13141.801758\tBest loss: 229.907806\tAccuracy: 76.43%\n",
      "12\tValidation loss: 6078.432129\tBest loss: 229.907806\tAccuracy: 84.91%\n",
      "13\tValidation loss: 13310.783203\tBest loss: 229.907806\tAccuracy: 77.40%\n",
      "14\tValidation loss: 4848.744629\tBest loss: 229.907806\tAccuracy: 90.66%\n",
      "15\tValidation loss: 3641.397217\tBest loss: 229.907806\tAccuracy: 91.20%\n",
      "16\tValidation loss: 223747.453125\tBest loss: 229.907806\tAccuracy: 54.77%\n",
      "17\tValidation loss: 7148.913086\tBest loss: 229.907806\tAccuracy: 93.16%\n",
      "18\tValidation loss: 8347.714844\tBest loss: 229.907806\tAccuracy: 90.70%\n",
      "19\tValidation loss: 4905.681641\tBest loss: 229.907806\tAccuracy: 89.99%\n",
      "20\tValidation loss: 3133.037109\tBest loss: 229.907806\tAccuracy: 94.53%\n",
      "21\tValidation loss: 3053.215332\tBest loss: 229.907806\tAccuracy: 94.18%\n",
      "22\tValidation loss: 18735.341797\tBest loss: 229.907806\tAccuracy: 80.26%\n",
      "23\tValidation loss: 5624.405762\tBest loss: 229.907806\tAccuracy: 92.89%\n",
      "24\tValidation loss: 2696.587646\tBest loss: 229.907806\tAccuracy: 95.27%\n",
      "25\tValidation loss: 1088266.125000\tBest loss: 229.907806\tAccuracy: 32.92%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.1, n_neurons=100, batch_size=50, total=  37.1s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.1, n_neurons=100, batch_size=50 \n",
      "0\tValidation loss: 39682.062500\tBest loss: 39682.062500\tAccuracy: 22.01%\n",
      "1\tValidation loss: 1146.159912\tBest loss: 1146.159912\tAccuracy: 34.01%\n",
      "2\tValidation loss: 427.924988\tBest loss: 427.924988\tAccuracy: 36.71%\n",
      "3\tValidation loss: 197.267761\tBest loss: 197.267761\tAccuracy: 63.29%\n",
      "4\tValidation loss: 252305.015625\tBest loss: 197.267761\tAccuracy: 21.89%\n",
      "5\tValidation loss: 14546.761719\tBest loss: 197.267761\tAccuracy: 34.28%\n",
      "6\tValidation loss: 5533.730957\tBest loss: 197.267761\tAccuracy: 42.10%\n",
      "7\tValidation loss: 7634.587891\tBest loss: 197.267761\tAccuracy: 46.99%\n",
      "8\tValidation loss: 55391.906250\tBest loss: 197.267761\tAccuracy: 40.15%\n",
      "9\tValidation loss: 1469933.000000\tBest loss: 197.267761\tAccuracy: 16.22%\n",
      "10\tValidation loss: 1118791.750000\tBest loss: 197.267761\tAccuracy: 18.73%\n",
      "11\tValidation loss: 64356.992188\tBest loss: 197.267761\tAccuracy: 48.20%\n",
      "12\tValidation loss: 78580.148438\tBest loss: 197.267761\tAccuracy: 58.41%\n",
      "13\tValidation loss: 41233.996094\tBest loss: 197.267761\tAccuracy: 70.09%\n",
      "14\tValidation loss: 32848.382812\tBest loss: 197.267761\tAccuracy: 78.46%\n",
      "15\tValidation loss: 19363.988281\tBest loss: 197.267761\tAccuracy: 80.61%\n",
      "16\tValidation loss: 35387.183594\tBest loss: 197.267761\tAccuracy: 75.53%\n",
      "17\tValidation loss: 23048.865234\tBest loss: 197.267761\tAccuracy: 81.08%\n",
      "18\tValidation loss: 25549.255859\tBest loss: 197.267761\tAccuracy: 72.44%\n",
      "19\tValidation loss: 15729.115234\tBest loss: 197.267761\tAccuracy: 73.30%\n",
      "20\tValidation loss: 12875.653320\tBest loss: 197.267761\tAccuracy: 77.01%\n",
      "21\tValidation loss: 10941.725586\tBest loss: 197.267761\tAccuracy: 88.66%\n",
      "22\tValidation loss: 19469.046875\tBest loss: 197.267761\tAccuracy: 89.56%\n",
      "23\tValidation loss: 83255.765625\tBest loss: 197.267761\tAccuracy: 73.03%\n",
      "24\tValidation loss: 20529.388672\tBest loss: 197.267761\tAccuracy: 88.31%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.1, n_neurons=100, batch_size=50, total=  30.5s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.1, n_neurons=100, batch_size=50 \n",
      "0\tValidation loss: 10191.410156\tBest loss: 10191.410156\tAccuracy: 31.67%\n",
      "1\tValidation loss: 166.457687\tBest loss: 166.457687\tAccuracy: 83.93%\n",
      "2\tValidation loss: 232.816727\tBest loss: 166.457687\tAccuracy: 84.44%\n",
      "3\tValidation loss: 79.806618\tBest loss: 79.806618\tAccuracy: 89.80%\n",
      "4\tValidation loss: 68.184593\tBest loss: 68.184593\tAccuracy: 92.92%\n",
      "5\tValidation loss: 306.302124\tBest loss: 68.184593\tAccuracy: 89.99%\n",
      "6\tValidation loss: 84.553741\tBest loss: 68.184593\tAccuracy: 92.26%\n",
      "7\tValidation loss: 120.121590\tBest loss: 68.184593\tAccuracy: 92.22%\n",
      "8\tValidation loss: 73.633492\tBest loss: 68.184593\tAccuracy: 92.92%\n",
      "9\tValidation loss: 60.912739\tBest loss: 60.912739\tAccuracy: 93.08%\n",
      "10\tValidation loss: 34.325939\tBest loss: 34.325939\tAccuracy: 95.00%\n",
      "11\tValidation loss: 59.417294\tBest loss: 34.325939\tAccuracy: 91.59%\n",
      "12\tValidation loss: 2283.323242\tBest loss: 34.325939\tAccuracy: 54.69%\n",
      "13\tValidation loss: 24199.154297\tBest loss: 34.325939\tAccuracy: 41.01%\n",
      "14\tValidation loss: 7042.683105\tBest loss: 34.325939\tAccuracy: 58.25%\n",
      "15\tValidation loss: 2077.548096\tBest loss: 34.325939\tAccuracy: 76.23%\n",
      "16\tValidation loss: 3846.573486\tBest loss: 34.325939\tAccuracy: 71.77%\n",
      "17\tValidation loss: 1430.465332\tBest loss: 34.325939\tAccuracy: 81.08%\n",
      "18\tValidation loss: 7195.795898\tBest loss: 34.325939\tAccuracy: 72.32%\n",
      "19\tValidation loss: 1407.502197\tBest loss: 34.325939\tAccuracy: 81.55%\n",
      "20\tValidation loss: 1098.882568\tBest loss: 34.325939\tAccuracy: 82.76%\n",
      "21\tValidation loss: 1205.299194\tBest loss: 34.325939\tAccuracy: 85.34%\n",
      "22\tValidation loss: 969.124634\tBest loss: 34.325939\tAccuracy: 90.38%\n",
      "23\tValidation loss: 62013.410156\tBest loss: 34.325939\tAccuracy: 41.32%\n",
      "24\tValidation loss: 1027.335083\tBest loss: 34.325939\tAccuracy: 87.53%\n",
      "25\tValidation loss: 3830.737793\tBest loss: 34.325939\tAccuracy: 81.20%\n",
      "26\tValidation loss: 1375684.375000\tBest loss: 34.325939\tAccuracy: 20.91%\n",
      "27\tValidation loss: 8789.862305\tBest loss: 34.325939\tAccuracy: 57.66%\n",
      "28\tValidation loss: 17033.117188\tBest loss: 34.325939\tAccuracy: 56.92%\n",
      "29\tValidation loss: 5299.544922\tBest loss: 34.325939\tAccuracy: 73.65%\n",
      "30\tValidation loss: 5465.178711\tBest loss: 34.325939\tAccuracy: 66.54%\n",
      "31\tValidation loss: 5247.061035\tBest loss: 34.325939\tAccuracy: 71.07%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0xb2dade2f0>, learning_rate=0.1, n_neurons=100, batch_size=50, total=  41.5s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.1, n_neurons=70, batch_size=100 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.508058\tBest loss: 0.508058\tAccuracy: 92.65%\n",
      "1\tValidation loss: 0.243707\tBest loss: 0.243707\tAccuracy: 94.02%\n",
      "2\tValidation loss: 0.537317\tBest loss: 0.243707\tAccuracy: 92.69%\n",
      "3\tValidation loss: 0.200299\tBest loss: 0.200299\tAccuracy: 94.29%\n",
      "4\tValidation loss: 0.186691\tBest loss: 0.186691\tAccuracy: 95.93%\n",
      "5\tValidation loss: 0.178332\tBest loss: 0.178332\tAccuracy: 96.72%\n",
      "6\tValidation loss: 0.193579\tBest loss: 0.178332\tAccuracy: 95.27%\n",
      "7\tValidation loss: 0.204459\tBest loss: 0.178332\tAccuracy: 94.68%\n",
      "8\tValidation loss: 0.156563\tBest loss: 0.156563\tAccuracy: 95.07%\n",
      "9\tValidation loss: 191305.468750\tBest loss: 0.156563\tAccuracy: 66.42%\n",
      "10\tValidation loss: 46120.761719\tBest loss: 0.156563\tAccuracy: 78.73%\n",
      "11\tValidation loss: 57695.832031\tBest loss: 0.156563\tAccuracy: 72.99%\n",
      "12\tValidation loss: 8230.427734\tBest loss: 0.156563\tAccuracy: 90.27%\n",
      "13\tValidation loss: 8071.119629\tBest loss: 0.156563\tAccuracy: 88.74%\n",
      "14\tValidation loss: 5395.086914\tBest loss: 0.156563\tAccuracy: 91.36%\n",
      "15\tValidation loss: 11441.649414\tBest loss: 0.156563\tAccuracy: 87.69%\n",
      "16\tValidation loss: 6911.943848\tBest loss: 0.156563\tAccuracy: 88.39%\n",
      "17\tValidation loss: 8542.449219\tBest loss: 0.156563\tAccuracy: 82.56%\n",
      "18\tValidation loss: 13908.499023\tBest loss: 0.156563\tAccuracy: 81.86%\n",
      "19\tValidation loss: 9783.634766\tBest loss: 0.156563\tAccuracy: 86.94%\n",
      "20\tValidation loss: 3169.693604\tBest loss: 0.156563\tAccuracy: 93.67%\n",
      "21\tValidation loss: 2715.212646\tBest loss: 0.156563\tAccuracy: 93.86%\n",
      "22\tValidation loss: 2738.821289\tBest loss: 0.156563\tAccuracy: 94.14%\n",
      "23\tValidation loss: 2755.424561\tBest loss: 0.156563\tAccuracy: 93.98%\n",
      "24\tValidation loss: 4410.112305\tBest loss: 0.156563\tAccuracy: 92.26%\n",
      "25\tValidation loss: 2850.161865\tBest loss: 0.156563\tAccuracy: 92.61%\n",
      "26\tValidation loss: 1966.035156\tBest loss: 0.156563\tAccuracy: 94.68%\n",
      "27\tValidation loss: 2726.714600\tBest loss: 0.156563\tAccuracy: 94.10%\n",
      "28\tValidation loss: 2160.864746\tBest loss: 0.156563\tAccuracy: 94.92%\n",
      "29\tValidation loss: 2597.587646\tBest loss: 0.156563\tAccuracy: 92.53%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.1, n_neurons=70, batch_size=100, total=  21.4s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.1, n_neurons=70, batch_size=100 \n",
      "0\tValidation loss: 7783.820312\tBest loss: 7783.820312\tAccuracy: 87.57%\n",
      "1\tValidation loss: 40.655186\tBest loss: 40.655186\tAccuracy: 95.04%\n",
      "2\tValidation loss: 24.513710\tBest loss: 24.513710\tAccuracy: 95.78%\n",
      "3\tValidation loss: 21.936117\tBest loss: 21.936117\tAccuracy: 96.40%\n",
      "4\tValidation loss: 11.362679\tBest loss: 11.362679\tAccuracy: 95.97%\n",
      "5\tValidation loss: 18.321484\tBest loss: 11.362679\tAccuracy: 95.15%\n",
      "6\tValidation loss: 7.223915\tBest loss: 7.223915\tAccuracy: 96.83%\n",
      "7\tValidation loss: 6.058515\tBest loss: 6.058515\tAccuracy: 96.79%\n",
      "8\tValidation loss: 7.076424\tBest loss: 6.058515\tAccuracy: 95.39%\n",
      "9\tValidation loss: 8.761731\tBest loss: 6.058515\tAccuracy: 95.93%\n",
      "10\tValidation loss: 12.244612\tBest loss: 6.058515\tAccuracy: 95.74%\n",
      "11\tValidation loss: 7.683410\tBest loss: 6.058515\tAccuracy: 96.68%\n",
      "12\tValidation loss: 8.699519\tBest loss: 6.058515\tAccuracy: 95.47%\n",
      "13\tValidation loss: 5.561969\tBest loss: 5.561969\tAccuracy: 96.91%\n",
      "14\tValidation loss: 16.241140\tBest loss: 5.561969\tAccuracy: 94.72%\n",
      "15\tValidation loss: 9.937673\tBest loss: 5.561969\tAccuracy: 96.40%\n",
      "16\tValidation loss: 4.340137\tBest loss: 4.340137\tAccuracy: 96.72%\n",
      "17\tValidation loss: 7.343180\tBest loss: 4.340137\tAccuracy: 93.67%\n",
      "18\tValidation loss: 8.043363\tBest loss: 4.340137\tAccuracy: 95.62%\n",
      "19\tValidation loss: 10025967.000000\tBest loss: 4.340137\tAccuracy: 30.26%\n",
      "20\tValidation loss: 173362.031250\tBest loss: 4.340137\tAccuracy: 71.74%\n",
      "21\tValidation loss: 69159.093750\tBest loss: 4.340137\tAccuracy: 85.50%\n",
      "22\tValidation loss: 35143.023438\tBest loss: 4.340137\tAccuracy: 87.61%\n",
      "23\tValidation loss: 45140.164062\tBest loss: 4.340137\tAccuracy: 85.14%\n",
      "24\tValidation loss: 20185.386719\tBest loss: 4.340137\tAccuracy: 90.54%\n",
      "25\tValidation loss: 16416.906250\tBest loss: 4.340137\tAccuracy: 92.73%\n",
      "26\tValidation loss: 17183.953125\tBest loss: 4.340137\tAccuracy: 93.98%\n",
      "27\tValidation loss: 21328.291016\tBest loss: 4.340137\tAccuracy: 93.39%\n",
      "28\tValidation loss: 13781.269531\tBest loss: 4.340137\tAccuracy: 93.35%\n",
      "29\tValidation loss: 25019.054688\tBest loss: 4.340137\tAccuracy: 92.61%\n",
      "30\tValidation loss: 18619.267578\tBest loss: 4.340137\tAccuracy: 92.96%\n",
      "31\tValidation loss: 17200.937500\tBest loss: 4.340137\tAccuracy: 94.33%\n",
      "32\tValidation loss: 23575.060547\tBest loss: 4.340137\tAccuracy: 90.93%\n",
      "33\tValidation loss: 15143.224609\tBest loss: 4.340137\tAccuracy: 93.16%\n",
      "34\tValidation loss: 21638.843750\tBest loss: 4.340137\tAccuracy: 93.04%\n",
      "35\tValidation loss: 19465.332031\tBest loss: 4.340137\tAccuracy: 94.02%\n",
      "36\tValidation loss: 21185.544922\tBest loss: 4.340137\tAccuracy: 92.49%\n",
      "37\tValidation loss: 832443.812500\tBest loss: 4.340137\tAccuracy: 63.92%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.1, n_neurons=70, batch_size=100, total=  28.8s\n",
      "[CV] activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.1, n_neurons=70, batch_size=100 \n",
      "0\tValidation loss: 0.916623\tBest loss: 0.916623\tAccuracy: 83.03%\n",
      "1\tValidation loss: 0.188519\tBest loss: 0.188519\tAccuracy: 95.50%\n",
      "2\tValidation loss: 0.225108\tBest loss: 0.188519\tAccuracy: 95.27%\n",
      "3\tValidation loss: 1799.395020\tBest loss: 0.188519\tAccuracy: 32.60%\n",
      "4\tValidation loss: 13505.201172\tBest loss: 0.188519\tAccuracy: 79.24%\n",
      "5\tValidation loss: 9794.957031\tBest loss: 0.188519\tAccuracy: 88.70%\n",
      "6\tValidation loss: 3526.398682\tBest loss: 0.188519\tAccuracy: 92.53%\n",
      "7\tValidation loss: 2584.461426\tBest loss: 0.188519\tAccuracy: 93.71%\n",
      "8\tValidation loss: 1805.060059\tBest loss: 0.188519\tAccuracy: 93.86%\n",
      "9\tValidation loss: 1505.719482\tBest loss: 0.188519\tAccuracy: 95.39%\n",
      "10\tValidation loss: 1730.880981\tBest loss: 0.188519\tAccuracy: 94.33%\n",
      "11\tValidation loss: 850.009277\tBest loss: 0.188519\tAccuracy: 95.97%\n",
      "12\tValidation loss: 3611.578857\tBest loss: 0.188519\tAccuracy: 95.78%\n",
      "13\tValidation loss: 1678.410889\tBest loss: 0.188519\tAccuracy: 93.94%\n",
      "14\tValidation loss: 1797.485352\tBest loss: 0.188519\tAccuracy: 93.75%\n",
      "15\tValidation loss: 912.130737\tBest loss: 0.188519\tAccuracy: 96.48%\n",
      "16\tValidation loss: 685.293274\tBest loss: 0.188519\tAccuracy: 96.64%\n",
      "17\tValidation loss: 1030.452637\tBest loss: 0.188519\tAccuracy: 94.92%\n",
      "18\tValidation loss: 718.141602\tBest loss: 0.188519\tAccuracy: 96.52%\n",
      "19\tValidation loss: 594.546143\tBest loss: 0.188519\tAccuracy: 96.68%\n",
      "20\tValidation loss: 1925.733032\tBest loss: 0.188519\tAccuracy: 95.97%\n",
      "21\tValidation loss: 810.327209\tBest loss: 0.188519\tAccuracy: 95.93%\n",
      "22\tValidation loss: 798.235474\tBest loss: 0.188519\tAccuracy: 96.60%\n",
      "Early stopping!\n",
      "[CV]  activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x110dd0158>, learning_rate=0.1, n_neurons=70, batch_size=100, total=  17.7s\n",
      "[CV] activation=<function elu at 0x129284e18>, learning_rate=0.05, n_neurons=100, batch_size=10 \n",
      "0\tValidation loss: 1.911381\tBest loss: 1.911381\tAccuracy: 18.73%\n",
      "1\tValidation loss: 2.296635\tBest loss: 1.911381\tAccuracy: 18.73%\n",
      "2\tValidation loss: 2.156011\tBest loss: 1.911381\tAccuracy: 19.27%\n",
      "3\tValidation loss: 2.073931\tBest loss: 1.911381\tAccuracy: 18.73%\n",
      "4\tValidation loss: 2.013968\tBest loss: 1.911381\tAccuracy: 19.27%\n",
      "5\tValidation loss: 1.684835\tBest loss: 1.684835\tAccuracy: 20.91%\n",
      "6\tValidation loss: 2.098236\tBest loss: 1.684835\tAccuracy: 18.73%\n",
      "7\tValidation loss: 1.991185\tBest loss: 1.684835\tAccuracy: 19.08%\n",
      "8\tValidation loss: 1.886133\tBest loss: 1.684835\tAccuracy: 19.08%\n",
      "9\tValidation loss: 2.482883\tBest loss: 1.684835\tAccuracy: 19.27%\n",
      "10\tValidation loss: 1.726859\tBest loss: 1.684835\tAccuracy: 20.91%\n",
      "11\tValidation loss: 1.677544\tBest loss: 1.677544\tAccuracy: 22.01%\n",
      "12\tValidation loss: 1.999842\tBest loss: 1.677544\tAccuracy: 19.08%\n",
      "13\tValidation loss: 1.756565\tBest loss: 1.677544\tAccuracy: 19.08%\n",
      "14\tValidation loss: 2.995764\tBest loss: 1.677544\tAccuracy: 20.91%\n",
      "15\tValidation loss: 1.696040\tBest loss: 1.677544\tAccuracy: 19.27%\n",
      "16\tValidation loss: 2.644184\tBest loss: 1.677544\tAccuracy: 20.91%\n",
      "17\tValidation loss: 2.019514\tBest loss: 1.677544\tAccuracy: 20.91%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\tValidation loss: 2.568100\tBest loss: 1.677544\tAccuracy: 22.01%\n",
      "19\tValidation loss: 2.833769\tBest loss: 1.677544\tAccuracy: 22.01%\n",
      "20\tValidation loss: 2.283542\tBest loss: 1.677544\tAccuracy: 22.01%\n",
      "21\tValidation loss: 2.377192\tBest loss: 1.677544\tAccuracy: 20.91%\n",
      "22\tValidation loss: 1.983339\tBest loss: 1.677544\tAccuracy: 18.73%\n",
      "23\tValidation loss: 2.143950\tBest loss: 1.677544\tAccuracy: 19.27%\n",
      "24\tValidation loss: 2.032364\tBest loss: 1.677544\tAccuracy: 20.91%\n",
      "25\tValidation loss: 1.827993\tBest loss: 1.677544\tAccuracy: 20.91%\n",
      "26\tValidation loss: 1.893211\tBest loss: 1.677544\tAccuracy: 19.08%\n",
      "27\tValidation loss: 2.163734\tBest loss: 1.677544\tAccuracy: 22.01%\n",
      "28\tValidation loss: 2.621584\tBest loss: 1.677544\tAccuracy: 19.08%\n",
      "29\tValidation loss: 2.281024\tBest loss: 1.677544\tAccuracy: 20.91%\n",
      "30\tValidation loss: 1.913920\tBest loss: 1.677544\tAccuracy: 19.27%\n",
      "31\tValidation loss: 2.141618\tBest loss: 1.677544\tAccuracy: 19.27%\n",
      "32\tValidation loss: 2.017617\tBest loss: 1.677544\tAccuracy: 19.27%\n",
      "Early stopping!\n",
      "[CV]  activation=<function elu at 0x129284e18>, learning_rate=0.05, n_neurons=100, batch_size=10, total= 2.0min\n",
      "[CV] activation=<function elu at 0x129284e18>, learning_rate=0.05, n_neurons=100, batch_size=10 \n",
      "0\tValidation loss: 1.781603\tBest loss: 1.781603\tAccuracy: 19.27%\n",
      "1\tValidation loss: 1.947553\tBest loss: 1.781603\tAccuracy: 22.01%\n",
      "2\tValidation loss: 2.197367\tBest loss: 1.781603\tAccuracy: 22.01%\n",
      "3\tValidation loss: 2.181736\tBest loss: 1.781603\tAccuracy: 22.01%\n",
      "4\tValidation loss: 2.006840\tBest loss: 1.781603\tAccuracy: 20.91%\n",
      "5\tValidation loss: 2.126854\tBest loss: 1.781603\tAccuracy: 19.27%\n",
      "6\tValidation loss: 1.798395\tBest loss: 1.781603\tAccuracy: 19.27%\n",
      "7\tValidation loss: 2.028978\tBest loss: 1.781603\tAccuracy: 19.08%\n",
      "8\tValidation loss: 2.357072\tBest loss: 1.781603\tAccuracy: 18.73%\n",
      "9\tValidation loss: 1.772412\tBest loss: 1.772412\tAccuracy: 19.27%\n",
      "10\tValidation loss: 2.146404\tBest loss: 1.772412\tAccuracy: 19.08%\n",
      "11\tValidation loss: 2.741696\tBest loss: 1.772412\tAccuracy: 22.01%\n",
      "12\tValidation loss: 2.146072\tBest loss: 1.772412\tAccuracy: 19.27%\n",
      "13\tValidation loss: 2.504707\tBest loss: 1.772412\tAccuracy: 19.27%\n",
      "14\tValidation loss: 2.295662\tBest loss: 1.772412\tAccuracy: 19.27%\n",
      "15\tValidation loss: 2.064730\tBest loss: 1.772412\tAccuracy: 18.73%\n",
      "16\tValidation loss: 1.657115\tBest loss: 1.657115\tAccuracy: 19.08%\n",
      "17\tValidation loss: 1.927751\tBest loss: 1.657115\tAccuracy: 18.73%\n",
      "18\tValidation loss: 1.869300\tBest loss: 1.657115\tAccuracy: 19.08%\n",
      "19\tValidation loss: 2.003773\tBest loss: 1.657115\tAccuracy: 18.73%\n",
      "20\tValidation loss: 2.406461\tBest loss: 1.657115\tAccuracy: 19.27%\n",
      "21\tValidation loss: 1.857644\tBest loss: 1.657115\tAccuracy: 19.27%\n",
      "22\tValidation loss: 2.096558\tBest loss: 1.657115\tAccuracy: 19.27%\n",
      "23\tValidation loss: 2.520514\tBest loss: 1.657115\tAccuracy: 22.01%\n",
      "24\tValidation loss: 1.893648\tBest loss: 1.657115\tAccuracy: 19.08%\n",
      "25\tValidation loss: 2.078130\tBest loss: 1.657115\tAccuracy: 22.01%\n",
      "26\tValidation loss: 2.804221\tBest loss: 1.657115\tAccuracy: 19.08%\n",
      "27\tValidation loss: 2.394260\tBest loss: 1.657115\tAccuracy: 20.91%\n",
      "28\tValidation loss: 1.977029\tBest loss: 1.657115\tAccuracy: 20.91%\n",
      "29\tValidation loss: 1.721668\tBest loss: 1.657115\tAccuracy: 19.27%\n",
      "30\tValidation loss: 2.202598\tBest loss: 1.657115\tAccuracy: 19.08%\n",
      "31\tValidation loss: 2.605292\tBest loss: 1.657115\tAccuracy: 22.01%\n",
      "32\tValidation loss: 2.066093\tBest loss: 1.657115\tAccuracy: 18.73%\n",
      "33\tValidation loss: 2.527793\tBest loss: 1.657115\tAccuracy: 19.08%\n",
      "34\tValidation loss: 2.730593\tBest loss: 1.657115\tAccuracy: 18.73%\n",
      "35\tValidation loss: 2.582781\tBest loss: 1.657115\tAccuracy: 22.01%\n",
      "36\tValidation loss: 2.845667\tBest loss: 1.657115\tAccuracy: 19.27%\n",
      "37\tValidation loss: 1.998369\tBest loss: 1.657115\tAccuracy: 19.27%\n",
      "Early stopping!\n",
      "[CV]  activation=<function elu at 0x129284e18>, learning_rate=0.05, n_neurons=100, batch_size=10, total= 2.3min\n",
      "[CV] activation=<function elu at 0x129284e18>, learning_rate=0.05, n_neurons=100, batch_size=10 \n",
      "0\tValidation loss: 1.855537\tBest loss: 1.855537\tAccuracy: 22.01%\n",
      "1\tValidation loss: 1.975352\tBest loss: 1.855537\tAccuracy: 19.27%\n",
      "2\tValidation loss: 2.228128\tBest loss: 1.855537\tAccuracy: 19.08%\n",
      "3\tValidation loss: 2.120886\tBest loss: 1.855537\tAccuracy: 19.27%\n",
      "4\tValidation loss: 3.102788\tBest loss: 1.855537\tAccuracy: 20.91%\n",
      "5\tValidation loss: 2.470538\tBest loss: 1.855537\tAccuracy: 20.91%\n",
      "6\tValidation loss: 2.003429\tBest loss: 1.855537\tAccuracy: 22.01%\n",
      "7\tValidation loss: 1.762941\tBest loss: 1.762941\tAccuracy: 22.01%\n",
      "8\tValidation loss: 1.701701\tBest loss: 1.701701\tAccuracy: 19.27%\n",
      "9\tValidation loss: 2.334348\tBest loss: 1.701701\tAccuracy: 22.01%\n",
      "10\tValidation loss: 1.855003\tBest loss: 1.701701\tAccuracy: 18.73%\n",
      "11\tValidation loss: 1.841025\tBest loss: 1.701701\tAccuracy: 19.27%\n",
      "12\tValidation loss: 2.090956\tBest loss: 1.701701\tAccuracy: 20.91%\n",
      "13\tValidation loss: 3.280327\tBest loss: 1.701701\tAccuracy: 19.08%\n",
      "14\tValidation loss: 2.147481\tBest loss: 1.701701\tAccuracy: 19.08%\n",
      "15\tValidation loss: 1.816309\tBest loss: 1.701701\tAccuracy: 19.27%\n",
      "16\tValidation loss: 1.802172\tBest loss: 1.701701\tAccuracy: 18.73%\n",
      "17\tValidation loss: 2.435385\tBest loss: 1.701701\tAccuracy: 20.91%\n",
      "18\tValidation loss: 1.970497\tBest loss: 1.701701\tAccuracy: 19.27%\n",
      "19\tValidation loss: 2.368726\tBest loss: 1.701701\tAccuracy: 22.01%\n",
      "20\tValidation loss: 2.211025\tBest loss: 1.701701\tAccuracy: 19.27%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def leaky_relu(alpha=0.01):\n",
    "    def parametrized_leaky_relu(z, name=None):\n",
    "        return tf.maximum(alpha * z, z, name=name)\n",
    "    return parametrized_leaky_relu\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n",
    "    \"batch_size\": [10, 50, 100, 500],\n",
    "    \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n",
    "    \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "    # you could also try exploring different numbers of hidden layers, different optimizers, etc.\n",
    "    #\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    #\"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],\n",
    "}\n",
    "\n",
    "fit_params={\"X_valid\": X_valid1, \"y_valid\": y_valid1, \"n_epochs\": 1000}\n",
    "rnd_search = RandomizedSearchCV(\n",
    "    DNNClassifier(random_state=42),\n",
    "    param_distribs,\n",
    "    n_iter=50,\n",
    "    fit_params=fit_params,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "rnd_search.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnd_search.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful! Tuning the hyperparameters got us up to 99.32% accuracy! It may not sound like a great improvement to go from 98.05% to 99.32% accuracy, but consider the error rate: it went from roughly 2% to 0.7%. That's a 65% reduction of the number of errors this model will produce!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search.best_estimator_.save(\"./my_best_mnist_model_0_to_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise: Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the best model found, once again, to see how fast it converges (alternatively, you could tweak the code above to make it write summaries for TensorBoard, so you can visualize the learning curve):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_clf = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                        n_neurons=140, random_state=42)\n",
    "dnn_clf.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we do indeed get 99.32% accuracy on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dnn_clf.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, now let's use the exact same model, but this time with batch normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_clf_bn = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                           n_neurons=90, random_state=42, batch_norm_momentum=0.95)\n",
    "dnn_clf_bn.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dnn_clf_bn.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, batch normalization did not improve accuracy. Let's see if we can find a good set of hyperparameters that will work well with batch normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n",
    "    \"batch_size\": [10, 50, 100, 500],\n",
    "    \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n",
    "    \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "    # you could also try exploring different numbers of hidden layers, different optimizers, etc.\n",
    "    #\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    #\"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],\n",
    "    \"batch_norm_momentum\": [0.9, 0.95, 0.98, 0.99, 0.999],\n",
    "}\n",
    "\n",
    "rnd_search_bn = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n",
    "                                   fit_params={\"X_valid\": X_valid1, \"y_valid\": y_valid1, \"n_epochs\": 1000},\n",
    "                                   random_state=42, verbose=2)\n",
    "rnd_search_bn.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_bn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnd_search_bn.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly better than earlier: 99.4% vs 99.3%. Let's see if dropout can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise: is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to the best model we trained earlier and see how it performs on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dnn_clf.predict(X_train1)\n",
    "accuracy_score(y_train1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs significantly better on the training set than on the test set (99.91% vs 99.32%), which means it is overfitting the training set. A bit of regularization may help. Let's try adding dropout with a 50% dropout rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_clf_dropout = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                                n_neurons=90, random_state=42,\n",
    "                                dropout_rate=0.5)\n",
    "dnn_clf_dropout.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dnn_clf_dropout.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are out of luck, dropout does not seem to help either. Let's try tuning the hyperparameters, perhaps we can squeeze a bit more performance out of this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n",
    "    \"batch_size\": [10, 50, 100, 500],\n",
    "    \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n",
    "    \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "    # you could also try exploring different numbers of hidden layers, different optimizers, etc.\n",
    "    #\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    #\"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],\n",
    "    \"dropout_rate\": [0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "}\n",
    "\n",
    "rnd_search_dropout = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n",
    "                                        fit_params={\"X_valid\": X_valid1, \"y_valid\": y_valid1, \"n_epochs\": 1000},\n",
    "                                        random_state=42, verbose=2)\n",
    "rnd_search_dropout.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_dropout.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh well, dropout did not improve the model. Better luck next time! :)\n",
    "\n",
    "But that's okay, we have ourselves a nice DNN that achieves 99.40% accuracy on the test set using Batch Normalization, or 99.32% without BN. Let's see if some of this expertise on digits 0 to 4 can be transferred to the task of classifying digits 5 to 9. For the sake of simplicity we will reuse the DNN without BN, since it is almost as good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
