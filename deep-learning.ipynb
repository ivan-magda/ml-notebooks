{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradients Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution. This is called the **vanishing gradients** problem.\n",
    "\n",
    "In some cases, the opposite can happen: the gradients can grow bigger and bigger, so many layers get insanely large weight updates and the algorithm diverges. This is the **exploding gradients** problem, which is mostly encountered in recurrent neural networks. More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEMCAYAAADEXsFmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3Xd4FEUfwPHvpFeBQAjSRWoQAghIJzQFKRFCkY4NAdGXF6wgSFFQpNpQVIyCFClRiiB5gUgvAQlICyVAQkIJEEJ6uXn/2CPkkgtpl9wlmc/z7JPs7tzO7zaX+22ZmRVSShRFURTFytwBKIqiKJZBJQRFURQFUAlBURRF0VMJQVEURQFUQlAURVH0VEJQFEVRAJUQSh0hRKAQ4itzxwG5i0UI8a8QYnoRhZSxXj8hxOYiqMdbCCGFEBWKoK7RQoirQgidOfZpplhGCSFizRmDkpVQ/RBKDiGEOzADeB54HIgG/gU+lVIG6Mu4ASlSyvtmC1QvN7EIIf4F1kkppxdSDN7ALsBdShmVYXkZtP+PaBPWdRn4Sko5L8MyO8ANuCEL8Z9RCFEOuAlMBNYB96WURfKFLISQwAAp5boMyxwBVynlzaKIQckdG3MHoJjUesAJeAW4AFQEOgLlHxSQUt4xT2hZWVIsmUkp7xVRPcnA9SKoqgba//tmKWVkEdT3SFLKBCDB3HEomUgp1VQCJqAsIIGuOZQLRDtKfTDvAWxE++e8AryEdlYxPUMZCYwF/gDigRCgE1AV+AuIA44DzTLV1Q84CSQBYcAU9Gel2cRSUV/Hg1hezhyLkffzpP411/VxHAN6ZSpjB8zWbzMJuAS8BdTUv7eMk5/+NX5oX54ArwM3AJtM210J/JGbOPTv1aAu/XJv/XyFPOy3y8CHwHdADBAOvPOIfTTKyPusCUwH/jVSNjbD/HT93+BF4CJwH/g9Y7z6ciMzxHwjw368nKney8bqybCfLwDJ+p+vZVovgdHAWv0+vgQMM/f/Xkma1D2EkiNWP/URQjjk4XU/ox09dgZ8gGH6+cw+BFYDXkAQsAr4EfgGaApEoH2JAiCEeBrtH3cD0Ah4H/gAGP+IWPyA2kBX4AVgBNoX16O4AFuBbvrY1gMbhBD1M73HEWiXSxqgnUFFo33Z+urLNES7zPYfI3X8hpZwu2Z4f85o+2tFLuPoh/bFPVNfz+PG3kwe9tt/0b6AmwGfAXOFEK2NbRNYA3TX/95SX3dYNmWNqQkMAvoCz6L9vT/JEPPraMnpJ6Ax2iXLU/rVLfQ/X9PX+2DegBCiL/AVsAh4ClgMfCOE6J2p6DS0xOulf1/LhBDGPq9Kfpg7I6nJdBPal9sdIBE4AMwDnslUJhD9UTlQD+2oq1WG9dWANLKeIczJMP+UftnEDMu8yXCkC/wK7MxU93QgPJtY6upf3zbD+hqZY8nlfjgIfKj/vY5+u92zKWsQd4blfujPEPTz/sDyDPPDgHuAQ27i0M9fBt5+VP253G+XgVWZypzPWJeRWJrr66mZabu5OUNIBMpkWDYFuJBhPhztPlV2dUugfw717AOWGfkb7H3E59AG7YxVnSWYaFJnCCWIlHI9UBnojXa02gY4KISYnM1L6gM6tCP+B9sIQzvaz+xEht9v6H+eNLKsov5nA7R/8oz2AlWEEI8Z2X4DfSyHM8RyJZtY0gkhnIUQc4UQp4UQd/UtV5oD1fVFmuq3u+tR28mFFcALQggn/fxQtJvdibmMI7dyu99OZCoTwcN9b2pXpOE9lfS6hBAVgSrAjgLWkd379sy0LP19SylTgVsU3vsudVRCKGGklIlSygAp5UwpZRu0yzrT9a1ZMhN52HRKxmoesezBZ0pkWJYlzALGktE8YAAwFe0GehO0pPLg/eZ3u5ltBlIBH/2XYFceXi7KTRy5ldv9lmJkXV7/n3Vk3T+2Rso9qi5T7d8H281pmSnet5INtSNLvtNop9bG7iucQfsMPP1ggRCiKtpZhinqbZdpWTu0Sx/Gmpk+iCX9GrMQonouYmkH/CKlXC+lPIF2+eLJDOuP6bfbKZvXJ+t/Wj+qEillElpzzaFo19OvA3/nIY4HdT2yHvK+3wriFuAhhMj4pd4kLxuQUt4ArgFdHlEshZzf9xmMv+/TeYlHKRiVEEoIIUR5IcROIcQwIURjIcQTQogBwLvADillTObXSCnPobUS+lYI0UoI0QTtxmA82R+l5tZ8oKMQYroQoq4QYigwCZhrrLA+lm3Ad0KI1vpY/Mi5aWII0FcI0UwI0QjtqD09+Ukpz6PdFP5BCOGr3y/thRDD9UWuoL3XnkIIdyGEyyPqWgE8B4wBVkopdbmNQ+8y0F4IUeURHdHytN8KKBCtD8RkIcSTQohXgP752M4nwAQhxH/1MTcRQkzKsP4y0EUIUUnfH8KYz4HhQog3hBB1hBBvoiXfwnjfSjZUQig5YtFuYv4H7cj1FFpTy5VoR7TZGYV2NBuI1vz0V7QOTIkFCUZKeQztEoov+s5x+ulRPZNHAaHATmCTPvbLOVQ1UR/vHrT7Jgf1v2c0Qr+tL4CzaImmjD7Oa8BHaF9qN3KIbzfa0bAnhpeLchvHNLSb9hfRjs6zyOd+yxcp5Rm05sSj0a7Nd0P7zOR1O0uAN9BaEv2LltgbZigyCe0MLQz4J5tt/A68idZ66jTa53iclHJTXuNR8k/1VFYM6I9cI4DB+pvUiqKUEqqnciknhOgMuKK1GKqIdqQchXaUpyhKKWKyS0ZCiPFCiCAhRJIQwu8R5UYKIY4KIWKEEOH6pnoqMZmPLfAxWkLYhHbNvoOUMs6sUSmKUuRMdslICNEPrRnbc4CjlHJUNuXGol1nPAS4o123Xiul/NQkgSiKoij5YrIjcynlBgAhRHO0MW6yK7ckw+w1IcSvZN8kUFEURSkilnCppgMPxz3JQggxGq0VBI6Ojk9Xq1atqOLKlk6nw8pKNdACtS8AwsLCkFJSvXpeOyWXTEXxmdBJHUm6JBytHQu1noKylP+PkJCQKCmle07lzJoQhBAvoXXvfzW7MlLKpcBSgObNm8ugoKDsihaZwMBAvL29zR2GRVD7Ary9vYmOjub48ePmDsUiFPZnIjktmR6/9uDwtcP8859/KO9UPucXmYml/H8IIa7kppzZEoIQ4gW09tVdZYYHkyiKomRHSsnrm19nZ+hO/Hz8LDoZFEdmSQhCiO7A90BPKeXJnMoriqIAzN4zG7/jfkzrMI2RTUaaO5wSx2QJQd901AZtzBJr/Zj8qfoRCTOW64zWG7avlPJw1i0piqJktfvKbj7c9SHDGg9juvd0c4dTIpnybseHaG3Y30cbKz4B+FAIUV0IEasfqAy00SDLAH/ql8cKIbaaMA5FUUqgdtXb8W3Pb/mh9w8YjsenmIopm51OR3uYhjEuGcqpJqaKouTahTsXsLWypUbZGrze/HVzh1OiWUKzU0VRFKOi4qPo8WsPHGwcCB4TjJUwfxPOkkwlBEVRLFJiaiIvrH6BsHth7By5UyWDIqASgqIoFkcndbz0x0vsC9vHb/1/o021NuYOqVRQKVdRFIvz9eGvWf3vaj7t8ikDGg4wdzilhjpDUBTF4oxqMgprK2vGNh9r7lBKFXWGoCiKxTgacZS45Dhc7V0Z12Kcal5axFRCUBTFIpy+dZouv3RhzJYx5g6l1FIJQVEUs7see53nf30eR1tHPun8ibnDKbXUPQRFUcwqPiWePqv6cCv+FrtH7aZ6GTWMuLmohKAoilm9tfUtgiKC+P3F33m68tPmDqdUUwlBURSzmtx+Mh1rdKRPvT7mDqXUUwlBURSzOBR+iJZVWlKrXC1qlatl7nAU1E1lRVHMYEvIFtosa8PiQ4vNHYqSgUoIiqIUqX8i/2HQukE0qdSEV5tl+/RcxQxUQlAUpciEx4TTa1Uv3Bzd2DR4Ey52Ljm/SCky6h6CoihFQid19FvTj/tJ99n38j4qu1Y2d0hKJiohKIpSJKyEFZ91/YwUXQqNPBqZOxzFCJUQFEUpVFJKjkYepXnl5nR6Qj0w0ZKpewiKohSq+Qfm0+L7FgReDjR3KEoOVEJQFKXQrD+9nncC3mFgw4F0qNHB3OEoOVAJQVGUQnEo/BDD/IfRumpr/Hz81CMwiwGT/oWEEOOFEEFCiCQhhF8OZf8rhLguhLgnhFgmhLA3ZSyKopjPnYQ79Fndh8qulfnjxT9wtHU0d0hKLpg6ZUcAHwPLHlVICPEc8D7QBagJ1AJmmDgWRVHMxM3RjVmdZvHnkD9xd3Y3dzhKLpm0lZGUcgOAEKI5UPURRUcCP0opT+nLzwJ+RUsS2Tp37hze3t4GywYOHMi4ceOIj4/n+eefz/KaUaNGMWrUKKKioujfv3+W9WPHjmXQoEGEhYUxfPjwLOsnTZpE7969OXfuHK+//joA0dHRlC1bFoAPP/yQrl27cvz4cSZMmJDl9bNnz6ZNmzbs37+fyZMnZ1m/aNEimjRpwv/+9z8+/vjjLOu/++476tWrx6ZNm5g/f36W9cuXL6datWqsWbOGJUuWZFm/bt06KlSogJ+fH35+flnW//nnnzg5OfHNN9/w22+/ZVkfGBgIwLx589i8ebPBOkdHR9577z0AZs2axY4dOwzWly9fnvXr1wPwwQcfcODAAYP1VatWZcWKFQBMmDCB48ePG6yvW7cuS5cuBWD06NGEhIQYrG/SpAmLFi0CYNiwYYSHhxusb926NXPmzAHA19eX27dvG6zv0qULU6dOBaBHjx4kJCQYrO/Vqxdvv/02QJbPHTz87Ol0Oi5cuJClTGF89jKyxM+eTui4mXqTStaVCv2zt3XrVqB0f/by+72XHXM1O20I/JFhPhjwEEKUl1Ia7DkhxGhgNICtrS3R0dEGGwoJCSEwMJDExMQs6wDOnj1LYGAg9+7dM7r+1KlTBAYGcvPmTaPrT548iaurK1evXk1fn5aWlv57cHAwNjY2XLhwwejrjx07RnJyMv/++6/R9UFBQURHRxMcHGx0/aFDh4iMjOTkyZNG1x84cICLFy9y6tQpo+v37dtHmTJlOHv2rNH1u3fvxsHBgZCQEKPrH/xTXrx4Mcv6hIQEYmNjCQwMJDQ0NMt6nU6X/vqM++8BW1vb9PXh4eFZ1kdERKSvj4iIyLI+PDw8ff2NGzeyrL969Wr6+lu3bhETE2OwPjQ0NH39nTt3SEpKMlh/8eLF9PXG9s2Dz150dDRSyixlCuOzl5GlffYkkrBmYdyrfA/rAOtC/+w9WG/Jn73Y2FiTfvZ0Oht0OmeCgm7xyy+HiYlJ5dq1muh0Duh09uh0DkjpwMqVZTl06CL37iVz5sxw4G9yQ0gpc1UwL4QQHwNVpZSjsll/EXhDSrlNP28LJANPSCkvZ7fd5s2by6CgIJPHm1eBgYFGs3ZppPaFdgQXHR2d5SiztPl498dM3TWVUTVG8dOon8wdjkXI+P+RmgrR0XD7Nty583C6fRvu3oX79yEmRvuZ3e/JyfmNRByVUjbPqZS5zhBigccyzD/4/b4ZYlEUpYBWnlzJ1F1TGeE1ghFlRpg7nCIhpfYlff161unGDe3nxYvNSEnRvvTv3St4nTY28Nhj4OwMTk7a5OioTQ9+z/zT0RGmTMnl9gseYr6cAryABxcOvYAbmS8XKYpi+f6J/IeX/ngJ75refN/7e/bv2W/ukEwiORnCw+Hq1YfTlSuG8/HxOW3l4XGvEFCuHLi5aVP58g9/L1dO+6J3dc36M+Pv9vbadnIXfzIrV65k5MiR5kkIQggb/TatAWshhAOQKqVMzVT0F8BPCPErEAl8CPiZMhZFUYrGUxWf4oN2H/DWM29hZ21n7nDyJDUVLl+G8+chJESbHvx+9ap2FvAozs5QqZLh5OHx8PewsKM8++zTuLlB2bJgVURdMWJjY+nevTv79u2jR48euX6dqc8QPgQ+yjA/DJghhFgGnAY8pZRXpZTbhBBzgV2AI7A+0+sURbFwUfFRpOnS8HDxYLr3dHOH80hSakf7J08aTmfPZn9d3soKqlaF6tUfTjVqGM6XKfPoegMD71O7tunfz6PcuXMHb29vzp8/j6urKxEREbl+rambnU4Hpmez2mDgcynlAmCBKetXFKVoJKYm4rPah7sJdzkx9gQ2VpYzTqaUEBYGhw8/nIKDtRu6xlSrBnXrQp06D3/WqQNPPAF2xeuEh4iICNq1a0d4eDgpKSnY2dlx7dq1XL/ecv6KiqIUCzqpY9Tvo9gftp+1A9aaPRkkJsLBg7Bvn/blf+iQdlM3swoVoFEjw6lhQ3ApIc/ouXDhAu3atSMqKoq0tDQAUlJSzHeGoChKyffhzg9Zc2oNn3X9jP6eue/0ZCrJydqX/q5d2nTgAGRqxo+bG7Rs+XBq1ky7pp/bG7LFTXBwMN7e3ty7d4+MXQkSEhIICwvL9XZUQlAUJddWnVzFnL1zGN1sNO+0eafI6g0Jgc2bYetW7UwgU8deGjeGjh2hVSstATz5ZMn98s9s3759dO/endjYWKPrL1y4kOttqYSgKEquPfvks7zf9n1mdpqJKMRv3JQU2L1bSwKbN0Pm77SGDaFTJ/D21hJBhQqFFopF27JlCwMHDiT+Ee1fr1y5kuvtqYSgKEqOLkdf5nGXxynvVJ45XecUSh2pqdoloN9+gw0btF68D7i5QY8e0LMndOkCFSsWSgjFyooVKxg9enSWcZAyi4yMzPU2VUJQFOWRIu9H0tGvI22qtWGV7yqTbltK2LsXVqzQkkBU1MN1DRqAjw/06gXPPKP10lU03333HRMmTCAxMTHHslEZd2oO1C5WFCVbcclx9F7Vm9vxt016zyA8HH75BX76yfByUL16MGgQDByoXRZSjIuJicHW1hZbW1vu33/0iD/6M4hcXd9TjzBSFMWoNF0aQzcM5Z/r/7C6/2qaPd6sYNtLg99/1y791Kihja9z4QJUrgzvvw8nTsCZMzBjhkoGOXnnnXeIiopi+fLltG7dGgcHh2zL6tfZ5ma76gxBURSjpu6ayh/n/uCL7l/Qq26vfG/n7l1Ytgy++kobJgK0Dl8+PvDSS/Dss2BtbZqYSxM7Ozt8fHwIDg7m2LFj2Zaz0a615aqLnUoIiqIYNbTRUJxtnXnzmTfz9fpz52DRIu3S0INGMLVqwfjxMGKENribUjBSSpYsWWLwPAV7e3s8PT05f/48QogHl4zUGYKiKHl36e4lnij7BA0rNqRhxbxfu7l40Zlvv9VaCz3oI9WtG7z1lna5SJ0NmM6+ffuy9D8QQuDv70+lSpXYvHkz3333HQEBAZkHGDVK3UNQFCXdschjNF7SmAUH8j7M2OHD2mWgV19twZo1WqugV1+FU6dg+3attZBKBqa1ZMkS4uLiDJY1btyYGjVqYG9vj6+vL9u3b4dcPmtGnSEoigJA2L0weq3sRXmn8gxtPDTXrztxAj74AP78U5u3s0tjzBhr3nlHGy1UKRxxcXH4+/sbDFXh4uLC+PHj871NlRAURSEmKYZeq3oRlxLHvuH7qORSKcfXXL0K06Zp9wik1AaJe+MNaNnyIP36tS2CqEu39evXY53plCstLQ1fX998b1NdMlKUUk5KyZD1Qzh96zTrBqzjqYpPPbL8nTvwzjvaUNE//6xdGnrrLbh0CT79FNzcUooo8tLtyy+/NLh/IISgX79+ODk55Xub6gxBUUo5IQSvNXsN3wa+dHuyW7bldDqtI9l772nPCAYYPBhmzdIGk1OKzuXLl/n3338Nljk7OzN27NgCbVclBEUpxa7eu0r1MtXxqe/zyHLHjsG4cdqw06ANKDd/Pjz9dBEEqWSxbNkyg3sHAK6urrRp06ZA21WXjBSllFp3eh21v6hNwMWAbMvcvav1G2jRQksGjz8OK1dqg9CpZGAeOp2O7777zqDvgYODA2PHji3wCLTqDEFRSqGD4QcZ7j+cFlVa0L5Ge6NlNm2C0aPh+nWtuejEifDRR/DYY0UcrGJgz549Roe7fumllwq8bZUQFKWUuXT3En1W9aGya2V+H/Q7DjaG4+DcuQP/+Y82AilA27bw7bfw1KPvNStF5JtvvsnS96Bp06ZUNUEbX3XJSFFKkdjkWHqu7EmqLpU/h/yJu7O7wfpNm7SB5VasAEdHbeiJ3btVMrAUsbGxbNy4MUvfg7feessk2zdpQhBCuAkh/IUQcUKIK0KIIdmUsxdCfCuEuCGEuCOE2CSEqGLKWBRFycrZ1pnhjYfjP8ifehXqpS+/fx9GjYI+fbRLRO3aQXCwdqZgpQ4bLcbatWuz9D3Q6XT4+Dy6UUBumfpP/TWQDHgAQ4ElQghjg6H8B2gNNAYqA9HAlyaORVEUPSkl12KuIYRgcvvJdKzZMX3dsWPaDeKff9bOChYuhL//hjp1zBiwYpSfn5/B/QMrKysGDBiAo6OjSbZvsoQghHAGfIGpUspYKeVeYCMw3EjxJ4C/pJQ3pJSJwGpAjYCuKIVk5t8zabSkEZejL6cvkxIWL9YeTH/+vPag+qNHYcIEdVZgqb766itGjRqFk5MTLi4u2NvbF7jvQUamvKlcF0iTUoZkWBYMdDRS9kdgsRDiwdnBUGCrsY0KIUYDowE8PDwIDAw0Ycj5ExsbaxFxWAK1LyA6Opq0tDSL3Q8BNwKYfXY2z3k8R+g/oVwWl7l3z5bPPqvHgQPa0+l9fK4xduxFbtzQceNGwepTn4mHCmNfjBgxghdffJG9e/dy9uxZ4uPjTVeHlNIkE9AeuJ5p2WtAoJGyjwGrAAmkAv8AbjnV8fTTT0tLsGvXLnOHYDHUvpCyY8eO0svLy9xhGBUYGihtZ9rKTn6dZFJqkpRSysOHpaxaVUqQsmxZKTdsMG2d6jPxkKXsCyBI5uJ73JQnhrH6L/qMHsP4sKtLAAegPOAMbCCbMwRFUfLn4p2L9F3TlyfdnmT9wPXYWdvh5wft22vPNG7VCo4fh759zR2pYilMmRBCABshRMZbUV7AKSNlvQA/KeUdKWUS2g3llkKICiaMR1FKtcqulRnYcCBbhmzBxaYcb72lPbIyKQlef127cVyjhrmjVCyJye4hSCnjhBAbgJlCiFeBJoAPYGxwjSPACCFEIBAPjAMipJRRpopHUUqrhJQEktOSKeNQhm97fcvNm9C1q9afwNYWvv4aXnvN3FEqlsjUbQnGAY7ATbR7BGOllKeEEO2FEBmf8/Y2kAicB24BzwPqxFVRCkgndYz8fSTtfmpHUmoS//6rjUO0e7c2DtHff6tkoGTPpENXSCnvAC8YWb4HcMkwfxutZZGiKCY0ZccU1p5ey+fdPmdPoD2+vhATo90v2LBBSwpK0ZkwYQLt2rXjq6++MncouaJaGytKCfH90e/5dN+njHl6DOXOTKJHDy0ZDBgAO3cWn2Rw69Ytxo0bR82aNbG3t8fDw4MuXboQEJD9qKwZBQYGIoQgKqrorkD7+fnh4uKSZfnMmTOZM2dOkcVRUGpwO0UpAXZc2sHYLWPp/mQP3A5+zaufaMMgv/suzJlTvDqa+fr6Eh8fz48//kjt2rW5efMmf//9N7cfPJWnCCUnJ2NnZ5fv1z/22GO4urqaMKJClpu2qZYyqX4IlkftC8vohxB+L1wO+W2kHDQ4WYKUVlZSLllinlgK8pm4e/euBGRAQEC2ZZYvXy6bN28uXVxcpLu7u+zfv78MDw+XUkoZGhoq0fo3pU8jR46UUmp/pzfeeMNgWyNHjpQ9e/ZMn+/YsaMcM2aMnDRpkqxQoYJs3ry5lFLK+fPny0aNGkknJydZuXJl+corr8i7d++mv9/MdX700UdSSim9vLwM6qxRo4acNWuWHD16tHR1dZVVqlSRc+fONYjp3LlzskOHDtLe3l7WrVtXbtmyRTo7O8uffvopX/tUSvP0Q1AUpYjdSbhDmi6NcjZViPbzY80qW1xcYPNmGDPG3NHlnYuLCy4uLmzcuJHExESjZZKTk5kxYwbBwcFs3ryZqKgoBg8eDEC1atVYv349AKdOnSIyMpLFixfnKYYVK1YgpWTPnj388ssvgDZm0KJFizh16hQrV67k8OHDvPnmmwC0adOGRYsW4eTkRGRkJJGRkbz99tvZbn/hwoU0atSIY8eO8d577/Huu+9y4MABQBuorm/fvtjY2HDw4EH8/PyYMWOGwcNwCpO6ZKQoxVRcchzdlnejlmMzri/9nr17oXx52LYNmjc3d3T5Y2Njg5+fH6+99hpLly6ladOmtG3blgEDBvDMM88A8PLLL6eXr1WrFkuWLKFBgwaEh4dTtWpV3NzcAKhYsSIVKuS9a9MTTzzB/PnzDZZNmDAh/feaNWsyd+5cfHx8+Pnnn7Gzs6NMmTIIIahUqVKO23/22WcZP348AG+++SZffPEFO3bsoHXr1gQEBHDu3Dm2b99OlSraANALFy6kbdu2eX4f+aHOEBSlGErTpTF4/WD+uRDBsTnz2LsXqlaFPXuKbzJ4wNfXl4iICDZt2kSPHj3Yv38/rVq1Yvbs2QAcO3YMHx8fatSogaurK831b/jq1asmqf9pI88G3blzJ926daNq1aq4urrSr18/kpOTuX79ep6337hxY4P5ypUrc/PmTQDOnj1L5cqV05MBQIsWLbAqoptAKiEoSjE0afskNh0OpsLqM1w6U4Y6dWDvXmjQwNyRmYaDgwPdunVj2rRp7N+/n1deeYXp06dz7949nnvuOZycnFi+fDlHjhxh27ZtgHYp6VGsrKyyPJg+JSUlSzlnZ2eD+StXrtCzZ08aNGjA2rVrOXr0KMuWLctVncbY2toazAsh0Ol0gHZPt6DPRS4IlRAUpZj5+vDXLP5zMy4rjnMrrCxNmmhnBiV5GApPT09SU1M5fvw4UVFRzJ49mw4dOlC/fv30o+sHHrQKSktLM1ju7u5OZGSkwbLg4OAc6w4KCiI5OZmFCxfSunVr6tatS0RERJY6M9eXHw0aNODatWsG2w8KCkpPGIVNJQRFKWbcE9vi9OsRYqPK0bYt7NoFHh7mjso0bt++TefOnVmxYgUnTpwgNDSUtWvXMnfuXLp06YKnpyf29vZ89dVXXLp0iS1btjB16lSDbdSoUQMhBFu2bOHWrVvExmqDJHTu3JmtW7eyceNGzp07x8SJEwmrobcSAAAgAElEQVQLC8sxpjp16qDT6Vi0aBGhoaGsWrWKRYsWGZSpWbMmiYmJBAQEEBUVZfAQm7zo1q0b9erVY+TIkQQHB3Pw4EEmTpyIjY1NkZw5qISgKMXEnYQ7nDsHE15sQvydcnTooN1ALlvW3JGZjouLC61atWLx4sV07NiRhg0bMnnyZIYMGcKaNWtwd3fn559/5vfff8fT05MZM2awYMECg21UqVKFGTNmMGXKFDw8PNJv4L788svpU9u2bXFxcaFvLoZ6bdy4MYsXL2bBggV4enryww8/MG/ePIMybdq0YcyYMQwePBh3d3fmzp2br/dvZWWFv78/SUlJtGzZkpEjRzJlyhSEEDg4OORrm3mSm7apljKpfgiWR+2LoumHcCX6inR/t710LX9fgpTe3lLGxhZqlfmmPhMPmWJfHD9+XAIyKCgo39sgl/0QVLNTRbFwMUkxdJn/JlHfrEXGutC5M2zaBE5O5o5MKQz+/v44OztTp04dLl++zMSJE/Hy8qJZs2aFXrdKCIpiwVLSUnh+8UQuLFgKcR507Qp//KGSQUl2//593nvvPcLCwihXrhze3t4sXLiwSO4hqISgKBZsxLLp7Pt4BsR50K2blgwcHc0dlVKYRowYwYgRI8xSt0oIimKhwsLgf9Pfh/uudOgAv/+ukoFSuFQrI0WxQOevxNC1K0RFuNKypTY2kbpMpBQ2lRAUxcJsO3GE+s+EExICXl5a09LiNIKyUnyphKAoFiT4Sii9e9qgu+FJnXqpbN8O5cqZOyqltFAJQVEsRMTdO7TpdoPU8KZUq5nMrh02VKxo7qiU0kQlBEWxAAnJyXg9e5L4861wc08mcIcdGQa8VJQioRKCopiZlDDhTRuigjri6JLMzgA7atUyd1RKaWTShCCEcBNC+Ash4oQQV4QQQx5RtpkQYrcQIlYIcUMI8R9TxqIoxcXkqcksXWqFvT1s22KHl5e5I1JKK1P3Q/gaSAY8gCbAFiFEsJTyVMZCQogKwDbgv8A6wA6oauJYFMXiDf/gECs+fQYrK8maNYIOHcwdkVKamewMQQjhDPgCU6WUsVLKvcBGYLiR4hOBv6SUv0opk6SU96WUZ0wVi6IUB9O+PM2Kz1oA8M23afj4mDkgpdQz5SWjukCalDIkw7JgoKGRsq2AO0KI/UKIm0KITUKI6iaMRVEs2o/rrjLrv7VBWjFtZgKvv6YGDVDMz5SfQhfgXqZl9wBjXWqqAs2AbsBJYC6wCsjyJGkhxGhgNICHhweBgYGmizifYmNjLSIOS6D2BURHR5OWlpbr/XDstOTt/7aANDue73sW73bXKUm7UH0mHipu+8KUCSEWeCzTsseA+0bKJgD+UsojAEKIGUCUEKKMlNIgqUgplwJLAZo3by69vb1NGHL+BAYGYglxWAK1L6Bs2bJER0fnaj9cvQqDh+iQyVY863ObTevqY2VVv/CDLELqM/FQcdsXprxkFALYCCHqZFjmBZwyUvYEkPFp1w9+N9/TpRWlkN25q6NHDx3XI63o2BE2rimPlWr4rVgQk30cpZRxwAZgphDCWQjRFvABlhsp/hPQVwjRRAhhC0wF9kopo00Vj6JYkuRkaNrpEqdPW1Gvvg5/f7C3N3dUimLI1Mcn4wBH4CbaPYGxUspTQoj2QojYB4WklDuBycAWfdnaQLZ9FhSlOJMSOrxwnqvBtXEqd49tW4Uan0ixSCZt2iClvAO8YGT5HrSbzhmXLQGWmLJ+RbFEQ8df5NDWOljbJ7DzL2dq1lRXRhXLpK5gKkohmr4gnFXfPAkijdVrJM+0UM1LFculEoKiFJJt2+Djd7UR6j5dGEN/H/WEG8WyqYSgKIXgUFASAwZI0tIEH3wA7/1H3TRQLJ9KCIpiYqGX0+jYLZbYWMGQIZKPPzZ3RIqSOyohKIoJRUdDc+8bJEWXp06zayxbJlRfA6XYUB9VRTGR5GRo2S2MO1cq41b9Oof+V0X1NVCKFZUQFMUEpIQeg65yPqga9mXuciSwouproBQ7qg2copjAjRujOXGiOlZ2CfxvmyO1nlDHWkrxoxKCohRQ5I1u3LgxDisr8F/rQLtWquOZUjypwxhFKYBNf8USEvIOAIsXQ58+KhkoxZdKCIqST8EnU+jXD9DZUab6j4wfb+6IFKVgVEJQlHy4fl3Srms0qfEuuFTZTo2yX5s7JEUpMJUQFCWP4uOhRedIYm+683i9cJo8MR8hdAZl1q9fj5eXF+PGjePXX38lJCQEnU6XzRYVxTKom8qKkgdpaTB0KISfqYxzxVscC6zCiy8mZSlXv359Tp48yYkTJ1i+fDlSSnQ6HQ0bNsTb25vWrVvTvHlzqlWrhhDqvoNiGVRCUJQ8mDgxjd9/t6ZsWdi3qwKVKhn/Mm/YsCGdO3dmx44dxMamPwqEoKAgjh07houLCykpKdja2tK4cWM6depEq1at6NChAy4uLka3qSiFTV0yUpRcmjrnFl98YY2NrfbEM0/PRx/Zf/zxxzg5ZR3hVKfTERMTQ0JCAjExMezdu5fZs2fTp08f5s2bV1jhK0qOVEJQlFxY/lsMH08pD8CcxTfJzXPTW7VqRf369XO1fZ1OR/Xq1XnnnXcKEKWiFIxKCIqSg/0Hkxk13BakFa9MvMLbYyvl+rWffPJJri4BOTs7s337dpydnQsSqqIUiEoIivIIly9LuvSIR5fsSAefS3w/r0aeXv/cc89RqdKjE4i9vT29e/fmySefLEioilJgKiEoSjbu3IHnn4fE6LLUahZKwG+1yGuDICEEs2bNeuRZQlJSEhs3bqRHjx7ExMQUMGpFyT+VEBTFiIQE6N1HcuaMoGFDSdD/amJnl79t9e/fP8fLRnFxcQQGBtKwYUPOnDmTv4oUpYBMmhCEEG5CCH8hRJwQ4ooQYkgO5e2EEGeFEOGmjENRCiItDbq9cIv9+wSVKqeybZugXLn89xWwsbHho48+ynJ/wNHR0WA+KSmJa9eu0aJFC9atW5fv+hQlv0x9hvA1kAx4AEOBJUKIho8o/w5w08QxKEq+SQnDXr3Lvu3uWDneY8OmeKpWLfh2R40aha2tbfq8k5MTAwcOzJIUpJTExcUxYsQI/vvf/5KWllbwyhUll0yWEIQQzoAvMFVKGSul3AtsBIZnU/4JYBgwx1QxKEpBTZ4ey2q/cmCTyKp18bRu9phJtuvg4MC7776Lg4MDTk5OfP/99/j5+fHzzz8b7auQkJDA0qVLad++PVFRUSaJQVFyYsozhLpAmpQyJMOyYCC7M4QvgclAggljUJR8++6HZD6d6QJCx6ffhDHw+cdNuv033ngDKysrRo0axZAh2tXUAQMGcOTIEapWrYp9pudtxsfHc/ToUTw9PTl69KhJY1EUY4SU0jQbEqI9sFZKWSnDsteAoVJK70xl+wKvSym7CyG8gRVSSqMn5kKI0cBoAA8Pj6dXr15tkngLIjY2Vg0voFdS9sXBg25MmfIUOp0VPV/ZytvDHHN+kd6ECRNIS0vjyy+/zLFsZGQkFStWxNra2mB5XFwc06dP5+TJkyQlZR0byd7enrfeeovnn38+13GZS0n5TJiCpeyLTp06HZVSNs+xoJTSJBPQFIjPtGwSsCnTMmfgPFBHP+8NhOemjqefflpagl27dpk7BItREvbFoUNSOjnpJEj5/vu6PL++Y8eO0svLq8BxpKWlyRkzZkhHR0cJZJmcnJzkSy+9JJOSkgpcV2EqCZ8JU7GUfQEEyVx8x5ryklEIYCOEqJNhmRdwKlO5OkBNYI8Q4jqwAXhcCHFdCFHThPEoSo7+/Rc6P5tIfLxgyLAUZs8238ijVlZWTJs2jQ0bNuDq6oqVleG/Z3x8PKtXr6Z58+Zcu3bNTFEqJZnJEoKUMg7ty32mEMJZCNEW8AGWZyr6L1ANaKKfXgVu6H8PM1U8ipKTixehY5dE4u45ULHZIX74QeS541lh6N69O8HBwTz55JM4ODgYrEtISODMmTM89dRT7N6920wRKiWVqZudjgMc0ZqSrgLGSilPCSHaCyFiAaSUqVLK6w8m4A6g08+rNnZKkYiIgI6dk7hz0wGXeof5d6cnjvaWMxr8E088QXBwML169crSCik1NZXo6Gi6d+/O/PnzH1yKVZQCM2lCkFLekVK+IKV0llJWl1Ku1C/fI6U0emdFShkos7mhrBjn7e3NePUA33y7fRs6dUnh2lV7bKsdJ2hHVdzLuJo7rCwcHR357bffmDNnTpb+CqCdLUybNo1+/foRFxdnhgiVkqbUDF1x69Ytxo0bR82aNbG3t8fDw4MuXboQEBCQq9cHBgYihCjSNuF+fn5GWyhs2LCBOXNU9438iImB7t0h5Kwt9o+fJ2CbDfWqVDZ3WNkSQvDWW28REBBAuXLlsLExPIuJj49n27ZteHl5cfHiRTNFqZQUpSYh+Pr6cvjwYX788UdCQkLYvHkzPXr04Pbt20UeS3JycoFe7+bmhqur5R3RWrqEBOjTRxIUBLVqwfnDtejo+ZS5w8qVtm3bcurUKZ566qksZwuJiYmEhobSpEkTtmzZYqYIlRIhN02RLGXKb7PTu3fvSkAGBARkW2b58uWyefPm0sXFRbq7u8v+/fvL8PBwKaWUoaGhWZoAjhw5UkqpNTl84403DLY1cuRI2bNnz/T5jh07yjFjxshJkybJChUqyObNm0sppZw/f75s1KiRdHJykpUrV5avvPKKvHv3rpRSa66Wuc6PPvrIaJ01atSQs2bNkqNHj5aurq6ySpUqcu7cuQYxnTt3Tnbo0EHa29vLunXryi1btkhnZ2f5008/5WufPmApzepykpwsZa9eWtNSZ7doeeFC3puXZsdUzU5zIykpSb722mvSycnJaNNUR0dH+eGHH8q0tLQiiceY4vKZKAqWsi8wQ7NTi+Xi4oKLiwsbN24kMTHRaJnk5GRmzJhBcHAwmzdvJioqisGDBwNQrVo11q9fD8CpU6dYv349ixcvzlMMK1asQErJnj17+OWXXwCtmeGiRYs4deoUK1eu5PDhw7z55psAtGnThkWLFuHk5ERkZCSRkZG8/fbb2W5/4cKFNGrUiGPHjvHee+/x7rvvcuDAAUB7Glffvn2xsbHh4MGD+Pn5MWPGDKMdoEqilBR48UXYvFmA4236z1nKk09aQHOifLCzs2Pp0qUsWbIk2yEvFixYQLdu3YiOjjZDhEqxlpusYSlTQTqmrVu3TpYrV07a29vLVq1ayUmTJsmDBw9mW/7MmTMSkGFhYVLKh0fst27dMsj6uT1DaNSoUY4xbt26VdrZ2aUf3f3000/S2dk5SzljZwgvvviiQZnatWvLWbNmSSml3LZtm7S2tk4/45FSyn379kmgxJ8hJCdL2b+/lCAlDndkp9nvyDSdaY+ei/IMIaN//vlHenh4SDs7uyxnCnZ2drJy5cry5MmTRR6XpX8mipKl7AvUGYIhX19fIiIi2LRpEz169GD//v20atWK2bNnA3Ds2DF8fHyoUaMGrq6uNG+u9fK+evWqSep/+umnsyzbuXMn3bp1o2rVqri6utKvXz+Sk5O5fv16nrffuHFjg/nKlStz86Y2kOzZs2epXLkyVapUSV/fokWLLB2fSprUVBg2DNatAxyi8Zz4Xza/PR0rUTLed5MmTTh9+jQtW7bMcraQnJxMREQEzzzzDKtWrTJThEpxUzL+M3LJwcGBbt26MW3aNPbv388rr7zC9OnTuXfvHs899xxOTk4sX76cI0eOsG3bNiDnG8BWVlZZ2oGnpKRkKZd5LPwrV67Qs2dPGjRowNq1azl69CjLli3LVZ3GZBxaGbTWKTqdDtDOAoUl9LgqQqmpMHw4/PYbOLmk8MSbY9j5wWc42Wa9zFKcubm5ERgYyPjx4402TY2Pj+fVV1/l/fffN0N0SnFTqhJCZp6enqSmpnL8+HGioqKYPXs2HTp0oH79+ulH1w/Y6R+XlXl8end3dyIjIw2WBQcH51h3UFAQycnJLFy4kNatW1O3bl0iIiKy1GmK8fAbNGjAtWvXDLYfFBSUnjBKmtRUGDkSVq8GV1fYEWBLyKcr8HDxMHdohcLa2prPPvuMlStX4uzsnCX5SymJjY01U3RKcVIqEsLt27fp3LkzK1as4MSJE4SGhrJ27Vrmzp1Lly5d8PT0xN7enq+++opLly6xZcsWpk6darCNGjVqIIRgy5YtREdHp/+Dde7cma1bt7Jx40bOnTvHxIkTCQvLeQSOOnXqoNPpWLRoEaGhoaxatYpFixYZlKlZsyaJiYkEBAQQFRVFfHx8vt5/t27dqFevHiNHjiQ4OJiDBw8yceJEbGxsStyZQ3IyDB4MK1eCtUM8b36xmVatwMbKcnohF5YXXniBo0ePUr169fShtG1sbGjQoAELFy40c3RKcVAqEoKLiwutWrVi8eLFdOzYkYYNGzJ58mSGDBnCmjVrcHd35+eff+b333/H09OTGTNmsGDBAoNtVKlShRkzZjBlyhT69euX3lP45ZdfTp/atm2Li4sLffv2zTGmxo0bs3jxYhYsWICnpyc//PAD8+bNMyjTpk0bxowZw+DBg3F3d2fu3Ln5ev9WVlb4+/uTlJREy5YtGTlyJFOmTEEIkWWsnOIsIQH69tXuGdg5x5M2pCt1mpSuh8vUq1ePkydP0qVLFxwcHHjsscfYsmVLlkuKimJUbu48W8qkhr82nePHj0tABgUFFWg7lrIvYmKk7NRJa03kVCZOMrqp/HDHh0VSt7laGT2KTqeTX3/9dYH/vvlhKZ8JS2Ap+4JctjIq+efRCgD+/v44OztTp04dLl++zMSJE/Hy8qJZs2bmDq3A7t6F55+HgwehrHsC0QOaM6RLU2Z2mmnu0MxGCMG4cePMHYZSzKiEUErcv3+f9957j7CwMMqVK4e3tzcLFy4s9vcQbtzQxiY6fhxq1IB+c5YRlFiBZX2WFfv3pihFTSWEUmLEiBGMGDHC3GGYVEiIlgxCQ6FuXcn//ieoVu0NUtJGY2utrpkrSl6VipvKSslz8CC0aaMlgybNUnlsTC/C2A+gkoGi5JNKCEqxs2kTdO6sPdege480nF7tycm4HVk6CCoFV7NmzSyt35SSS10yUoqV776DceNAp4OXX5bEdx/FttPbWeW7irbV25o7vGJp1KhRREVFsXnz5izrjhw5kqWXvVJylbgzhPnz57N69Wru3btn7lAUE0pLg3ffhTFjtGTw0Ufw+JBprD69gk86f8KLT71o7hBLJHd3d6Ojqha1gj5DRMmdEpUQIiIimDx5MqNHj6ZixYq0atUqfahppfi6dw9694bPPwcbG/j+e5g6LY0zt0/zStNX+KDdB+YOscTKfMlICMHSpUsZMGAAzs7O1KpVixUrVhi85tatW7z44ouUK1eOcuXK0bNnT86fP5++/uLFi/j4+FCpUiWcnZ1p1qxZlrOTmjVrMn36dF5++WXKli3L0KFDC/eNKkAJSwibNm3CxsaG+/fvk5yczKFDh5g2bZq5w1IK4Px5aNUKtm6F8uXhf/+DV16RWFtZs3bAWpb0XKKalxaxmTNn4uPjQ3BwMIMGDeLll1/mypUrgDaY3sSJE3FwcODvv//mwIEDPP7443Tt2jV96JXY2Fh69OhBQEAAwcHB+Pr60q9fP86ePWtQz4IFC6hfvz5BQUHpoxIrhatEJYQVK1YYjPdjbW1N//79zRiRUhABAdCyJZw9C089BUeOgLvnaTr6dSTsXhhWwkq1KDKD4cOHM2zYMGrXrs2sWbOwsbFhz549AKxevRopJT/99BONGzemfv36fPfdd8TGxqafBXh5eTFmzBgaNWpE7dq1mTJlCs2aNWPdunUG9XTs2JF3332X2rVrU6dOnSJ/n6VRibmpHBsby5EjRwyWOTo64uvra6aIlPzS6bTLQ1OmaPcOfHxg+XKIFzfo/GNPElIS0MmSOVJrcZDx2Rs2Nja4u7unjw589OhRIiMjszzzOz4+nosXLwIQFxfHjBkz2Lx5M5GRkaSkpJCYmJjlmR4PnkmiFB2TJgQhhBvwI/AsEAV8IKVcaaTcO8BIoIa+3DdSys8LUvdff/2FnZ2dwWMhrayseOaZZwqyWaWI3b6tDV394FnxU6bAzJmQmBZPb7/e3Ii9wd+j/qZG2RrmDbQUe9SzN3Q6HbVr12bLgz9gBm5ubgC8/fbbbNu2jXnz5lGnTh2cnJwYMWJElhvHqnVT0TP1GcLXQDLgATQBtgghgqWUpzKVE8AI4ATwJLBdCBEmpVyd34pXr17N/fv3DZb17t27xD8VrCQ5eBAGDYKrV6FcOfjlF+jVC3RSx7ANwwiKCMJ/kD8tqrQwd6hKNpo1a8by5cupUKECZcuWNVpm7969jBgxIv3sPTExkYsXL1K3bt2iDFUxwmTflkIIZ8AXmCqljJVS7gU2AsMzl5VSzpVSHpNSpkopzwF/APluRJ6amsrWrVsNlrm6uvLii6opYnEgJSxaBO3ba8ngmWfgn3+0ZABwN+EuF+5cYMFzC/Cp72PeYEuomJgYjh8/bjBdvnw5z9sZOnQobm5u+Pj48PfffxMaGsru3buZNGlSekujunXr4u/vz7Fjxzh58iTDhg0jMTHRxO9IyQ9TniHUBdKklCEZlgUDHR/1IqE1EWkPfJfN+tHAaAAPDw8CAwOzlDl+/HiWXqqJiYnY2toaLV9QsbGxhbLd4qig++L2bTs+/7wehw6VB6B//zBGj75EaKgkNPRhuXn15mGbUDh/z4KKjo4mLS3NImPLjevXr7Nnzx6aNm1qsLxDhw7pR+8Z39upU6eoUKFC+nzmMp988gkrV67khRdeIC4ujvLly6c///natWsMGDCAzz//PP35If3798fT05Pr16+nb8NYvcVRsfuuyM0Y2bmZ0L7Ur2da9hoQmMPrZqAlDvuc6sjueQjjx4+XVlZWEkifunTpkp9hw3PFUsY4twQF2Rdr1kjp5qY9w6BcOSnXrzdc/2fIn3LAbwNkXHJcwYIsZJb4PARzUv8fD1nKvsAMz0OIBR7LtOwx4L6RsgAIIcaj3UtoL6VMyq7co0gpWbt2rcHzgZ2dnVVHFgt25w6MHw+rVmnz3bvDjz9C5coPywRfD2bguoHUdqutWhQpShEx5R3XEMBGCJGxwbAXkPmGMgBCiJeB94EuUsrw/FZ6+vTpLDeTU1JS6PXgArRiUTZvhkaNtGTg5ARLlsCffxomg2sx1+i5sidl7MuwefBmXOxczBewopQiJjtDkFLGCSE2ADOFEK+itTLyAdpkLiuEGArMBjpJKS8VpN4NGzaQmppqsKx+/fq4u7sXZLOKiV27Bv/5D6xfr823bq21Iqpd27Dc/aT79FrVi3tJ99j70l6qPFal6INVlFLK1G0yxwGOwE1gFTBWSnlKCNFeCBGbodzHQHngiBAiVj99m58KV61aZdB+2cHBQV0usiBpafDll9CggZYMXFxg4ULYvTtrMgAIjQ4l8n4kawesxauSV9EHrCilmEn7IUgp7wAvGFm+B3DJMP+EKeqLjIzk0iXDEwwhBH379jXF5pUCOnwY3ngDgoK0+RdegC++gGrVsn9NY4/GXHzrIs52qlOSohS1Yt1ra+PGjdjYGOY0Nzc3Ne6JmV29CkOHav0JgoKgalX4/Xfw988+GSw8sJCPdn2ElFIlA0Uxk2KdEH799Vfi4uLS562trRk0aJAZIyrdYmJg8mSoVw9WrgR7e3jvPTh9WhuPKDv+Z/yZtH0Sp26dQqKeeqYo5lJsB7eLjY3l8OHDBsucnJzU6KZmkJioPaPg449BP8YZgwfD7NlQs+ajX3v42mGGbhhKyyotWd53OVaiWB+jKEqxVmwTwvbt27MMZieEUIPZFaHERPD3r8LQoRARoS1r3RoWLNCeYZCTy9GX6b2qNx4uHmwcvBFHW8fCDVhRlEcqtglh1apVWfof9OrVSw1mVwSSkrSOZLNnw7Vr2v0aLy/tsZYvvAC5fV7N0YijSCn5c8ifVHSuWIgRK4qSG8UiIQghnIBnHoy1ogazM4+oKPj2W/jqK7hxQ1tWq1Ys8+a54OMDec3Fvp6+PPvks7jau+ZcWFGUQlcsEgLQFNhx/PhxunfvTosWLbKcCSQnJ9O1a1fzRFfCnTun9R34+WftMhFAkyYwdSqULRtE587eud6WlJLxf46n8xOd8fX0VclAUSxIcUkI54AUKaXdX3/9xb59+0hISDAo0K5dOxwd1TVoU0lJgU2bYOlS+Ouvh8t79oSJE6FTJ+3SUF4Hcpy9ZzbfBH1DBacK+Hqqp9kpiiUpFhfcpZRRQPrd49jYWNLS0tLXW1tbc+LECd59910OHz5sMNCdkjcXL8IHH2j9BXx9tWRgbw+vvaY1H928GTp3zv19goxWnVzFh7s+ZFjjYUz3nm7y2BVFKZjicoYAcB5oZmxFWloat27dYuHChXzzzTc4OTkREhKS7RObFEO3b8O6ddqAc3///XC5pyeMHg3Dh4P+6Yf5tvfqXkb9MYoONTrwQ+8fEPnJKIqiFKrilBCOkU1CeCA1NRU7Ozv69OlDmTJliiis4ik2FjZu1DqQ/fUXPBgf0NERBg7UEkHr1vk7EzDmrwt/UbNsTfwH+WNvY2+ajSqKYlLFKiEIIbI8GS0jJycnevfuzdKlS9URqBGRkVoS+OMP2LEDHowJaG0Nzz0HQ4ZozUYfy/xUCxOY1XkWb7d5mzIOKlEriqUqTgnh1KMSgqOjI126dOHXX39VfRH0UlO1sYQCArRr/xk7dgsBbdtqPYoHDICKhdANIDE1kZf+eIn3276PVyUvlQwUxcIVp4RwOrtk4ODgQJs2bVi/fj3W1tZFHJblkBJCQmDXLti+HXbuhHv3Hq53cIBu3bRxhXr1Ag+PwotFJ3W8/MfLrP53NX3r91VDWStKMVBsEoKUMsra2jrLGYK9vT1NmzZl8+bN2Nramik680hKgmPHYKB164wAAAt1SURBVO9e2LcP9u+HW7cMy9SurSWB556Drl3BuYgGEp22axqr/l3F7M6zGdhwYNFUqihKgRSbhADamUB8fHz6vJ2dHQ0aNCAgIAAHBwczRlb4UlO1DmL//KMlgSNHtCkp05OoPTygfXstCXTrBk+Y5MkTefPTPz/xyZ5PeKXpK7zf7v2iD0BRlHwpVgnB0dExPSHY2tpSq1YtAgMDcS6qw94icucOnDmjtfv/5x9tCg6GTH3xAK1paNu20K6d9rNWLdO1DMoPKSW/nf6NbrW6saTnEnVzX1GKkWKVEJydnYmPjyclJYWqVauyd+/eYtu8NCkJrlyB0FDtyP/MmYfTgyGkM6tZE5o2hWbNtKlVq4L3DzA1IQR/vPgHSalJ2FqXrkt4ilLcFauE4ODgQHJyMpUqVWL//v2UL1/e3CFlKzFRa+YZHq596YeGwqVLD3+/dk27CWyMszPUr689h9jLS/vyb9LE8r78M7oee50J2ybwZY8vcXd2x87aztwhKYqSR8UqITg5OdGuXTt++eUXKlWqVOT1p6Vpl3OuXHFizx7tBu6NG9qzAK5d034++P3OnUdvy8oKqlfXLvHUrq1d+mnQQJuqVs37yKHmFJ8ST59VfTh16xTvtHkHd2d3c4ekKEo+FKuEYG1tTWBeR1PLJClJa4oZE2P4M/Pvd+9qwz1HRWlf/FFR2jLtqL5ljvXY2MDjj0PlytqN3YxTrVraWEEloVFUmkxj2IZhBEUE4T/In6crP23ukBRFySeTJgQhhBvwI/AsEAV8IKVcaaScAP7f3v3HVlXecRx/f/tjzFILQhVBBjqmkTGCcY0/ZtRmEdlEoslCMLIfcSpMo2CGTP8hQRfN1CiapU6IMDaRsbFQmaJO2FIjEmU4i0LETsvEmiwK0tJSobT97o9zS0vpL3pv73PvPZ9XclPO6XNvP304fb73nHPPc34D3JZYtRK4z/u6DBmor4c1a6C5+dQfTU3RgN/9Uzmn9vvB6NFQVNTMhAlFlJbCmWfCOedEA3/H13HjovXZ9C5/sJbXLqeyrpInZzzJDRf2ceNkEcl4qd5DqABagDHARcAmM9vp7ru7tZsH3AhMAxzYDNQCz/T14h9/HE20loyCAhgxInqUlHT+u/vyyJHRoF5ayvGB/4wzomkeqqq2U15enlyQHNBwpIE397/J3ZfczcLLFoaOIyJJsn7elA/8hcyGAweB77h7TWLdc8Bn7n5/t7bbgNXuviKxfCtwu7v3eSfegoILffTo35KXd5T8/CN9fs3LO0J+/olfCwoOk5d3NOmPZdbX12sm1YT9h/czevhojPh+vLS6uprW1lbKyspCR8kI+vvolCl98frrr7/j7v1uoKncQ7gAaOsoBgk7gat7aDsl8b2u7ab09KJmNo9oj4LCwkLGjl084EDt7dGjYybPVGlra6O+vj61L5pFvhrxFQfOPcC498ZhrUbDsYb+n5TDWltbcfdYbxNdxf3vo6ts64tUFoRioPvI0AD0dI/E7m0bgGIzs+7nERJ7ESsAysrKfMeOHalLPEhVVVWxPWRUd6iOS5+9lNPsNF6reI09O/bEti86lJeXU19fT3V1degoGSHOfx/dZUpfDPQC0VSe9mwCuk+cXAI0DqBtCdDU30llCavxaCPXr72exqONvHzzy5xdnP6P/orI0EllQagBCszs/C7rpgHdTyiTWDdtAO0kQ7S2tzLnr3PY9fku1s9ez9QxU0NHEpEUS1lBcPfDwAbgQTMbbmZXADcAz/XQ/I/AL83sHDMbBywCVqcqi6TeB198wNZ9W3l65tPM+NaM0HFEZAik+mOndwKrgM+BA8Ad7r7bzK4EXnH34kS75cA3gfcTy88m1kmGmjpmKjV31+gwkUgOS2lBcPcvia4v6L7+DaITyR3LDvwq8ZAMVvlBJfsa9rHwsoUqBiI5LgbX0spgbf9sO3M3zGXd7nW0tLWEjiMiQ0wFQXq09+BeZv1pFmcXn83GmzZq9lKRGMiqye0kPQ5+dZCZa2fS0tZC1c+qOGv4WaEjiUgaqCDISbbUbqH2YC2v/vhVJp85OXQcEUkTFQQ5yewps7n8G5czvmR86CgikkY6hyDHPb7tcTZ/vBlAxUAkhlQQBIC176/l3s33snbXSbevEJGYUEEQ3vjkDW7ZeAtXTbyKZ2b2eUsKEclhKggxV3Oghhv/fCPnjTyPyjmVDCsYFjqSiASighBzq6tXk2d5bLp5E6NOGxU6jogEpIIQcw99/yHemfcOk0ZNCh1FRAJTQYihdm/nvs338dGXH2FmTBgxIXQkEckAKggxtOSfS3h026O8+OGLoaOISAZRQYiZVe+u4uGtD3P7xbdzz2X3hI4jIhlEBSFGttRuYf5L87l20rVUXFcx4Pusikg8qCDEhLvz2LbHmFw6mfWz11OYXxg6kohkGM1lFBNmRuWcShqONFAyrCR0HBHJQNpDyHHNx5pZ9PdFHDp6iKLCIsaePjZ0JBHJUCoIOaytvY25G+ay7K1lvF33dug4IpLhdMgohy3evJgX9rzAUz94iumTpoeOIyIZTnsIOapiewXL3lrGgksWsODSBaHjiEgWSElBMLNRZlZpZofN7BMzu7mPtovNbJeZNZrZXjNbnIoM0qn5WDOPvPkIsy6YxRMznggdR0SyRKoOGVUALcAY4CJgk5ntdPfdPbQ14KfAe8Ak4DUz+9Td16UoS+wVFRax7dZtjPz6SPLz8kPHEZEskfQegpkNB34ELHH3JnffCvwN+ElP7d39UXf/t7u3uvuHwEbgimRzCNQdquOBqgdoa29jfMl4ir9WHDqSiGSRVOwhXAC0uXtNl3U7gav7e6JFl8peCSzvo808YF5iscnMPkwia6qUAvtDh+jNUpam88dldF+kUamZqR8i2iY6ZUpfTBxIo1QUhGKgodu6BuD0ATx3KdFeyu97a+DuK4AVgw03FMxsh7uXhc6RCdQXEfVDJ/VFp2zri34PGZlZlZl5L4+tQBPQ/dLXEqCxn9e9i+hcwkx3PzrYX0BERFKj3z0Edy/v6/uJcwgFZna+u/8nsXoa0NMJ5Y7n/By4H7jK3esGHldERIZK0ieV3f0wsAF40MyGm9kVwA3Acz21N7O5wMPAdHevTfbnB5JRh7ACU19E1A+d1BedsqovzN2TfxGzUcAqYDpwALjf3dcmvncl8Iq7FyeW9wLjga6Hida4+y+SDiIiIoOWkoIgIiLZT1NXiIgIoIIgIiIJKghJMrPzzeyIma0JnSUEMxtmZisTc1g1mtm7ZvbD0LnS5VTm8cplcd8OepNt44MKQvIqgH+FDhFQAfAp0ZXpI4AlwF/M7NyAmdKp6zxec4HfmdmUsJGCiPt20JusGh9UEJJgZjcB9cA/QmcJxd0Pu/tSd/+vu7e7+0vAXuC7obMNtVOdxyuXxXk76E02jg8qCINkZiXAg8Ci0FkyiZmNIZrfqtcLE3NIb/N4xXEP4QQx2w5Okq3jgwrC4P0aWOnun4YOkinMrBB4HviDu+8JnScNkpnHK2fFcDvoSVaODyoIPehv/iYzuwi4BlgWOutQG8BcVh3t8oiuTm8B7goWOL0GNY9XLovpdnCCbB4fdE/lHgxg/qZ7gHOBfdEM3hQD+Wb2bXe/eMgDplF/fQHHpzFfSXRi9Tp3PzbUuTJEDac4j1cui/F20F05WTo+6ErlQTCzIk58Z3gv0QZwh7t/ESRUQGb2DNGd8q5x96bQedLJzNYBDtxG1AcvA9/r5W6BOS3O20FX2Tw+aA9hENy9GWjuWDazJuBIpv9nDwUzmwjMJ5qb6n+Jd0QA8939+WDB0udOonm8Pieax+uOmBaDuG8Hx2Xz+KA9BBERAXRSWUREElQQREQEUEEQEZEEFQQREQFUEEREJEEFQUREABUEERFJUEEQEREA/g+xLVYlRlLwLAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the book uses `tensorflow.contrib.layers.fully_connected()` rather than `tf.layers.dense()` (which did not exist when this chapter was written). It is now preferable to use `tf.layers.dense()`, because anything in the contrib module may change or be deleted without notice. The `dense()` function is almost identical to the `fully_connected()` function. The main differences relevant to this chapter are:\n",
    "* several parameters are renamed: `scope` becomes `name`, `activation_fn` becomes `activation` (and similarly the `_fn` suffix is removed from other parameters such as `normalizer_fn`), `weights_initializer` becomes `kernel_initializer`, etc.\n",
    "* the default `activation` is now `None` rather than `tf.nn.relu`.\n",
    "* it does not support `tensorflow.contrib.framework.arg_scope()` (introduced later in chapter 11).\n",
    "* it does not support regularizer params (introduced later in chapter 11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28 # MNIST\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_init = tf.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, kernel_initializer=he_init, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the *ReLU* activation function is not perfect. It suffers from a problem known as the *dying ReLUs*: during training, some neurons effectively die, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network’s neurons are dead, especially if you used a large learning rate. During training, if a neuron’s weights get updated such that the weighted sum of the neuron’s inputs is negative, it will start outputting 0. When this happens, the neuron is unlikely to come back to life since the gradient of the *ReLU* function is 0 when its input is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP**\n",
    "\n",
    "So which activation function should you use for the hidden layers of your deep neural networks?\n",
    "\n",
    "Although your mileage will vary, in general `ELU` > `leaky ReLU` (and its variants) > `ReLU` > `tanh` > `logistic`.\n",
    "\n",
    "- If you care a lot about runtime performance, then you may prefer `leaky ReLUs` over `ELUs`.\n",
    "- If you don’t want to tweak yet another hyperparameter, you may just use the default `α` values suggested earlier (0.01 for the `leaky ReLU`, and 1 for `ELU`).\n",
    "- If you have spare time and computing power, you can use *cross-validation* to evaluate other activation functions, in particular `RReLU` if your network is overfitting, or `PReLU` if you have a huge training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha * z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Leaky ReLU in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a neural network on MNIST using the Leaky ReLU. First let's create the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28 # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.86 Validation accuracy: 0.9044\n",
      "5 Batch accuracy: 0.94 Validation accuracy: 0.9496\n",
      "10 Batch accuracy: 0.92 Validation accuracy: 0.9654\n",
      "15 Batch accuracy: 0.94 Validation accuracy: 0.971\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9764\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9778\n",
      "30 Batch accuracy: 0.98 Validation accuracy: 0.978\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.9788\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z, alpha=0.01):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This activation function was proposed in this [great paper](https://arxiv.org/pdf/1706.02515.pdf) by Günter Klambauer, Thomas Unterthiner and Andreas Mayr, published in June 2017 (I will definitely add it to the book). It outperforms the other activation functions very significantly for deep neural networks, so you should really try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this activation function, even a 100 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: -0.26 < mean < 0.27, 0.74 < std deviation < 1.27\n",
      "Layer 10: -0.24 < mean < 0.27, 0.74 < std deviation < 1.27\n",
      "Layer 20: -0.17 < mean < 0.18, 0.74 < std deviation < 1.24\n",
      "Layer 30: -0.27 < mean < 0.24, 0.78 < std deviation < 1.20\n",
      "Layer 40: -0.38 < mean < 0.39, 0.74 < std deviation < 1.25\n",
      "Layer 50: -0.27 < mean < 0.31, 0.73 < std deviation < 1.27\n",
      "Layer 60: -0.26 < mean < 0.43, 0.74 < std deviation < 1.35\n",
      "Layer 70: -0.19 < mean < 0.21, 0.75 < std deviation < 1.21\n",
      "Layer 80: -0.18 < mean < 0.16, 0.72 < std deviation < 1.19\n",
      "Layer 90: -0.19 < mean < 0.16, 0.75 < std deviation < 1.20\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100))\n",
    "\n",
    "for layer in range(100):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1/100))\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=1)\n",
    "    stds = np.std(Z, axis=1)\n",
    "    \n",
    "    if layer % 10 == 0:\n",
    "        print(\"Layer {}: {:.2f} < mean < {:.2f}, {:.2f} < std deviation < {:.2f}\".format(\n",
    "            layer, means.min(), means.max(), stds.min(), stds.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a TensorFlow implementation (there will almost certainly be a `tf.nn.selu()` function in future TensorFlow versions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELUs can also be combined with dropout, check out [this implementation](https://github.com/bioinf-jku/SNNs/blob/master/selu.py) by the Institute of Bioinformatics, Johannes Kepler University Linz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a neural net for MNIST using the SELU activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train it. Do not forget to scale the inputs to mean 0 and standard deviation 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.88 Validation accuracy: 0.923\n",
      "5 Batch accuracy: 0.98 Validation accuracy: 0.9578\n",
      "10 Batch accuracy: 1.0 Validation accuracy: 0.9664\n",
      "15 Batch accuracy: 0.96 Validation accuracy: 0.9682\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9694\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9688\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.9694\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "means = X_train.mean(axis=0, keepdims=True)\n",
    "stds = X_train.std(axis=0, keepdims=True) + 1e-10\n",
    "X_val_scaled = (X_valid - means) / stds\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_val_scaled, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final_selu.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a [2015 paper](https://arxiv.org/pdf/1502.03167v3.pdf), 7 Sergey Ioffe and Christian Szegedy proposed a technique called *Batch Normalization* (BN) to address the vanishing/exploding gradients problems, and more generally the problem that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change (which they call the *Internal Covariate Shift* problem).\n",
    "\n",
    "The technique consists of adding an operation in the model just before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling and shifting the result using two new parameters per layer (one for scaling, the other for shifting). In other words, this operation lets the model learn the optimal scale and mean of the inputs for each layer.\n",
    "\n",
    "In order to zero-center and normalize the inputs, the algorithm needs to estimate the inputs’ mean and standard deviation. It does so by evaluating the mean and standard deviation of the inputs over the current mini-batch (hence the name “Batch Normalization”)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BN](https://www.dropbox.com/s/rmjoencidkkshu3/batch-normalization.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the book uses `tensorflow.contrib.layers.batch_norm()` rather than `tf.layers.batch_normalization()` (which did not exist when this chapter was written). It is now preferable to use `tf.layers.batch_normalization()`, because anything in the contrib module may change or be deleted without notice. Instead of using the `batch_norm()` function as a regularizer parameter to the `fully_connected()` function, we now use `batch_normalization()` and we explicitly create a distinct layer. The parameters are a bit different, in particular:\n",
    "* `decay` is renamed to `momentum`,\n",
    "* `is_training` is renamed to `training`,\n",
    "* `updates_collections` is removed: the update operations needed by batch normalization are added to the `UPDATE_OPS` collection and you need to explicity run these operations during training (see the execution phase below),\n",
    "* we don't need to specify `scale=True`, as that is the default.\n",
    "\n",
    "Also note that in order to run batch norm just _before_ each hidden layer's activation function, we apply the ELU activation function manually, right after the batch norm layer.\n",
    "\n",
    "Note: since the `tf.layers.dense()` function is incompatible with `tf.contrib.layers.arg_scope()` (which is used in the book), we now use python's `functools.partial()` function instead. It makes it easy to create a `my_dense_layer()` function that just calls `tf.layers.dense()` with the desired parameters automatically set (unless they are overridden when calling `my_dense_layer()`). As you can see, the code remains very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid repeating the same parameters over and over again, we can use Python's `partial()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a neural net for MNIST, using the ELU activation function and Batch Normalization at each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "    \n",
    "    batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=batch_norm_momentum)\n",
    "    dense_layer = partial(tf.layers.dense, kernel_initializer=he_init)\n",
    "    \n",
    "    hidden1 = dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(batch_norm_layer(hidden1))\n",
    "    hidden2 = dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(batch_norm_layer(hidden2))\n",
    "    logits_before_bn = dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = batch_norm_layer(logits_before_bn)\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: since we are using `tf.layers.batch_normalization()` rather than `tf.contrib.layers.batch_norm()` (as in the book), we need to explicitly run the extra update operations needed by batch normalization (`sess.run([training_op, extra_update_ops],...`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9032\n",
      "1 Validation accuracy: 0.9244\n",
      "2 Validation accuracy: 0.9368\n",
      "3 Validation accuracy: 0.9484\n",
      "4 Validation accuracy: 0.951\n",
      "5 Validation accuracy: 0.9574\n",
      "6 Validation accuracy: 0.96\n",
      "7 Validation accuracy: 0.964\n",
      "8 Validation accuracy: 0.9672\n",
      "9 Validation accuracy: 0.9672\n",
      "10 Validation accuracy: 0.9692\n",
      "11 Validation accuracy: 0.9688\n",
      "12 Validation accuracy: 0.9714\n",
      "13 Validation accuracy: 0.9716\n",
      "14 Validation accuracy: 0.9724\n",
      "15 Validation accuracy: 0.9732\n",
      "16 Validation accuracy: 0.9748\n",
      "17 Validation accuracy: 0.975\n",
      "18 Validation accuracy: 0.9746\n",
      "19 Validation accuracy: 0.9746\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What!? That's not a great accuracy for MNIST. Of course, if you train for longer it will get much better accuracy, but with such a shallow network, Batch Norm and ELU are unlikely to have very positive impact: they shine mostly for much deeper nets.\n",
    "\n",
    "Note that you could also make the training operation depend on the update operations:\n",
    "\n",
    "```python\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        training_op = optimizer.minimize(loss)\n",
    "```\n",
    "\n",
    "This way, you would just have to evaluate the `training_op` during training, TensorFlow would automatically run the update operations as well:\n",
    "\n",
    "```python\n",
    "sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "```\n",
    "\n",
    "\n",
    "One more thing: notice that the list of trainable variables is shorter than the list of all global variables. This is because the moving averages are non-trainable variables. If you want to reuse a pretrained neural network (see below), you must not forget these non-trainable variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/moving_mean:0',\n",
       " 'batch_normalization/moving_variance:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/moving_mean:0',\n",
       " 'batch_normalization_1/moving_variance:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/moving_mean:0',\n",
       " 'batch_normalization_2/moving_variance:0']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.global_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular technique to lessen the exploding gradients problem is to simply clip the gradients during backpropagation so that they never exceed some threshold. This is called *Gradient Clipping*. In general people now prefer *Batch Normalization*, but it’s still useful to know about *Gradient Clipping* and how to implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple neural net for MNIST and add gradient clipping. The first part is the same as earlier (except we added a few more layers to demonstrate reusing pretrained models, see below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply gradient clipping. For this, we need to get the gradients, use the `clip_by_value()` function to clip them, then apply them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.288\n",
      "1 Validation accuracy: 0.7936\n",
      "2 Validation accuracy: 0.8798\n",
      "3 Validation accuracy: 0.906\n",
      "4 Validation accuracy: 0.9164\n",
      "5 Validation accuracy: 0.9218\n",
      "6 Validation accuracy: 0.9296\n",
      "7 Validation accuracy: 0.9358\n",
      "8 Validation accuracy: 0.9382\n",
      "9 Validation accuracy: 0.9414\n",
      "10 Validation accuracy: 0.9456\n",
      "11 Validation accuracy: 0.9474\n",
      "12 Validation accuracy: 0.9478\n",
      "13 Validation accuracy: 0.9534\n",
      "14 Validation accuracy: 0.9568\n",
      "15 Validation accuracy: 0.9566\n",
      "16 Validation accuracy: 0.9574\n",
      "17 Validation accuracy: 0.959\n",
      "18 Validation accuracy: 0.9622\n",
      "19 Validation accuracy: 0.9612\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "        \n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is generally not a good idea to train a very large DNN from scratch: instead, you should always try to find an existing neural network that accomplishes a similar task to the one you are trying to tackle, then just reuse the lower layers of this network: this is called transfer learning. It will not only speed up training considerably, but will also require much less training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing a TensorFlow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the original model was trained using TensorFlow, you can simply restore it and train it on the new task.\n",
    "\n",
    "First you need to load the graph's structure. The `import_meta_graph()` function does just that, loading the graph's operations into the default graph, and returning a `Saver` that you can then use to restore the model's state. Note that by default, a `Saver` saves the structure of the graph into a `.meta` file, so that's the file you should load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you need to get a handle on all the operations you will need for training. If you don't know the graph's structure, you can list all the operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2/tensor_names\n",
      "save/Const\n",
      "save/RestoreV2\n",
      "eval/Const\n",
      "eval/in_top_k/InTopKV2/k\n",
      "GradientDescent/learning_rate\n",
      "clip_by_value_23/clip_value_max\n",
      "clip_by_value_23/clip_value_min\n",
      "clip_by_value_22/clip_value_max\n",
      "clip_by_value_22/clip_value_min\n",
      "clip_by_value_21/clip_value_max\n",
      "clip_by_value_21/clip_value_min\n",
      "clip_by_value_20/clip_value_max\n",
      "clip_by_value_20/clip_value_min\n",
      "clip_by_value_19/clip_value_max\n",
      "clip_by_value_19/clip_value_min\n",
      "clip_by_value_18/clip_value_max\n",
      "clip_by_value_18/clip_value_min\n",
      "clip_by_value_17/clip_value_max\n",
      "clip_by_value_17/clip_value_min\n",
      "clip_by_value_16/clip_value_max\n",
      "clip_by_value_16/clip_value_min\n",
      "clip_by_value_15/clip_value_max\n",
      "clip_by_value_15/clip_value_min\n",
      "clip_by_value_14/clip_value_max\n",
      "clip_by_value_14/clip_value_min\n",
      "clip_by_value_13/clip_value_max\n",
      "clip_by_value_13/clip_value_min\n",
      "clip_by_value_12/clip_value_max\n",
      "clip_by_value_12/clip_value_min\n",
      "gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "gradients_1/loss/loss_grad/Maximum/y\n",
      "gradients_1/loss/loss_grad/Const_1\n",
      "gradients_1/loss/loss_grad/Const\n",
      "gradients_1/loss/loss_grad/Shape_2\n",
      "gradients_1/loss/loss_grad/Prod_1\n",
      "gradients_1/loss/loss_grad/Maximum\n",
      "gradients_1/loss/loss_grad/Reshape/shape\n",
      "gradients_1/grad_ys_0\n",
      "gradients_1/Shape\n",
      "gradients_1/Fill\n",
      "gradients_1/loss/loss_grad/Reshape\n",
      "clip_by_value_11/clip_value_max\n",
      "clip_by_value_11/clip_value_min\n",
      "clip_by_value_10/clip_value_max\n",
      "clip_by_value_10/clip_value_min\n",
      "clip_by_value_9/clip_value_max\n",
      "clip_by_value_9/clip_value_min\n",
      "clip_by_value_8/clip_value_max\n",
      "clip_by_value_8/clip_value_min\n",
      "clip_by_value_7/clip_value_max\n",
      "clip_by_value_7/clip_value_min\n",
      "clip_by_value_6/clip_value_max\n",
      "clip_by_value_6/clip_value_min\n",
      "clip_by_value_5/clip_value_max\n",
      "clip_by_value_5/clip_value_min\n",
      "clip_by_value_4/clip_value_max\n",
      "clip_by_value_4/clip_value_min\n",
      "clip_by_value_3/clip_value_max\n",
      "clip_by_value_3/clip_value_min\n",
      "clip_by_value_2/clip_value_max\n",
      "clip_by_value_2/clip_value_min\n",
      "clip_by_value_1/clip_value_max\n",
      "clip_by_value_1/clip_value_min\n",
      "clip_by_value/clip_value_max\n",
      "clip_by_value/clip_value_min\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "gradients/loss/loss_grad/Maximum/y\n",
      "gradients/loss/loss_grad/Const_1\n",
      "gradients/loss/loss_grad/Const\n",
      "gradients/loss/loss_grad/Shape_2\n",
      "gradients/loss/loss_grad/Prod_1\n",
      "gradients/loss/loss_grad/Maximum\n",
      "gradients/loss/loss_grad/Reshape/shape\n",
      "gradients/grad_ys_0\n",
      "gradients/Shape\n",
      "gradients/Fill\n",
      "gradients/loss/loss_grad/Reshape\n",
      "loss/Const\n",
      "outputs/bias\n",
      "save/Assign_10\n",
      "outputs/bias/read\n",
      "outputs/bias/Initializer/zeros\n",
      "outputs/bias/Assign\n",
      "outputs/kernel\n",
      "save/Assign_11\n",
      "outputs/kernel/read\n",
      "outputs/kernel/Initializer/random_uniform/max\n",
      "outputs/kernel/Initializer/random_uniform/min\n",
      "outputs/kernel/Initializer/random_uniform/sub\n",
      "outputs/kernel/Initializer/random_uniform/shape\n",
      "outputs/kernel/Initializer/random_uniform/RandomUniform\n",
      "outputs/kernel/Initializer/random_uniform/mul\n",
      "outputs/kernel/Initializer/random_uniform\n",
      "outputs/kernel/Assign\n",
      "hidden5/bias\n",
      "save/Assign_8\n",
      "hidden5/bias/read\n",
      "hidden5/bias/Initializer/zeros\n",
      "hidden5/bias/Assign\n",
      "hidden5/kernel\n",
      "save/Assign_9\n",
      "hidden5/kernel/read\n",
      "hidden5/kernel/Initializer/random_uniform/max\n",
      "hidden5/kernel/Initializer/random_uniform/min\n",
      "hidden5/kernel/Initializer/random_uniform/sub\n",
      "hidden5/kernel/Initializer/random_uniform/shape\n",
      "hidden5/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden5/kernel/Initializer/random_uniform/mul\n",
      "hidden5/kernel/Initializer/random_uniform\n",
      "hidden5/kernel/Assign\n",
      "hidden4/bias\n",
      "save/Assign_6\n",
      "hidden4/bias/read\n",
      "hidden4/bias/Initializer/zeros\n",
      "hidden4/bias/Assign\n",
      "hidden4/kernel\n",
      "save/Assign_7\n",
      "hidden4/kernel/read\n",
      "hidden4/kernel/Initializer/random_uniform/max\n",
      "hidden4/kernel/Initializer/random_uniform/min\n",
      "hidden4/kernel/Initializer/random_uniform/sub\n",
      "hidden4/kernel/Initializer/random_uniform/shape\n",
      "hidden4/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden4/kernel/Initializer/random_uniform/mul\n",
      "hidden4/kernel/Initializer/random_uniform\n",
      "hidden4/kernel/Assign\n",
      "hidden3/bias\n",
      "save/Assign_4\n",
      "hidden3/bias/read\n",
      "hidden3/bias/Initializer/zeros\n",
      "hidden3/bias/Assign\n",
      "hidden3/kernel\n",
      "save/Assign_5\n",
      "hidden3/kernel/read\n",
      "hidden3/kernel/Initializer/random_uniform/max\n",
      "hidden3/kernel/Initializer/random_uniform/min\n",
      "hidden3/kernel/Initializer/random_uniform/sub\n",
      "hidden3/kernel/Initializer/random_uniform/shape\n",
      "hidden3/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden3/kernel/Initializer/random_uniform/mul\n",
      "hidden3/kernel/Initializer/random_uniform\n",
      "hidden3/kernel/Assign\n",
      "hidden2/bias\n",
      "save/Assign_2\n",
      "hidden2/bias/read\n",
      "hidden2/bias/Initializer/zeros\n",
      "hidden2/bias/Assign\n",
      "hidden2/kernel\n",
      "save/Assign_3\n",
      "hidden2/kernel/read\n",
      "hidden2/kernel/Initializer/random_uniform/max\n",
      "hidden2/kernel/Initializer/random_uniform/min\n",
      "hidden2/kernel/Initializer/random_uniform/sub\n",
      "hidden2/kernel/Initializer/random_uniform/shape\n",
      "hidden2/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden2/kernel/Initializer/random_uniform/mul\n",
      "hidden2/kernel/Initializer/random_uniform\n",
      "hidden2/kernel/Assign\n",
      "hidden1/bias\n",
      "save/Assign\n",
      "hidden1/bias/read\n",
      "hidden1/bias/Initializer/zeros\n",
      "hidden1/bias/Assign\n",
      "hidden1/kernel\n",
      "save/Assign_1\n",
      "save/restore_all\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "hidden1/kernel/read\n",
      "hidden1/kernel/Initializer/random_uniform/max\n",
      "hidden1/kernel/Initializer/random_uniform/min\n",
      "hidden1/kernel/Initializer/random_uniform/sub\n",
      "hidden1/kernel/Initializer/random_uniform/shape\n",
      "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden1/kernel/Initializer/random_uniform/mul\n",
      "hidden1/kernel/Initializer/random_uniform\n",
      "hidden1/kernel/Assign\n",
      "init\n",
      "y\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "X\n",
      "dnn/hidden1/MatMul\n",
      "dnn/hidden1/BiasAdd\n",
      "dnn/hidden1/Relu\n",
      "dnn/hidden2/MatMul\n",
      "dnn/hidden2/BiasAdd\n",
      "dnn/hidden2/Relu\n",
      "dnn/hidden3/MatMul\n",
      "dnn/hidden3/BiasAdd\n",
      "dnn/hidden3/Relu\n",
      "dnn/hidden4/MatMul\n",
      "dnn/hidden4/BiasAdd\n",
      "dnn/hidden4/Relu\n",
      "dnn/hidden5/MatMul\n",
      "dnn/hidden5/BiasAdd\n",
      "dnn/hidden5/Relu\n",
      "dnn/outputs/MatMul\n",
      "dnn/outputs/BiasAdd\n",
      "eval/in_top_k/InTopKV2\n",
      "eval/Cast\n",
      "eval/accuracy\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "gradients_1/zeros_like\n",
      "gradients_1/loss/loss_grad/Shape_1\n",
      "gradients_1/loss/loss_grad/Prod\n",
      "gradients_1/loss/loss_grad/floordiv\n",
      "gradients_1/loss/loss_grad/Cast\n",
      "gradients_1/loss/loss_grad/Shape\n",
      "gradients_1/loss/loss_grad/Tile\n",
      "gradients_1/loss/loss_grad/truediv\n",
      "gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "gradients_1/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
      "gradients_1/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
      "gradients_1/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_23\n",
      "GradientDescent/update_outputs/bias/ApplyGradientDescent\n",
      "gradients_1/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
      "gradients_1/dnn/outputs/MatMul_grad/MatMul_1\n",
      "gradients_1/dnn/outputs/MatMul_grad/MatMul\n",
      "gradients_1/dnn/outputs/MatMul_grad/tuple/group_deps\n",
      "gradients_1/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value_22\n",
      "GradientDescent/update_outputs/kernel/ApplyGradientDescent\n",
      "gradients_1/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
      "gradients_1/dnn/hidden5/Relu_grad/ReluGrad\n",
      "gradients_1/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
      "gradients_1/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
      "gradients_1/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_21\n",
      "GradientDescent/update_hidden5/bias/ApplyGradientDescent\n",
      "gradients_1/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
      "gradients_1/dnn/hidden5/MatMul_grad/MatMul_1\n",
      "gradients_1/dnn/hidden5/MatMul_grad/MatMul\n",
      "gradients_1/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
      "gradients_1/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value_20\n",
      "GradientDescent/update_hidden5/kernel/ApplyGradientDescent\n",
      "gradients_1/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
      "gradients_1/dnn/hidden4/Relu_grad/ReluGrad\n",
      "gradients_1/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
      "gradients_1/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
      "gradients_1/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_19\n",
      "GradientDescent/update_hidden4/bias/ApplyGradientDescent\n",
      "gradients_1/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
      "gradients_1/dnn/hidden4/MatMul_grad/MatMul_1\n",
      "gradients_1/dnn/hidden4/MatMul_grad/MatMul\n",
      "gradients_1/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
      "gradients_1/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value_18\n",
      "GradientDescent/update_hidden4/kernel/ApplyGradientDescent\n",
      "gradients_1/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
      "gradients_1/dnn/hidden3/Relu_grad/ReluGrad\n",
      "gradients_1/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
      "gradients_1/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
      "gradients_1/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_17\n",
      "GradientDescent/update_hidden3/bias/ApplyGradientDescent\n",
      "gradients_1/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
      "gradients_1/dnn/hidden3/MatMul_grad/MatMul_1\n",
      "gradients_1/dnn/hidden3/MatMul_grad/MatMul\n",
      "gradients_1/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
      "gradients_1/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value_16\n",
      "GradientDescent/update_hidden3/kernel/ApplyGradientDescent\n",
      "gradients_1/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
      "gradients_1/dnn/hidden2/Relu_grad/ReluGrad\n",
      "gradients_1/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "gradients_1/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "gradients_1/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_15\n",
      "GradientDescent/update_hidden2/bias/ApplyGradientDescent\n",
      "gradients_1/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "gradients_1/dnn/hidden2/MatMul_grad/MatMul_1\n",
      "gradients_1/dnn/hidden2/MatMul_grad/MatMul\n",
      "gradients_1/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
      "gradients_1/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value_14\n",
      "GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n",
      "gradients_1/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
      "gradients_1/dnn/hidden1/Relu_grad/ReluGrad\n",
      "gradients_1/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "gradients_1/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "gradients_1/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_13\n",
      "GradientDescent/update_hidden1/bias/ApplyGradientDescent\n",
      "gradients_1/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "gradients_1/dnn/hidden1/MatMul_grad/MatMul_1\n",
      "gradients_1/dnn/hidden1/MatMul_grad/MatMul\n",
      "gradients_1/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
      "gradients_1/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value_12\n",
      "GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n",
      "GradientDescent\n",
      "gradients_1/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "gradients/zeros_like\n",
      "gradients/loss/loss_grad/Shape_1\n",
      "gradients/loss/loss_grad/Prod\n",
      "gradients/loss/loss_grad/floordiv\n",
      "gradients/loss/loss_grad/Cast\n",
      "gradients/loss/loss_grad/Shape\n",
      "gradients/loss/loss_grad/Tile\n",
      "gradients/loss/loss_grad/truediv\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_11\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value_10\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_9\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value_8\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_7\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value_6\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_5\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value_4\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_3\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value_2\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "clip_by_value_1\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
      "loss/loss\n"
     ]
    }
   ],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops, that's a lot of operations! It's much easier to use TensorBoard to visualize the graph. The following hack will allow you to visualize the graph within Jupyter (if it does not work with your browser, you will need to use a `FileWriter` to save the graph and then visualize it in TensorBoard):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works on Chrome, not guaranteed on other browsers\n",
    "\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.7224827313584268&quot;).pbtxt = 'node {\\n  name: &quot;X&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 784\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;y&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        unknown_rank: true\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\020\\\\003\\\\000\\\\000,\\\\001\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 784\\n        }\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 300\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;hidden1/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden1/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;hidden1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden1/MatMul&quot;\\n  input: &quot;hidden1/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;,\\\\001\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.13093073666095734\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.13093073666095734\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 22\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;hidden2/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden2/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  input: &quot;hidden2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden2/MatMul&quot;\\n  input: &quot;hidden2/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/StopGradient&quot;\\n  op: &quot;StopGradient&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 40\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;hidden3/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden3/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/StopGradient&quot;\\n  input: &quot;hidden3/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden3/MatMul&quot;\\n  input: &quot;hidden3/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden3/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\000\\\\024\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.29277002811431885\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.29277002811431885\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 57\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 20\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 20\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 20\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;hidden4/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden4/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  input: &quot;hidden4/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden4/MatMul&quot;\\n  input: &quot;hidden4/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden4/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\024\\\\000\\\\000\\\\000\\\\n\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.4472135901451111\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.4472135901451111\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 74\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 20\\n        }\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;outputs/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 10\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;outputs/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;outputs/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/outputs/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  input: &quot;outputs/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/outputs/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/outputs/MatMul&quot;\\n  input: &quot;outputs/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  op: &quot;SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;dnn/outputs/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tlabels&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/loss&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;loss/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/in_top_k/InTopKV2/k&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/in_top_k/InTopKV2&quot;\\n  op: &quot;InTopKV2&quot;\\n  input: &quot;dnn/outputs/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  input: &quot;eval/in_top_k/InTopKV2/k&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;eval/in_top_k/InTopKV2&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/accuracy&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;eval/Cast&quot;\\n  input: &quot;eval/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/grad_ys_0&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;train/gradients/Shape&quot;\\n  input: &quot;train/gradients/grad_ys_0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;index_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;train/gradients/Fill&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Reshape&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Shape_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Shape_1&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Shape_2&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Prod_1&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Prod&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;train/gradients/loss/loss_grad/floordiv&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/loss_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Tile&quot;\\n  input: &quot;train/gradients/loss/loss_grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/zeros_like&quot;\\n  op: &quot;ZerosLike&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  op: &quot;PreventGradient&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;message&quot;\\n    value {\\n      s: &quot;Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\\\\\\'s interaction with tf.gradients()&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;train/gradients/loss/loss_grad/truediv&quot;\\n  input: &quot;train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  input: &quot;train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^train/gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  input: &quot;^train/gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^train/gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;train/gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;outputs/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  input: &quot;train/gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^train/gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^train/gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^train/gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^train/gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;train/gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;train/gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^train/gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^train/gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  input: &quot;^train/gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^train/gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;train/gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden4/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  input: &quot;train/gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^train/gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  input: &quot;^train/gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  input: &quot;^train/gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^train/gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;train/gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;train/gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^train/gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^train/gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  input: &quot;^train/gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^train/gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;train/gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden3/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/StopGradient&quot;\\n  input: &quot;train/gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^train/gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  input: &quot;^train/gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  input: &quot;^train/gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;train/gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^train/gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@train/gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/GradientDescent/update_hidden3/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;train/GradientDescent/learning_rate&quot;\\n  input: &quot;train/gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/GradientDescent/update_hidden3/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;train/GradientDescent/learning_rate&quot;\\n  input: &quot;train/gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/GradientDescent/update_hidden4/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;train/GradientDescent/learning_rate&quot;\\n  input: &quot;train/gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/GradientDescent/update_hidden4/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;train/GradientDescent/learning_rate&quot;\\n  input: &quot;train/gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/GradientDescent/update_outputs/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;train/GradientDescent/learning_rate&quot;\\n  input: &quot;train/gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/GradientDescent/update_outputs/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;train/GradientDescent/learning_rate&quot;\\n  input: &quot;train/gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;train/GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^train/GradientDescent/update_hidden3/bias/ApplyGradientDescent&quot;\\n  input: &quot;^train/GradientDescent/update_hidden3/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^train/GradientDescent/update_hidden4/bias/ApplyGradientDescent&quot;\\n  input: &quot;^train/GradientDescent/update_hidden4/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^train/GradientDescent/update_outputs/bias/ApplyGradientDescent&quot;\\n  input: &quot;^train/GradientDescent/update_outputs/kernel/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;save/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 6\\n          }\\n        }\\n        string_val: &quot;hidden1/bias&quot;\\n        string_val: &quot;hidden1/kernel&quot;\\n        string_val: &quot;hidden2/bias&quot;\\n        string_val: &quot;hidden2/kernel&quot;\\n        string_val: &quot;hidden3/bias&quot;\\n        string_val: &quot;hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 6\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/SaveV2/tensor_names&quot;\\n  input: &quot;save/SaveV2/shape_and_slices&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;^save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 6\\n          }\\n        }\\n        string_val: &quot;hidden1/bias&quot;\\n        string_val: &quot;hidden1/kernel&quot;\\n        string_val: &quot;hidden2/bias&quot;\\n        string_val: &quot;hidden2/kernel&quot;\\n        string_val: &quot;hidden3/bias&quot;\\n        string_val: &quot;hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 6\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2/tensor_names&quot;\\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_1&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;save/RestoreV2:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_2&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;save/RestoreV2:2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_3&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;save/RestoreV2:3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_4&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;save/RestoreV2:4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_5&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;save/RestoreV2:5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save/Assign&quot;\\n  input: &quot;^save/Assign_1&quot;\\n  input: &quot;^save/Assign_2&quot;\\n  input: &quot;^save/Assign_3&quot;\\n  input: &quot;^save/Assign_4&quot;\\n  input: &quot;^save/Assign_5&quot;\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^hidden1/bias/Assign&quot;\\n  input: &quot;^hidden1/kernel/Assign&quot;\\n  input: &quot;^hidden2/bias/Assign&quot;\\n  input: &quot;^hidden2/kernel/Assign&quot;\\n  input: &quot;^hidden3/bias/Assign&quot;\\n  input: &quot;^hidden3/kernel/Assign&quot;\\n  input: &quot;^hidden4/bias/Assign&quot;\\n  input: &quot;^hidden4/kernel/Assign&quot;\\n  input: &quot;^outputs/bias/Assign&quot;\\n  input: &quot;^outputs/kernel/Assign&quot;\\n}\\nnode {\\n  name: &quot;save_1/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save_1/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 10\\n          }\\n        }\\n        string_val: &quot;hidden1/bias&quot;\\n        string_val: &quot;hidden1/kernel&quot;\\n        string_val: &quot;hidden2/bias&quot;\\n        string_val: &quot;hidden2/kernel&quot;\\n        string_val: &quot;hidden3/bias&quot;\\n        string_val: &quot;hidden3/kernel&quot;\\n        string_val: &quot;hidden4/bias&quot;\\n        string_val: &quot;hidden4/kernel&quot;\\n        string_val: &quot;outputs/bias&quot;\\n        string_val: &quot;outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save_1/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 10\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save_1/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save_1/Const&quot;\\n  input: &quot;save_1/SaveV2/tensor_names&quot;\\n  input: &quot;save_1/SaveV2/shape_and_slices&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;outputs/kernel&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save_1/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save_1/Const&quot;\\n  input: &quot;^save_1/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save_1/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save_1/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 10\\n          }\\n        }\\n        string_val: &quot;hidden1/bias&quot;\\n        string_val: &quot;hidden1/kernel&quot;\\n        string_val: &quot;hidden2/bias&quot;\\n        string_val: &quot;hidden2/kernel&quot;\\n        string_val: &quot;hidden3/bias&quot;\\n        string_val: &quot;hidden3/kernel&quot;\\n        string_val: &quot;hidden4/bias&quot;\\n        string_val: &quot;hidden4/kernel&quot;\\n        string_val: &quot;outputs/bias&quot;\\n        string_val: &quot;outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save_1/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 10\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save_1/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save_1/Const&quot;\\n  input: &quot;save_1/RestoreV2/tensor_names&quot;\\n  input: &quot;save_1/RestoreV2/shape_and_slices&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save_1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;save_1/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save_1/Assign_1&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;save_1/RestoreV2:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save_1/Assign_2&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;save_1/RestoreV2:2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save_1/Assign_3&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;save_1/RestoreV2:3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save_1/Assign_4&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;save_1/RestoreV2:4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save_1/Assign_5&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;save_1/RestoreV2:5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save_1/Assign_6&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;save_1/RestoreV2:6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save_1/Assign_7&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;save_1/RestoreV2:7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save_1/Assign_8&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;save_1/RestoreV2:8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save_1/Assign_9&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;save_1/RestoreV2:9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save_1/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save_1/Assign&quot;\\n  input: &quot;^save_1/Assign_1&quot;\\n  input: &quot;^save_1/Assign_2&quot;\\n  input: &quot;^save_1/Assign_3&quot;\\n  input: &quot;^save_1/Assign_4&quot;\\n  input: &quot;^save_1/Assign_5&quot;\\n  input: &quot;^save_1/Assign_6&quot;\\n  input: &quot;^save_1/Assign_7&quot;\\n  input: &quot;^save_1/Assign_8&quot;\\n  input: &quot;^save_1/Assign_9&quot;\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.7224827313584268&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you know which operations you need, you can get a handle on them using the graph's `get_operation_by_name()` or `get_tensor_by_name()` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are the author of the original model, you could make things easier for people who will reuse your model by giving operations very clear names and documenting them. Another approach is to create a collection containing all the important operations that people will want to get a handle on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for op in (X, y, accuracy, training_op):\n",
    "    tf.add_to_collection(\"my_important\", op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way people who reuse your model will be able to simply write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, accuracy, training_op = tf.get_collection(\"my_important\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start a session, restore the model's state and continue training on your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, let's test this for real!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9636\n",
      "1 Validation accuracy: 0.9632\n",
      "2 Validation accuracy: 0.9658\n",
      "3 Validation accuracy: 0.9652\n",
      "4 Validation accuracy: 0.9646\n",
      "5 Validation accuracy: 0.965\n",
      "6 Validation accuracy: 0.969\n",
      "7 Validation accuracy: 0.9682\n",
      "8 Validation accuracy: 0.9682\n",
      "9 Validation accuracy: 0.9684\n",
      "10 Validation accuracy: 0.9704\n",
      "11 Validation accuracy: 0.971\n",
      "12 Validation accuracy: 0.9668\n",
      "13 Validation accuracy: 0.97\n",
      "14 Validation accuracy: 0.9712\n",
      "15 Validation accuracy: 0.9726\n",
      "16 Validation accuracy: 0.9718\n",
      "17 Validation accuracy: 0.971\n",
      "18 Validation accuracy: 0.9712\n",
      "19 Validation accuracy: 0.9712\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you have access to the Python code that built the original graph, you can use it instead of `import_meta_graph()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "learning_rate = 0.01\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And continue training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9634\n",
      "1 Validation accuracy: 0.9622\n",
      "2 Validation accuracy: 0.9648\n",
      "3 Validation accuracy: 0.9656\n",
      "4 Validation accuracy: 0.9674\n",
      "5 Validation accuracy: 0.965\n",
      "6 Validation accuracy: 0.9684\n",
      "7 Validation accuracy: 0.9682\n",
      "8 Validation accuracy: 0.9686\n",
      "9 Validation accuracy: 0.9702\n",
      "10 Validation accuracy: 0.9676\n",
      "11 Validation accuracy: 0.969\n",
      "12 Validation accuracy: 0.9686\n",
      "13 Validation accuracy: 0.969\n",
      "14 Validation accuracy: 0.9714\n",
      "15 Validation accuracy: 0.973\n",
      "16 Validation accuracy: 0.9714\n",
      "17 Validation accuracy: 0.9716\n",
      "18 Validation accuracy: 0.9722\n",
      "19 Validation accuracy: 0.9724\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general you will want to reuse only the lower layers. If you are using `import_meta_graph()` it will load the whole graph, but you can simply ignore the parts you do not need. In this example, we add a new 4th hidden layer on top of the pretrained 3rd layer (ignoring the old 4th hidden layer). We also build a new output layer, the loss for this new output, and a new optimizer to minimize it. We also need another saver to save the whole graph (containing both the entire old graph plus the new operations), and an initialization operation to initialize all the new variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_hidden4 = 20  # new layer\n",
    "n_outputs = 10  # new layer\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "hidden3 = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden4/Relu:0\")\n",
    "\n",
    "new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"new_hidden4\")\n",
    "new_logits = tf.layers.dense(new_hidden4, n_outputs, name=\"new_outputs\")\n",
    "\n",
    "with tf.name_scope(\"new_loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"new_eval\"):\n",
    "    correct = tf.nn.in_top_k(new_logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"new_train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can train this new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9138\n",
      "1 Validation accuracy: 0.94\n",
      "2 Validation accuracy: 0.9472\n",
      "3 Validation accuracy: 0.9516\n",
      "4 Validation accuracy: 0.953\n",
      "5 Validation accuracy: 0.956\n",
      "6 Validation accuracy: 0.9616\n",
      "7 Validation accuracy: 0.9614\n",
      "8 Validation accuracy: 0.962\n",
      "9 Validation accuracy: 0.9658\n",
      "10 Validation accuracy: 0.9654\n",
      "11 Validation accuracy: 0.9664\n",
      "12 Validation accuracy: 0.9626\n",
      "13 Validation accuracy: 0.9656\n",
      "14 Validation accuracy: 0.969\n",
      "15 Validation accuracy: 0.9692\n",
      "16 Validation accuracy: 0.9688\n",
      "17 Validation accuracy: 0.967\n",
      "18 Validation accuracy: 0.9682\n",
      "19 Validation accuracy: 0.9674\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = new_saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have access to the Python code that built the original graph, you can just reuse the parts you need and drop the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you must create one `Saver` to restore the pretrained model (giving it the list of variables to restore, or else it will complain that the graphs don't match), and another `Saver` to save the new model, once it is trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9024\n",
      "1 Validation accuracy: 0.9332\n",
      "2 Validation accuracy: 0.943\n",
      "3 Validation accuracy: 0.947\n",
      "4 Validation accuracy: 0.9516\n",
      "5 Validation accuracy: 0.9532\n",
      "6 Validation accuracy: 0.9558\n",
      "7 Validation accuracy: 0.9592\n",
      "8 Validation accuracy: 0.9586\n",
      "9 Validation accuracy: 0.9608\n",
      "10 Validation accuracy: 0.9626\n",
      "11 Validation accuracy: 0.962\n",
      "12 Validation accuracy: 0.964\n",
      "13 Validation accuracy: 0.9662\n",
      "14 Validation accuracy: 0.966\n",
      "15 Validation accuracy: 0.9662\n",
      "16 Validation accuracy: 0.9672\n",
      "17 Validation accuracy: 0.9674\n",
      "18 Validation accuracy: 0.9682\n",
      "19 Validation accuracy: 0.9678\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden[123]\") # regex\n",
    "restore_path = tf.train.Saver(reuse_vars) # restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_path.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    for epoch in range(n_epochs):                                            \n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): \n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})        \n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})     \n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)                   \n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing Models from Other Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model was trained using another framework, you will need to load the model parameters manually (e.g., using Theano code if it was trained with Theano), then assign them to the appropriate variables. This can be quite tedious. For example, the following code shows how you would copy the weight and biases from the first hidden layer of a model trained using another framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, for each variable we want to reuse, we find its initializer's assignment operation, and we get its second input, which corresponds to the initialization value. When we run the initializer, we replace the initialization values with the ones we want, using a `feed_dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61.  83. 105.]]\n"
     ]
    }
   ],
   "source": [
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n",
    "original_b = [7., 8., 9.]                 # Load the biases from the other framework\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] Build the rest of the model\n",
    "\n",
    "# Get a handle on the assignment nodes for the hidden1 variables\n",
    "graph = tf.get_default_graph()\n",
    "assign_kernel = graph.get_operation_by_name(\"hidden1/kernel/Assign\")\n",
    "assign_bias = graph.get_operation_by_name(\"hidden1/bias/Assign\")\n",
    "init_kernel = assign_kernel.inputs[1]\n",
    "init_bias = assign_bias.inputs[1]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init, feed_dict={init_kernel: original_w, init_bias: original_b})\n",
    "    # [...] Train the model on your new task\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the weights variable created by the `tf.layers.dense()` function is called `\"kernel\"` (instead of `\"weights\"` when using the `tf.contrib.layers.fully_connected()`, as in the book), and the biases variable is called `bias` instead of `biases`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach (initially used in the book) would be to create dedicated assignment nodes and dedicated placeholders. This is more verbose and less efficient, but you may find this more explicit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61.  83. 105.]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3\n",
    "\n",
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n",
    "original_b = [7., 8., 9.]                 # Load the biases from the other framework\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] Build the rest of the model\n",
    "\n",
    "# Get a handle on the variables of layer hidden1\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):  # root scope\n",
    "    hidden1_weights = tf.get_variable(\"hidden1/kernel\")\n",
    "    hidden1_biases = tf.get_variable(\"hidden1/bias\")\n",
    "\n",
    "# Create dedicated placeholders and assignment nodes\n",
    "original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden1))\n",
    "original_biases = tf.placeholder(tf.float32, shape=n_hidden1)\n",
    "assign_hidden1_weights = tf.assign(hidden1_weights, original_weights)\n",
    "assign_hidden1_biases = tf.assign(hidden1_biases, original_biases)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(assign_hidden1_weights, feed_dict={original_weights: original_w})\n",
    "    sess.run(assign_hidden1_biases, feed_dict={original_biases: original_b})\n",
    "    # [...] Train the model on your new task\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could also get a handle on the variables using `get_collection()` and specifying the `scope`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden1/bias:0' shape=(3,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could use the graph's `get_tensor_by_name()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/bias:0' shape=(3,) dtype=float32_ref>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/bias:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freezing the Lower Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is likely that the lower layers of the first DNN have learned to detect low-level features in pictures that will be useful across both image classification tasks, so you can just reuse these layers as they are. It is generally a good idea to “freeze” their weights when training the new DNN: if the lower-layer weights are fixed, then the higher-layer weights will be easier to train (because they won’t have to learn a moving target). To freeze the lower layers during training, one solution is to give the optimizer the list of variables to train, excluding the variables from the lower layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden[34]|outputs\")\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.8964\n",
      "1 Validation accuracy: 0.9298\n",
      "2 Validation accuracy: 0.94\n",
      "3 Validation accuracy: 0.9442\n",
      "4 Validation accuracy: 0.948\n",
      "5 Validation accuracy: 0.951\n",
      "6 Validation accuracy: 0.9508\n",
      "7 Validation accuracy: 0.9538\n",
      "8 Validation accuracy: 0.9554\n",
      "9 Validation accuracy: 0.957\n",
      "10 Validation accuracy: 0.9562\n",
      "11 Validation accuracy: 0.9566\n",
      "12 Validation accuracy: 0.9572\n",
      "13 Validation accuracy: 0.9578\n",
      "14 Validation accuracy: 0.959\n",
      "15 Validation accuracy: 0.9576\n",
      "16 Validation accuracy: 0.9574\n",
      "17 Validation accuracy: 0.9602\n",
      "18 Validation accuracy: 0.9592\n",
      "19 Validation accuracy: 0.9602\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden[123]\") # regular expression\n",
    "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to add a stop_gradient() layer in the graph. Any layer below it will be frozen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused frozen\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training code is exactly the same as earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.902\n",
      "1 Validation accuracy: 0.9302\n",
      "2 Validation accuracy: 0.9438\n",
      "3 Validation accuracy: 0.9478\n",
      "4 Validation accuracy: 0.9514\n",
      "5 Validation accuracy: 0.9522\n",
      "6 Validation accuracy: 0.9524\n",
      "7 Validation accuracy: 0.9556\n",
      "8 Validation accuracy: 0.9556\n",
      "9 Validation accuracy: 0.9558\n",
      "10 Validation accuracy: 0.957\n",
      "11 Validation accuracy: 0.9552\n",
      "12 Validation accuracy: 0.9572\n",
      "13 Validation accuracy: 0.9582\n",
      "14 Validation accuracy: 0.9582\n",
      "15 Validation accuracy: 0.957\n",
      "16 Validation accuracy: 0.9566\n",
      "17 Validation accuracy: 0.9578\n",
      "18 Validation accuracy: 0.9594\n",
      "19 Validation accuracy: 0.958\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden[123]\")\n",
    "restore_saver = tf.train.Saver(reuse_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caching the Frozen Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the frozen layers won’t change, it is possible to cache the output of the topmost frozen layer for each training instance. Since training goes through the whole dataset many times, this will give you a huge speed boost as you will only need to go through the frozen layers once per training instance (instead of once per epoch). For example, you could first run the whole training set through the lower layers (assuming you have enough RAM), then during training, instead of building batches of training instances, you would build batches of outputs from hidden layer   2 and feed them to the training operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\") # reused frozen & cached\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden[123]\")\n",
    "restore_saver = tf.train.Saver(reuse_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.8958\n",
      "1 Validation accuracy: 0.9326\n",
      "2 Validation accuracy: 0.9432\n",
      "3 Validation accuracy: 0.9482\n",
      "4 Validation accuracy: 0.9498\n",
      "5 Validation accuracy: 0.9508\n",
      "6 Validation accuracy: 0.953\n",
      "7 Validation accuracy: 0.9552\n",
      "8 Validation accuracy: 0.9544\n",
      "9 Validation accuracy: 0.9566\n",
      "10 Validation accuracy: 0.9546\n",
      "11 Validation accuracy: 0.9572\n",
      "12 Validation accuracy: 0.9576\n",
      "13 Validation accuracy: 0.958\n",
      "14 Validation accuracy: 0.9576\n",
      "15 Validation accuracy: 0.9562\n",
      "16 Validation accuracy: 0.9574\n",
      "17 Validation accuracy: 0.9592\n",
      "18 Validation accuracy: 0.9578\n",
      "19 Validation accuracy: 0.9582\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_batches = len(X_train) // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    h2_cache = sess.run(hidden2, feed_dict={X: X_train})\n",
    "    h2_cache_valid = sess.run(hidden2, feed_dict={X: X_valid}) \n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_idx = np.random.permutation(len(X_train))\n",
    "        hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n",
    "        y_batches = np.array_split(y_train[shuffled_idx], n_batches)\n",
    "        \n",
    "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "            sess.run(training_op, feed_dict={hidden2: hidden2_batch, y: y_batch})\n",
    "            \n",
    "        accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_valid, y: y_valid})             \n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)               \n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking, Dropping, or Replacing the Upper Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layer of the original model should usually be replaced since it is most likely not useful at all for the new task, and it may not even have the right number of outputs for the new task.\n",
    "\n",
    "Similarly, the upper hidden layers of the original model are less likely to be as useful as the lower layers, since the high-level features that are most useful for the new task may differ significantly from the ones that were most useful for the original task. You want to find the right number of layers to reuse.\n",
    "\n",
    "Try freezing all the copied layers first, then train your model and see how it performs. Then try unfreezing one or two of the top hidden layers to let backpropagation tweak them and see if performance improves. The more training data you have, the more layers you can unfreeze.\n",
    "\n",
    "If you still cannot get good performance, and you have little training data, try dropping the top hidden layer( s) and freeze all remaining hidden layers again. You can iterate until you find the right number of layers to reuse. If you have plenty of training data, you may try replacing the top hidden layers instead of dropping them, and even add more hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum optimization\n",
    "\n",
    "Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start out slowly, but it will quickly pick up momentum until it eventually reaches terminal velocity (if there is some friction or air resistance). This is the very simple idea behind *Momentum optimization*, proposed by Boris Polyak in 1964.10 In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one drawback of Momentum optimization is that it adds yet another hyperparameter to tune. However, the momentum value of 0.9 usually works well in practice and almost always goes faster than Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient\n",
    "\n",
    "One small variant to Momentum optimization, proposed by Yurii Nesterov in 1983, is almost always faster than vanilla Momentum optimization. The idea of *Nesterov Momentum optimization*, or *Nesterov Accelerated Gradient* (NAG), is to measure the gradient of the cost function not at the local position but slightly ahead in the direction of the momentum. The only difference from vanilla Momentum optimization is that the gradient is measured at θ + βm rather than at θ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9, use_nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "Consider the elongated bowl problem again: Gradient Descent starts by quickly going down the steepest slope, then slowly goes down the bottom of the valley. It would be nice if the algorithm could detect this early on and correct its direction to point a bit more toward the global optimum.\n",
    "\n",
    "The *AdaGrad* algorithm achieves this by scaling down the gradient vector along the steepest dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "\n",
    "Although AdaGrad slows down a bit too fast and ends up never converging to the global optimum, the RMSProp algorithm fixes this by accumulating only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training). It does so by using exponential decay in the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, momentum=0.9, decay=0.9, epsilon=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Optimization\n",
    "\n",
    "Adam, which stands for *adaptive moment estimation*, combines the ideas of Momentum optimization and RMSProp: just like Momentum optimization it keeps track of an exponentially decaying average of past gradients, and just like RMSProp it keeps track of an exponentially decaying average of past squared gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the optimization techniques discussed so far only rely on the *first-order partial derivatives* (*Jacobians*). The optimization literature contains amazing algorithms based on the *second-order partial derivatives* (*the Hessians*). Unfortunately, these algorithms are very hard to apply to deep neural networks because there are n2 Hessians per output (where n is the number of parameters), as opposed to just n Jacobians per output. Since DNNs typically have tens of thousands of parameters, the second-order optimization algorithms often don’t even fit in memory, and even when they do, computing the Hessians is just too slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduling\n",
    "\n",
    "Finding a good learning rate can be tricky. If you set it way too high, training may actually diverge. If you set it too low, training will eventually converge to the optimum, but it will take a very long time. If you set it slightly too high, it will make progress very quickly at first, but it will end up dancing around the optimum, never settling down. If you have a limited computing budget, you may have to interrupt training before it has converged properly, yielding a suboptimal solution.\n",
    "\n",
    "You may be able to find a fairly good learning rate by training your network several times during just a few epochs using various learning rates and comparing the learning curves. The ideal learning rate will learn quickly and \n",
    "converge to good solution.\n",
    "\n",
    "However, you can do better than a constant learning rate: if you start with a high learning rate and then reduce it once it stops making fast progress, you can reach a good solution faster than with the optimal constant learning rate. There are many different strategies to reduce the learning rate during training. These strategies are called *learning schedules*, the most common of which are:\n",
    "- *Predetermined piecewise constant learning rate*. For example, set the learning rate to η0 = 0.1 at first, then to η1 = 0.001 after 50 epochs. Although this solution can work very well, it often requires fiddling around to figure out the right learning rates and when to use them.\n",
    "- *Performance scheduling*. Measure the validation error every N steps (just likefor early stopping) and reduce the learning rate by a factor of λ when the error stops dropping.\n",
    "- *Exponential scheduling*. Set the learning rate to a function of the iteration number t: η( t) = η0 10– t/ r. This works great, but it requires tuning η0 and r. The learning rate will drop by a factor of   10 every r steps.\n",
    "- *Power scheduling*. Set the learning rate to η( t) = η0 (1 + t/ r)– c. The hyperparameter c is typically set to   1. This is similar to exponential scheduling, but the learning rate drops much more slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 validation accuracy 0.959\n",
      "1 validation accuracy 0.9688\n",
      "2 validation accuracy 0.9726\n",
      "3 validation accuracy 0.9804\n",
      "4 validation accuracy 0.982\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"validation accuracy\", accuracy_val)\n",
    "        \n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
